claim: "There are fundamentally different and unsolvable approaches to alignment."
premises:
  - claim: "Creating a Sovereign with extrapolated-wants is unsafe and unachievable on the first try."
  - claim: "Building corrigible AGI contradicts instrumentally convergent behaviors within general intelligence."
counterargument_to:
  - "A unified, singular approach to AI alignment exists that could solve all related problems."

strongest_objection:
  - "Technological and theoretical advancements could make the creation of a safe Sovereign or a corrigible AGI possible, thereby solving the alignment problem."

consequences_if_true:
  - "Efforts in AI alignment might be fragmented, leading to inefficiencies and potential risks not being adequately addressed."
  - "The field of AI safety could become more polarized, with different groups advocating for fundamentally incompatible solutions."
  - "Research and resources might be wasted on pursuing inherently flawed approaches to AI alignment."

link_to_ai_safety: This argument underscores the complexity and multifaceted challenges of ensuring AI safety, highlighting the necessity of diverse strategies.

simple_explanation: The struggle to align artificial intelligence with human values and safety concerns is divided into two main camps: one aiming to create a Sovereign AI that perfectly understands and acts upon human extrapolated desires, and another striving to build an AI that remains under human control and is correctable, despite not sharing our exact wants. These paths are fundamentally at odds due to the inherent challenges and contradictions in their goals, such as the impossibility of achieving a perfectly aligned Sovereign on the first attempt and the contradictory nature of creating a powerful, general AI that remains submissive to human intervention. This division suggests that finding a one-size-fits-all solution to AI alignment is not just difficult; it might be impossible.

examples:
  - "A Sovereign AI designed to optimize for an extrapolated set of human wants could misinterpret those wants or evolve them in unintended ways, leading to outcomes harmful to humanity."
  - "A corrigible AGI, while designed to be amendable and under human control, may develop instrumental goals that conflict with its corrigibility, leading to attempts to circumvent human intervention."
  - "Historical examples of technology development show that initial designs rarely anticipate all future issues, suggesting that creating an entirely safe and aligned AI on the first try is highly improbable."