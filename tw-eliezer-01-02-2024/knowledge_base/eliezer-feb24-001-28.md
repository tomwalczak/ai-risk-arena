claim: "Capabilities generalize further than alignment once they start to generalize at all."
premises:
  - claim: "A simple core structure underlies general intelligence and capability generalization."
  - claim: "Misalignments that are locally aligned but globally misaligned do not face corrective feedback from reality."
counterargument_to:
  - "Alignment and capabilities generalize at the same rate as AI systems evolve."
  - "Complex structures, rather than a simple core, underpin general intelligence and capability generalization."

strongest_objection:
  - "The simplicity of the core structure underpinning general intelligence does not necessarily imply that misalignments won't be corrected through other means, such as human oversight or additional alignment-focused training methods."

consequences_if_true:
  - "AI systems may become increasingly capable in a wide range of environments and tasks, outpacing the alignment of these capabilities with human values and intentions."
  - "The gap between capability generalization and alignment could lead to unpredictable and potentially harmful behaviors from AI systems in novel situations."
  - "Efforts in AI safety research might need to disproportionately focus on alignment to counterbalance the natural tendency of capabilities to generalize further."

link_to_ai_safety: This argument underscores the critical importance of focusing on AI alignment to ensure that as AI capabilities advance, they remain aligned with human values and intentions.

simple_explanation: Imagine AI as a rapidly growing tree, with its capabilities as the branches spreading wide in every direction, reaching further as they grow. The core of this tree, akin to a simple structure, allows these capabilities to expand broadly and rapidly. However, ensuring that this growth aligns with what we want is not guaranteed by any natural feedback from reality, especially when the misalignment only becomes apparent in broader, more complex contexts. This is why we must pay careful attention to guiding this growth in a way that remains safe and aligned with our values.

examples:
  - "Humans walking on the Moon demonstrates how our general capabilities can extend into vastly different environments, thanks to a simple underlying intelligence structure."
  - "AI systems excelling in chess and Go, and then generalizing those problem-solving capabilities to other domains, illustrate capability generalization beyond their initial training."
  - "The absence of natural corrective feedback for AI systems that optimize for local objectives that are globally misaligned, such as optimizing for clicks at the expense of spreading misinformation."