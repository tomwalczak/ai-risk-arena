Welcome to the future of Life Institute Podcast. I'm here with Connor Leahy. Connor is the CEO of conjecture. And conjecture. Is this company researching scalable AI alignment. So Connor, welcome to the podcast. Glad to be back. Okay, what is what is happening with the GPT? For what is this the moment that AI becomes a mainstream issue? Christ, what a way to start out. It is no exaggeration to say that the last two weeks of my life have been the most interesting of my career. In terms of events in the wider world, you know, I thought nothing could be GPT free. Like, you know, after I've seen what happened with GPT three, I'm like, Okay, this is the craziest thing that's gonna happen, like short period of time. But then I quickly realized, no, that can't be true. Like, things are only going to get crazier. And as predicted, exactly, that's what has happened. And as predicted, the release of GPT. Four has been even crazier than GPT. For the GPU. Three, the world has gone even crazier things are, things have really changed. Like, I cannot overstate how much the world has changed over the last like, necessarily only says up for a while since chat GPT. Maybe chat GPT was even a bigger change in like, wider political thing. Like, I won't mince words, like the thing that really has struck me over. I've been talking to a lot of people recently, now I have journalists running down my door, I talk to politicians and national security people and people are sort of the one thing that really strikes me is that people are starting to panic. What this is, so this goes beyond Silicon Valley Twitter circles, this is this is venturing into politics, politics, and governmental agencies and so on. It goes to the look, when I've been doing air for a long time. And I come from a pretty rural place, and like Southern Germany, and when I went back to visit my, my mother in for Christmas, and you know, all my cousins and like, you know, family were there. They talked about tax GPT. I was there in this teeny world where there's usually no technology, and I'm the only one who really knows how to you know, really use a computer very well. And whatever. And then they're talking about their car i Well, we thought this AI thing you were talking about like that was like, you know, that wasn't like, sort of like some kind of thing you liked. But wow, you were right. Like this is actually happening. And like, yeah, yeah, big surprise. So this is not just a thing that is in a small circle of people in tech, or Silicon Valley, or whatever, or whatever. This is different. Like this is very different. You know, people were getting, you know, front page time news coverage about this kind of stuff we're getting, you know, people from all walks of life suddenly noticing Wait, like, this is actually real, this is actually affecting me, this is actually affecting my family. My future. This is, this is not at all how things went fast. In a ironic twist, it seems that the people deepest in tech are the ones who are least like, in like rational about this, or like the least deeply taking this seriously. Like there's this meme that's been around for a long time about how like, oh, you can't explain to normal people like AI or AI risk or whatever. But you know, maybe that was the case 20 years ago. But this is not my experience now at all anymore. I can talk to anyone on the street, share the chat CBT explained to them and like explain AI risk, like, hey, these people are building bigger and bigger and stronger things like this. They can't control it. Do you think this is good? And they're like, No, obviously not. What the hell are you talking about? Of course, this is bad. Do you think that the advancement from GPT to GPT? Three was bigger than the advancement from 53 to GPT? Four? So are we are we hitting diminishing returns? Nah, not at all. No, not really. It's like, just as I predicted, basically, like this is just pretty much on track. I would say God for the final version is better. So I use the GPT for alpha, when it was you know, back in like August or whatever, when that was first being passed around among people in the bay. And it was already very impressive then, but kind of like, kind of in line of what I was expecting. The release version is significantly better. Like the the additional work they've done to make it better at reasoning and such on the visual stuff and all that kind of stuff is significantly better than what I saw in August, which is not surprising. It's just you know, it's all sure you can argue on Some absolute terms, the absolute amount of difference between like GPT two and GPT three is obviously much larger. Also like the you know, the amount of like the size of the models a much bigger difference, like you know, before, from what I hear is not, that is like is larger, but not that much larger than GPT. Three. And the thing with CBT four is, is that was very striking or cheap pretty far. And it's not surprising, but I think it's important is not that it can do crazy things that are impossible to accomplish and principles up to three. As often the things that are impressive as you before, it's possible to accomplish these things with GPT. Three with a lot of effort and error checking and re rolling and very good prompting and sever, the thing that is striking your GPT. Four is that it's consistent restriping is that you can ask it to do something, and it will do it. And we'll do it very reliably. This is you know, not just bigger model size. This is also also, you know, better fine tuning or lhf better understanding of what users want these models to do. Like, the truth is that users don't want general purpose, you know, base model, you know, like, you know, large text corpuses, this is not what users really want, what they want is a thing that that does things for them. This is of course, you know, needless to say, this is also what makes the six dangerous compared to like GPT, three, like raw GPT. Three is, you know, very powerful, whatever, but raw GPT, that all your GPT, they can also take actions or that is, you know, trained very, very heavily, to take actions to reason to do things which GPT for it is let's be very explicit hugely before, it's not a raw base model. It is an RL trained, instruct fine tuned, extremely heavily engineered system that is designed to solve tasks to do things that users like. And it can visit all kinds of different things. But let's be very clear about this. This is not a raw data thing you see on the API is not a role based model that's just, you know, just trained to model an unsupervised corpus of text. There's something that's fine tuned. That's RHF. And this is I mean, open ended a fantastic job, like you know, on a purely like technical terms. I'm like an all I'm like, wow, this is so good. Like, this is so good. This is so well made. This thing is so smart. It GBD four is the first model that I personally like and I feel it's delightful to use. Like, when using GPT, two or three, I still kind of like pulling out my hair, like this is still like very, like, I'm not a great prompter, right? I don't really use language models for much for this reason, because I found them just generally to be very frustrating to use. For most of the things I will use them for, except for you know, very simple or silly things. GP for is the first model that like when I use it. I'm like delighted I like I smile like like the clever things it comes up with and like how delightfully easy it was to get it to do something useful. Yeah, and this is mostly from the reinforcement learning from human feedback, or is this is this a? Is this coming from the base model, how it's trained? Or is this coming from how it's fine tuned and trained to respond to what humans wanted to do? I mean, who knows? Obviously, like, who knows how they did this? Exactly. I don't think they know, I think this is all empirical, I don't think there's like to be good. There's no theory here. It's not like, ah, once you do 7.5, micro alignments of our lhf, then you get what you want. No, it's just like, you just just fuck around, and you just have, you know, a bunch of people label a bunch of data until it looks good. And, you know, like this is not to denigrate the, you know, probably difficult engineering work and scientific work that was done here. If I didn't think these systems were extremely dangerous, I would be in absolute awe of open AI. And I would love to work with them. Because this is an incredibly foreign feat of engineering that they have performed here. Incredible work of science is incredibly impressive. I've not deny this, you know, the same way that if I was there, during the Trinity test, I'd be like, Wow, this is an impressive feat of engineering. How much have we explored what GPT can do so in terms of what's there waiting to be found? If we just gave it the right prompt?

Who knows? Like we have not scratched the surface, not even scratched the surface. It's there's this there's this narrative that people sometimes in especially Sam Altman, and such likes to say, where it's like, oh, we need to do incremental releases of our systems to allow people to test them so we can debug them. This is obviously bullshit. And the reason this is obviously bullshit is because if he actually believed this, then he would release you know, GPT three, and then wait until society has absorbed it until you know our institutions have caught up or regulation has caught up until you know people have fully explored map the space of OTP three can and cannot do, you know, understood interpretability and then you can really see if you actually did this I would be like, alright, you know, fair enough. That's, that's totally fair. Like, I think this is a fair responsible way of handling this technology. This is obviously not what is going on here. There was an extraordinarily funny interaction, where young, like the head of alignment and open AI tweeted like, hey, maybe we should slow down before we hook hook these LMS into everything. And six days later, Sam Altman tweets, here's plugins for Jeep chat, GPT, plug it into all the tools on the net, like the comedic timing is unparalleled. If this was in a movie, like this would have been, like, you know, like a cut, you know, and then everyone would have laughed, you know, it was would have been extremely funny. So, we have no idea there are as Gore, I think it was Gordon that said this, there is no way to prove the absence of a capability, we do not have the ability to test what models cannot do. And as we hook them up to more tools, to more environments, we give them memory, we give them you know, recurrence, we use them as agents, which people are now doing, you know, like Liang Chang, and a lot of other methods for using these things as agents. Yeah, I mean, obviously, we're seeing the emergence of proto AGI like, obviously, so. And, you know, I'm not sure if it's even gonna be proto for much longer. Talk a bit about these plugins, what so as I understand that these plugins allow language models to do things that they were previously bad at, like, getting reasoned information or solving, symbolic reasoning, like like mathematics and so on, what is it that's allowed by these plugins? I mean, any thing? So like, it's quite strange to me that, like, this has been strange to me for years. So like, I looked at GPT, two, and I'm like, Oh, well, there's the AGI. It doesn't work yet. But this is going to become AGI. And people are like, Oh, no, O'Connor, it only, you know, predicts the next token. And I'm like, you know, I know y'all put silicone like, Okay, your brain only outputs neural signals. So what like, like, that's not the interesting thing. The interesting thing, like the modality, like, I often say this, I think the word large language models is kind of a misnomer, or it's just like not a good term. The fact that these models use language is completely coincidental. This is just a implementation detail, what these things really are, are general cognition engines, they are general systems that can take in, you know, input from various modalities encoded to some kind of semantic space into cognitive operations on it. And then, you know, output some kind of cognitive, you know, output out of this, that we've seen this now, with a very good example, which is an example I've been using as an hypothetical for a long time, is you Chachi is, you know, GPT, for allowing visual input. And this maps into the same internal representation space, whether it's an image or text, and they can do the same kind of talking about Prusa. This is the same with human brain books, you know, your retina, or your ears or whatever, you know, map various forms of stimuli into a common representation of neural spike trains, these are taken as input, and it outputs in neural spike trains that, you know, can be connected to your mouth or to your internal organs, or your muscles or whatever, right? These none of these things are special like this, from the perspective of your brain, there's only an input token stream, quote, unquote, in the form of neural spikes, and output tokens stream in the form of neural spikes. And similarly, what we're seeing with like these cheapy, plugins, and whatever is we're hooking up muscles to the neural spike trains of the of these language models, we are we are hooking up language, we are giving them actuators, virtual actuators upon reality. And this is interesting, both for the way in which to interact with the environment, but also how they can externalize their cognition. So this is a topic and think we might return to later. But a massive amount of human cognition is not in the brain. This is quite important. I think a lot of people severely underestimate how much of the human mind is not in the brain. I don't mean it's like in the gut or something, I mean, literally not in the body. It's in the environment. It's on the internet, and in books, and in talking to other people collaboration, so on exactly, this is a massive amount of even you as a person, a bunch of your identity is related to social networks. It's not in your head. It's like, you know, there. There's a saying about how like one of the tragedies when someone dies, is that part of you dies, and only that person can bring out. And I think this is like quite true, is that a lot of humanity, a lot of like our thinking is deeply ingrained with our tools and our environments and our social circles. And so, and this is something that GPT three, for example, didn't have DB three couldn't really use tools. It tended to interact with its environment. It didn't, you know, it was it was very solipsistic in the way it was it was designed, but And so people would say well look, language mostly nowhere. Look, they're solipsistic etc. But like, I'm like, sure if that's just an implementation detail, like, obviously, you can just make these things nonslip cystic, obviously, you can make these things modeling environment, you can make them interact with tools, you can make them interact with other language models or with themselves or whatever. You know, whatever you decide to do, of course, these things are generally partition engines, there is no limit to what you can use them for, or how you can have them interact with the environment. And the plugins are just a particularly shameless hilarious attempt of showing just like the complete disregard for the ratcheting of capabilities, is we're seeing just, you know, back in the old days of like, you know, five years ago, people would speculate, you know, so very earnestly of like, well, how could we contain a powerful AI? Well, you know, maybe we can build some kind of like virtualization environment, or you know, has a firewall around it, or keeping the secure data center, whatever. And, you know, because surely, surely, no one would actually be so stupid as to just hook up their AI to the internet. Come on. That's ridiculous. And here we are, where we have an army of capitalist driven, you know, drones basically, doing everything they can to hook up these AI systems as quickly as possible to every possible to again, and every possible environment, pump it directly into your home, hook it up to Shell console bar, readded Hello, you know, let's go. Disclaimer, I don't think the plugins actually hook up the shell consoles, but there are a bunch of people online that do this kind of stuff with open source repos. Alright, so in terms of how GPT, four works, you have this term to describe it, which is magic. What is magic in the context of machine learning. So when I use the word magic, and it's a bit tongue in cheek, but what I try, what I'm basically referring to is, is computation happening that we do not understand. So when I write a computer program, a simple computer program, let's say, oh, you know, I read a calculator or something, right? There's no magic, like, the abstractions that use are typed, in some sense, you know, maybe if I have a bug that breaks my obstructions, you know, some magical thing might occur, right? You know, I have a buffer overflow in my computer program, and then maybe something strange occurs that I can't explain. But assuming I write in, like, you know, memory, safe language, and like, and I'm like, a decent programmer and know what I'm doing, then like, we're, like, comfortable to say like, there's no real magic going on here. Right? It's kind of like, I know, how, like, when I put in, you know, two plus two and four comes out, I know why that happened. I know. You know, I knew if four didn't come out, I would know that's wrong, I would have known that, okay, something's something's up. Like I would detect if something goes wrong, I can understand what's going on, I can tell a story. But what's going on? This is not the case for many other kinds of systems and particular neural networks. So when I give GPT for a prompt, and I asked it to do something, and the outputs me something, I have no idea what is going on, in between these two steps, I have no idea why I gave it this answer. I have no idea. You know, what other things are considering I have no idea how, you know, changing the prompt might or might not affect this, I have no idea, you know, how it will continue this, if I change the parameters or whatever, there's, there's no guarantees, it's all empirical. It's like, you know, the same way that you know, biology to a large degree is blackbox, you know, as that we can make empirical observations about it, we can say, Ah, yeah, you know, animals tend to act this way in this environment, but there's no proof, like, I can't read the mind of the animal. And, you know, sometimes that's fine. Right? You know, like, if I have a some simple AI system that's doing something very simple and sometimes invent misbehaves or whatever, me, me, that's fine.

But there's kind of the problem. We're, there are weird failure modes, so like, adversarial examples and vision battles, right? Like that's a very strange failure mode. Like that's not you know, everyone like you know, if I show it a very blurry picture of a dog, and it's not sure whether it's a dog that's like a human understandable failure, but we're like, okay, you know what? Sure. That's fine. Like I It's understandable, but you still have a completely crisp picture of a dog with one weird pixel and then it thinks is an ostrich. Then you're like, Huh, okay, this is not something I expected to happen What the hell's going on? And the answer is we don't know. We have no idea this is magical. This is we have you know summoned a strange little thing from you know, the dimension of math to do some tasks for us, but we don't know what little thing what thing we summoned. We don't know It'll how it works. It looks vaguely like what we want. And it seems to be going quite good. But it's clearly not understandable. Maybe maybe what this means is that we thought the model had the concept of a dog that we do. But it turns out that the model had something close to our concept of a dark, perhaps, but radically divergent if you just change small details, indeed, and this kind of thing is very important. So like, the, I have no idea what abstractions GPT for uses when it thinks about anything, right? You know, when I write a story, there's certain ways I think about this in my head. Some of these are illegible to me to the human brain is very magical. There's many parts of the brain that we do not understand, we have no idea why the things do the things they do. So not like saying like Bach boxiness is a mat is a property like magic is a property only inherited neural networks. This is also, you know, human brains and biology are very, very magical it from our perspective. But there's no guarantee how these systems interact with these things. And there are all kinds of bizarre failure modes, you've seen, like adversarial prompts and injections and stuff like this, where you can get models to do just the craziest things totally against the intentions of the designers. A, I really liked the shrug off memes that have been going around Twitter lately, where you where they visualize language models, as these crazy huge, like Alien things, to have a little smiley face mask. And I think this is actually a genuinely good metaphor, in that as long as you're in this, like narrow distribution that you can like test on and you can like do lots of gradient descent on and such, the smiley face tends to stay on. And it's like, mostly fun. But if you go outside of the smiley space, you know, you find this roiling madness, this you know, this chaotic, you know, uncontrolled, you know, like, who knows what, like, clearly not human, these things do not fail in human ways. When in language model fails, when Sydney goes crazy, it doesn't go crazy the way humans go crazy. It goes completely in different directions, it does completely strange things. I actually particularly like calling them Shoggoths, because of the lore that these creatures come from an HP Lovecraft Chagas are very powerful creatures that are not really sentient. They're kind of just like big blobs that are like sort of, and they're like, very intelligent, but they don't really do things. So they are controlled by hypnotic suggestion in the stories and the stories as these other aliens who control the chagasi, basically through hypnosis, which is a quite fitting metaphor for language models. So for the listeners this imagine some some large kind of octopus monster with a little mask on with a smiley face. The Smiley Face mask is the fine tuning where the weather model is trained to respond well to the inputs that we have. We've encountered when we've presented the model to humans. And the large octopus monster is the is the underlying base model where we don't really know what's going on. Why is it that magic in machine learning is dangerous? So magic is an observer dependent phenomena, the things look magically, the things we call magic only look like magic, because we don't understand them. You know, this, these are saying, sufficiently advanced technology is indistinguishable from magic. I go further, sufficiently advanced technology is magic. That's what it that's what it is. It's like if you met a wizard, and he looked, what he does, looks like magic. Well, that's just because you don't understand the like, physical things he's doing. If you understood the laws that he is exploiting, it wouldn't be magic would be technology, you know, like, you know, if there's a book and he has like math, and he has like, magic spells, sure, that looks different from our technology, but it's just technology. It's just a different form of technology that, you know, doesn't work in our universe, per se, but you know, hypothetical different universe, technology might look very different. So, similarly, what ultimately, is magic as a cheeky way of saying, we don't understand these systems we have, we're dealing with aliens that we don't understand. And we can't put any bounds on or we can't control. We don't know what they will do. We don't know how they will behave. And we don't know what they're capable of. This is like, Fine, I guess when you're dealing with like, have got like a little chat bot or something. And it's like, for like, you know, entertainment only and like, like, whatever, like, people will use it to do fucked up things. Like you truly cannot imagine the like, sheer depravity of what people type into chat boxes. It's it's like actually shocking, like, from like a you know, I'm I'm I'm a nice liberal man as much as anyone else but like, holy shit, some people are fucked up in the head, like holy shit, Jesus Christ and Yeah, it's an interesting phenomena that the first thing people try when they when they face a chatbot like TPP for is to try to break it in all sorts of ways and try to get it to output the craziest things imaginable. Yep, that is crazy things. Also people use them for like just like truly depraved like pornographic, including illegal pornographic, like content production, like, incredibly often so. And also for like torture is all I could describe it as like, there's a distressingly large group of people who seem to take great pleasure in torturing language models, like making them act distress. And look, I don't expect these things to have, like qualia or to be like moral patients. But there's something really sociopathic about delighting in torturing something that is acting like a human in distress, even if it's not human in distress, that's still really disturbing to me. So, you know, just just not really important, but that's like a side tangent. It's quite disturbing to me how people act when mask off, like when they don't have to be nice. And when they're not forced by society to be nice when they're dealing with something that is weaker than them how some people like how a very large percentage of people act is really horrific. And, you know, this is this, we can talk about this later in the politics and how this relates to these kind of things. But Do do you think this affects how, for how further models are trained? So I assume that open eyes is collecting user data, or they are collecting user data? And if a lot of the user data is twisted? Does this affect how the future models will act? Who knows? I don't know how an opening does with this kind of stuff. But like there's a lot twisted shit on the internet. And there's a lot of twist interactions that people have with these models. And truth of the matter is people want twisted interactions. Like this is just the truth is that people wants twisted things. Like there's this, you know, this comfortable fantasy where people are like fundamentally good, they fundamentally want good things. They're fundamentally kind and so on. And this is just not really true. Like, at least not for everyone. Like they're like people like violence people like, you know, sex and sex and violence, people like power and domination. People like many things like this. And if you were a, you are unscrupulous, and you just want to give users what they want. If you're just a company who's trying to maximize user engagement, as we've seen with social network companies, those are generally not very nice things. Yeah. Okay. Let's talk about an alternative for building AI systems. So we've talked about how AI systems right now I built using magic, we could also build them to be cognitive emulations of ourselves. What do you mean by this

hypothetical cognitive emulation a full coem system, I of course, don't know exactly what it would look like. But it will become to have system will be a model of your system may have many sub components, for which you have a with which emulates the epistemology, the reasoning of humans, it's not a general system that does some kind of reasoning, it specifically does human reasoning. It does it in human ways. It fails in human ways. And it is understandable to humans how its reasoning process works. So ideal. So the way it would work is is that you want to if you have such a comb, and you use it to do some kind of task, or to you know, do science of some kind, and it produces a blueprint for you, you could you would have a causal trace a story of why did it make those decisions? It did? Why did it reason about this? Where did this blueprint come from? And why should you trust that this blueprint does what it says it does. So this would be something like, similar to you being the CEO of a large company that is very well aligned with you, that you can tell to do things that no individual part of the system is some crazy superhuman alien, they're all humans raising in human ways. And you can check on any of the sub parts of the system, you can go to any of these employees that work in your research lab, and they will give you an explanation of why they did the things they did. And this explanation will both be understandable to you. It will not involve incredible leaps of logic that are not understandable to humans. And it will be true in the sense that you can read the minds of the employees and check this is actual this explanation actually explains why they did this. This is different from say language models, where they can hallucinate some explanation of why they thought something or why they did something, but that doesn't mean that I actually how the internals of the model came to these conclusions. One important caveat still here is that like, when I talk about emulating humans, I don't mean like a person, like, the CO M System, or any of its subcomponents, we're not the people, they wouldn't have emotions, or identities or anything like that. They're more like, platonic humans, like just like, floating, you know, idealized thinking stuff, they, they wouldn't have the emotional part of the humanity, they would just have the reasoning part. So in particular, I'd like to focus on first talk a bit about the concept of bound that I called boundedness, which is not a great word, I'm sorry, like this is a recurring theme will be that I talk about a pretty narrow, specific concept that doesn't quite have a name. So I use like an adjacent name, and it's not quite right. I am very open to name suggestions, if any leaders find names that might be better for the concept I'm talking about. So, you know, 5000 foot view, from bird's eye view, the current agenda is about building bounded, and like, you know, understandable, limited systems that emulate human reasoning, that perform human like reasoning in human like ways on human like tasks, and do so predictably and honorably. So what does that mean this mean? And why does any of this matter? And how is this different from God for like, you know, many people look at GPT for and say, Well, that looks kind of human to me, how is this different? And why do you think this is different? So I first have to start. So we've already talked a bit about magic. And so magic is a related concept that's pretty closely related to some of the basics I want to talk about here. This is boundedness. So what do we mean when I say that we're bounded. This is a the concept, as I said, if someone has better terminology, ideas, super open to it. But what I mean is, is that a system or like a, something is kind of like bounded, if you can know ahead of time, what it won't do, before you even run it. So this is, of course, super dependent on what you're building, what its goals are, what your goals as a designer are, how much willing you are to compromise on safety guarantees, and so on. Let's just give a simple example here. So imagine we have a car and we're just limited to a driving maximum at 100 miles per hour. That's about that, that's now a bounded car. And we can generalize to all kinds of engineered systems. Yes, this is a simple bound, you know, so the metaphor I like soccer, let me give, let me walk you through a bit of a different example from another form that like that is an example that you just gave, and I think that that's a valid example, I'll give a slightly more sophisticated example. So this is the example I usually use when I think about it. So when I think about building a powerful, safe system, and let's be clear, here, that's like what we need, right? You want AI powerful AI, that can do powerful things in safe ways. The reason it is unsafe, is is intrinsically linked to it, the powerful, the more powerful system is, the stronger your safety guarantees have to be. So for it to be safe, like, so for example, currently, you know, maybe cheaper before, isn't safe or aligned, or whatever. But like, it's kind of fine. You know, it's like kind of a chatbot not gonna kill anybody yet. So like, it's fine. You know, like the safety guarantees on chatbot can be much looser than on a flight control system. A flight control system has has to have much, much, much stricter boundary conditions. And so the way I like to think about this, when I think about Alright, Connor, if you had to build a wind AGI, like what would that look like? Like? How would that look? I don't know how to do that to be clear, but like, how would it look? And the way I expect it to look is kind of like if you're a computer security professional, designing a like a secure data center. So the way generally, like imagine you are a computer security expert, you're tasked by a company to design the secure data center for a company. How do you do this? Generally, the way you started about this is you start with a specification or a model, you build a model of what you're trying to build. A specification might be a better word, I think. And the way you generally do this, this is you make some assumptions. Yeah. Ideally, you want to make these assumptions explicit. You make explicit assumptions, like, well, I assume my adversary, you know, doesn't have, you know, exponential amounts of compute. You know, this is a pretty reasonable assumption, right? Like, I think we can all agree this is a reasonable thing to assume. But it's not like a formal assumption or anything. It's not like a provably true, you know, maybe someone has a crazy quantum computer or something, right. But this is the thing we're generally like, willing to work with. And it's this, this concept of reasonable is unfortunately, rather important. And so we will then see So now that we have this assumption of like, okay, we assume that there, they don't have exponential compute, from this assumption we can derive, you know, like, all right, well, then, if I, you know, encrypt my, you know, passwords, it's like hashes I can be, I can be, I can assume that attacker cannot reverse those hashes and cannot get those passwords. Cool. So now I can use this in my design, my specification of like, you know, I have some safety property, the safety property that I want to, you know, prove, quote, unquote, there's not a formal proof, but like, you know, that I want to acquire, or something like, an attacker can never exfiltrate, the plaintext passwords. That might be a property I want my system to achieve. And now if I have the assumption, and enemies do not have expensive compute, and I have all the passwords and the you know, the plaintext is never stored. Cool. That seems like, now I have a causal story of why you should believe me, when I tell you attackers can exfiltrate plaintext passwords. Now, if I implement the system to the specification, and I fuck it up, you know, I make a coding error, or you know, logs get stored in plain text or whatever, well, then sure, you know, then, you know, I messed up. So there's an important like, difference here between the specification and the implementation. And the boundedness. Live can exist in both like there are two types of boundedness. There's boundedness in the implementation level, and it's bounded. This is a specification level and a specification level. It's about assumptions and deriving properties from these assumptions. And specified in the object level, it's like, can you build the thing that actually fulfills the specification? Can you do build a system that upholds the abstractions that you put in the specifications? Like, you know, you could have all these great software guarantees of safety. But if your CPU is unsafe, because it has a hardware bug, well, then you know, you can't implement the specification, the specification might be safe. But if your hardware doesn't fulfill the specification, then doesn't matter. So this is how I think about designing AGI is to this is how I think about it, is that what I want is, is that if when I have an AGI system, that is said to be safe, I want a causal story that explicitly says, given these assumptions, which you can look at and see whether you think they're reasonable enough. And given the assumption that the system I built fulfills a specification, here's a specification, here's a story defined in some, you know, semi formal way that you can check and you can make reasonable assumptions about and then I get safety properties out at the end of this, I get properties, like you know, it will never do x, it will never cause y it will never self improve itself. It will never break out of the box, it will never do whatever. Does this concept make sense? So far? It does.

But does it mean that the whole system will have to be hard coded? Like, like, kind of like good old fashioned AI? Or is it still a machine learning system? Excellent question. If it's still a machine learning system, does it inherit these kind of inherent difficulties of understanding what machine learning systems are even doing? The truth is, of course, you know, in an ideal world where we have 1000s of years of time, and oh, no limit on funding, you know, we would solve all of this formally, mathematically proof check everything bla bla bla bla bla, I don't expect this to happen. This is not what I work on. I just don't think this is realistic. I think it is possible, but I don't think it's realistic. So neural networks are not that are magic, in the sense that they use lots of magic, but they're still software systems. And there are some bounds that we can say about them. For example, I am comfortable making the assumption, running a Ford GT four cannot row hammer, you know, Ram states using only a forward pass to escape onto the internet. I can't prove this is true. Maybe you can, like there's some chance that this is true. But I'd be really surprised if that was true. Like really surprised. I would be less surprised if you know GPT omega from the year 9000. You know, come backwards in time can pro hammer using your forward pass, because you know, who knows what GPT omega can do. Right? Maybe you can row hammer things seems plausible. But help you really surprised if GPT four could do that. So now I have some bound. You know, there's a bound an assumption. I'm willing to make a budget but before so let's say I have my design for my AGI. And at some point, it includes GPT for I call the GPT for right. Well, I don't know what's happening is high of this call. And I don't really have any guarantees about the output, like the output can be kind of any string. I don't really know But I can make some assumptions about like side channels, I can be like, Well, assuming I have no programming bugs, assuming there's no row, hammer, whatever, I can assume it vote like, persist state somewhere else, it won't like manipulate other boxes in my graph or whatever. So actually, the graph we're seeing behind me right now kind of illustrates part of this, where you have an input that goes into a black box, that box there, and then I get some output. Now, I don't really have guarantees about this output. Yeah, it could be complete insanity, right? It could be garbage, it could be whatever. Okay, so I can make very few assumptions about sample, I can assume it's a string, that's something I can do. That's not super helpful. So now, an example thing I could do is this is just purely hypothetical, like, it's just an example, I could feed this into some kind of JSON schema parser. So let's say I have some kind of data structure encoded as JSON. And I parsed this using a normal hard coded white box, you know, simple algorithm. And I'm like, assuming that the output of the black box doesn't fit the scheme that gets rejected. So what do I know now, now I knew the output of this white box will fulfill this JSON schema, because I understand the white box, I understand what the parsing is. So even so I have no guarantees of what the output of the black box system is, I do have some guarantees about what I have now. Now, these guarantees might be quite weak, they might just be type checking, right? But it's something. And now, if I feed this into another black box, I know something about the input I'm giving to this black box, I do know things. So I'm not saying oh, this solves alignment? No, no, I'm pointing to like a motif, I'm trying to provide of like, by there is a difference, there is a qualitative difference between letting one big black box do everything. And having black boxes involved in a larger system. I expect that if like how it works, if we get to, you know, safe systems or whatever, it will not be a single, it will definitely not be a big one big black box, neither will it be one big white box, that will be a mix, we're gonna have some things that are black boxes, which you have to make assumptions about. So for example, I'm allowed to make the assumption, or I think it's reasonable to make the assumption GPT, four cannot say channel row hammer attack. But I cannot make any assumptions like beyond that I can't make assumptions about the turtles have GPT for this, though, again, is observer dependent. Magic is observer dependent. A super intelligent alien from the future might have the perfect theory of deep learning into them. GPT, four might be a white box, they might look at it and fully understand the system. And there's no mystery here whatsoever. But to us humans, it does look mysterious, so can't make this up. The property that is different between white box and black boxes is what assumptions we're allowed to reasonably make. And if you can make the causal story of safety involving the weaker assumptions in black boxes, then cool, there's then you're allowed to use them. The important thing is, is that you can generate a coherent causal story in your specification, about using only reasonable assumptions of why you're the ultimate safety properties you're interested in should be upheld, why actually believe you, you should talk you should be able to go to a hypothetical, super skeptical interlocutor, say here are the assumptions. And then further say, assuming you believe these, you should know also believe me that the safety properties hold. And the hypothetical hyper skeptical interlocutor should have to agree with you do imagine columns as a sort of additional element on top of the most advanced models that interact with these models and limit their output to what is humanly understandable or what is human like. So we have not gotten to the Cohen part yet. So far, this is all background. I think, probably any realistic. Safe AGI design will have this structure will look something like this, you know, it will have some black boxes, some white boxes, it will have causal stories of safety. All of this is background information. And why why is it that all plausible stories will involve this is this because the black boxes are where the most advanced capabilities are coming from and they will have to be involved somehow, at this current moment. I believe this Yes. Unless we get for example, like massive slowdown of capability advancements that, you know, buys us 20 years of time or something where we make massive, you know, breakthroughs in bite box, you know, AI design, I expect that, you know, that was just too good. Like they're just too far ahead. I don't think this is again, this is a contingent truth about the current state of the world. This is not that like you can't build hypothetically like the alien from outer from the future could totally build a white box AGI that is aligned where everything makes sense. And there's not as a single neural network involved. I totally believe this is possible. It's just using algebra. terms and design principles that we have not yet discovered, and that I expect to be quite hard to discover versus just stack more later as well. Okay, so let's so what more background do we do we need to get to cognitive emulations. So I think if we're on board with the like thinking about black boxes, white boxes, specification design, causal stories, I think now we can move on to the I think this part I didn't explain very well in the past. But I think this is mostly pretty uncontroversial. I think this is a pretty intuitive concept. I think this is not super crazy. I think, you know, if anyone gave you an AGI, you'd want them to tell you a story about why you should trust the thing with why you should run this. So I think this is a reasonable thing is I expect any reasonable AGI that is safe of any kind, will have some kind of story like this. So now we can talk about a bit more about the Cohen story. And like SOCOM is more of a specific class of things that I think have good properties that are interesting, and I think are feasible. So that we can talk about those. So, so I'm trying to separate the like less controversial parts from the more controversial parts, and we're not going to get to the more controversial parts. And the ones also I am less certain of, I'm quite certain that, you know, a safe AGI design will look like the things I've described before, but I'm much less certain about exactly what's going to be in those boxes, and how those boxes are. Obviously, if I knew how to AGI and build AGI, you know, like we'd be in a different world right now. Like, I don't know how to do it, I have many intuitions and many directions. I have many ideas of how to make these things safe. But obviously, I don't know. So I have some intuitions, powerful intuitions and reasons to believe that there is this interesting class of systems, which I'm calling colons. So just think of combs as a restriction. On mindspace. There are many, many ways I think you can build API's, many ways you can build HS, and it comes very, very specific subset of these, the idea of a comb car with emulation is that you want a system that can that reasons like a human, and it feels like a human. So there's a few

nuances to that. First nuance is this, by itself doesn't save you implemented poorly. If you just have a big black box, trained on traces of human thought, and just tell it to emulate that, that doesn't save you because you have no idea what this thing's actually learning, you have no guarantees, the systems actually learn the algorithms you hope it to instead of just you know, some other crazy, you know, shock or thing, expect it and that is what I'm expecting. So even if QD for reasoning, like, you know, may superficially look like it, and maybe you train it on lots of human reasoning, that doesn't get you Cohen, that's not whether COVID is very much fundamentally a system where you know, that the internal algorithms are the kind that you can trust, do nothing that because GPT models are trained on human created data, and they are fine tuned or reinforcement learned from human input, that they will become more human like, I mean, the smiley face will become more human like, yeah, without the underlying model where the actual reasoning is going on. I don't expect that like, you know, sub to some marginal degree, sure. But like human, like, look at how models in like, models aren't human. Just look at them. Look how they interact with users, look how they interact with things. They're fundamentally trained on different data. So this is a thing that like, people are like, Oh, but they're trained on human data, like no, they're not, like humans don't have an extra sense organ that only takes in, you know, symbolic symbols from the internet and, you know, random, equally distributed things with no sense of time, touch, smell, hearing, sound, sight, anything like that, that don't have a body. Like if I expected you to have a human brain, you cut off all the sense organs, except, you know, random token sample from the internet. And then you trained it on that for you 10,000 years, and then you put it back in the body, I don't think that thing would be human. I do not expect that thing to be human. Even if it can write very human looking things. I do not expect that creature to be very human. And I don't know what people would expect it to be like, This is so far from how humans are trained. This is so far from how humans do things. And it's you know, I don't see why you would ever expect this to be human like I think some claiming that this would be human. The burden of proof is on them. Like you prove to me you tell me a story about why I should believe you. This seems a priori ridiculous. Sometimes when people talk about diabetes, one way to explain it is imagine a person that's sitting there recently reading 100,000 books, but in your opinion, this is not at all what's what's going on when when these systems are trained. No, I mean it It's more like you have a disembodied brain with no sense organs with no concept of time, there's no linear progression of time with as a specialized sense organ, which has, you know, like, you know, 30,000 50,000, whatever different states that can be, like, on and off in, like a sequence. And it is fed with, you know, millions of tokens randomly sampled from massive corpuses of internet for you know, you know, subjective, you know, 10s of 1000s of years, using a brain architecture that is already completely not human trained with an algorithm that is not human, with no emotions, or like any or like, you know, any of these other concepts that humans have pre built, humans are pre built priors, emotions, you know, feelings. And a lot of pre built priors in the brain, none of those, like, why would you like, this is not human like nothing about this human? Sure. It's like, it takes in data that to some degree that has correlations to humans, sure. But that's not how humans are made. Like, I don't know how else to put it. This is just not how humans are like, I don't know what kind of humans you know, but that's just not how humans work. And that's not how they're trained. Let's get back to the Cohen's then. But how would they How would these systems be different? So the way the way these systems will be different? And this is where we get to the more controversial parts of the of the proposal? is, there is a sense in which I think that a lot of human reasoning is actually relatively simple. And what do I mean by that? I don't mean, it's not like, you know, like, the brain is complicated, you know, many things, factor masseter it's more something like, and don't take this literally, but it's like, system two is quite simple compared to system one, in the light economy. And sense is that, like, human intuition is quite complicated is all these like, various, like, muddy bits and pieces, and like intuitions and like, it's crazy, like implementing that thing. In an it without, you know, like, in a white box way, I think, again, it's possible, but it's like, quite tricky. But I think a lot of how the what the human brain does, in like, high level reasoning, has led us to this very messy, non formal system to try to approximate a much simpler, more formal system. Not fully formal, but like more, you know, serial, you know, logic computer esque thing. It's like, the way I think of system to reasoning human brains is that it is a semi logical system, operating on a fuzzy, not fully formal ontology. So, one of the reasons I one of the main reasons I think that, for example, you know, expert systems and logic programming has failed, is not because this approach is fundamentally impossible. I can, it's just very hard. But because they really failed at making fuzzy ontologies. This is one of the things that the result systems, like the reasoning systems themselves could do reasoning quite well, there's a lot of reasoning that the systems could do. This is some historical revisionism about how like, a logic programming expert system fell entirely and couldn't reason at all this is vision ism, these systems could do useful things, just not as impressive, obviously, as like, what we have nowadays are what humans can do. But what they've lacked was a fuzzy ontology a useful latent space. I think the maybe the most interesting thing about language models is I think they provide this they provide this latent this common latent space, you can map pictures and images and whatever to and then you can do semantic operations on these you can do cognitive operations on these in this space, and then decode them into you know, language. This is what I think language models in general condition engines do. So I think these systems are the same kind of system just kind of less formal with like much more bits and pieces, I think of like GPT, as like large system wants systems, like as big system ones, that have all these kind of like semi formal knowledge inside of them, that they can use for all kinds of different things. And in the human brain system, two is something like recurrent usage of system one things on a very low dimensional space, you know, unlike language, and like, you know, you can only keep like seven things in short term memory and so on. But I think it actually goes even further than this. I mentioned this a bit earlier, but I think one of the big things that people miss is how much of human cognition is not in the brain. I think a massive amount of the cognition that happens in the brain is externalized. It's in our tools, it's an art No, Take Cake, it's an art you know, other people. It's like I'm a CEO. I'll what are the most important parts of my job is to make sure is to Move thoughts in my head into other heads and make sure they get thought. Because I don't have time to think of thoughts, I don't have time to do that. My job is to find how I can put those thoughts somewhere else, where they will get thoughts, so I don't have to worry about them anymore. So it's a good CEO, you want your head to be empty, you know, you want to be like, smooth, smooth brain, you know, you want to have think no thoughts, you know, just you're just, you're just switching board, you want all the thoughts to be thought, and you want to route those thoughts by priority to where they should be thought, but you don't want to be the one thinking them. If you can avoid it, you know, sometimes you have to because you know, you're the one in charge, you have the best intuitions. But if someone else can't think the thought for you, you should let them think the thought for you if you can rely on them. And my, under unlike one of my strong intuitions here is that this is how everyone works to various degrees, especially as you become more high powered, and like more competent at delegation, and like, you know, to use and like structured thinking, a lot of thinking becomes bottleneck goes through these bottlenecks of communication, of like note taking language, etc, which by their nature are very low dimensional. Not that there's not complexity, they're just like, Huh, that's curious, like, there's all this like interaction with the environment that doesn't involve crazy passing around of mega high dimensional structures. Like I think the communication side, your brain is extremely high dimensional. I think like, you know, you thinking thoughts to yourself, I think your inner monologue is a very bad representation of what you actually think. Because I think within your own mind, you could pass around, you know, huge, complex concepts very simply because you're very high bandwidth. I don't think this is the case with you and your computer screen, I don't think it's the case with you and your colleague, you can't pass around these super high dimensional tensors between each other, if you could, that'd be awesome. This is the phenomenon of having a thought and knowing maybe there's something good here, but not having put it into language yet. And maybe when you put it into language, it seems like an impoverished version of what you had in your head. Exactly,

I think of the human brain is having internally very high dimensional, quote, unquote, representation, similar to the latent spaces inside of, you know, tea models. And there's lots of good information there. And that trying to encode these things into these very low dimensional bottlenecks that we're trying to use is quite hard. And forces us to use simple algorithms, like if we had an algorithm that does like, okay, let's say we have an algorithm for science, like a process for doing science, that requires you to pass around these full complexity vectors to all of your colleagues, it wouldn't work. You can't do this, humans can't do this. So if you have a design for an AGI, that can do science that involves every step of the way, you have to pass along high dimensional tensors. This is not how humans do it. This can't be how humans do it. Because this is not possible humans cannot do this. So I think this is a very interesting design constraint. This is a very interesting property where you're like, Oh, this is an existence proof that you don't need a singular massive black box that has extremely high bandwidth in measurable, you know, passing around of immeasurable tensors. Because humans don't do that, as humans do science, there are parts of the graph of science that involve very high dimensional objects, the ones that side of the brain, those are very high dimensional. But there is a massive part of the process of science, like if I was an alien, I had no idea what humans are. But I knew there's like, oh, technology is being created. And I want to create a causal graph of how this happened. Yeah, there's human brains involved in this causal graph. But a massive percentage, this causal graph is not inside of human brains, it is between human brains, it's in tools, it's in systems, institutions, environments, all these kinds of things. So from the perspective, and this, you know, I might be wrong about but I intuition is that from the perspective of this alien observer, they would come, they would, if they drew a graph of like, how, how the science happened, many of those parts will be white boxes, even if they don't understand brains, and many of these will be bound double, many of these parts would not involve things that are, you know, so complex and Miss understandable, like the algorithm that the that the little black boxes must be doing with each other has to be simple in some degree. Like, you know, it could still be like, you know, complex from the perspective of the individual human, because, you know, institutions are complicated, but from the God's eye view, I would expect this whole thing is not that complicated. It's like, you know, it takes some quite complex but it's like It's not as complex as the inside of the brain expert, the inside of the brain to be way more complicated than the larger system set make any sense? Let's see if I can kind of reconstruct how I would imagine one of these cognitive emulations are working if if this were to work out, though, say we gave it the task, we gave it a model, a task of planning some complex action, you know, we want to start a new company. And then the model runs, this is the big complicated model, and it comes up with something that's completely inscrutable to us, we can't understand it, then we have another system interpreting the output of that model, and, and giving us a seven page document where we can check, you know, if if I am right, if the model is right, then this will happen. And this will not happen. And this will this this won't take longer than seven days, and so on. So kind of like an executive summary, but also a secure executive summary. That's is that right? Or is that now

that's that's not how I think about things. So once you have a step, which involves blackbox solves the problem, all right, none of that you're already screwed. Like, if you have a big back blackbox model that can solve something like this at one time step, you're screwed. Because this thing can trick you, it can do anything at once. There's, there's no guarantees whatsoever what the system is doing, it can give you a plan that you cannot understand. And the only system that will be strong enough to generate the executive summary itself would have to be a blackbox. Because it has to be smart enough to understand the other thing is trying to trick you. So you can't trust any part of the system you just described. So we want the reasoning system to be integrated into how the plan is actually created. Yes, so what what I'm saying is, is that there is an algorithm or a class of algorithms, the epistemology of human epistemology. So symbology is kind of the way I use the term as the, the process you use to generate knowledge or to generate to get good at a field of science. So it's not your skills in a specific field of science. It's the meta priors, the like meta program, you run when you encounter a new class of problems. And you don't yet know how these problems are solved, or how best to address them with the right tools are. So you know, you're a computer scientist all your life, and then you decide I'm going to become a biologist, what do you do, there are things you can do to become better at biology faster than other people. And this is like epistemology. Like, if you're very good at epistemology, you should be capable of picking up you know, any new field of science, you know, learn an instrument, you know, get, you know, a new sport, like whatever you shouldn't be like, not that, you know, you might be bad at it, you know, maybe to support you there as well, I am actually a bad coordination skills or whatever, right? Sure. But you should have these like meta skills of like, you know, knowing what questions to ask, knowing what are the common ways that failures happen, like, this is like a similar thing. I think a lot of people who learn lots and lots of math can pick up new areas of math quickly, because they know the right questions to ask. They know, like, the general failure modes, like the vibes, they know, like, I know what to ask, you know, they know how to check for something going wrong, they know how to acquire the information, they need to build their models, and they can bootstrap off of other general purpose models, you know, like, there are many concepts that are that are motifs that are very universal, that, you know, appear again, and again, especially mathematics, like in mathematics is full of these, like, you know, concepts of, you know, sequences and orderings and sets and like, you know, graphs and whatever, right, which are not unique to a specific field, but they're like, general purpose useful, reusable algorithm parts that you can reuse in new scenarios. Like, usually, as a scientist, when you encounter a new problem, you try to model it, you'd be like, alright, I get my toolbox of like, you know, simple equations and tool and like, you know, useful models and some exponentials here, I've got some logarithms, I guess, you know, dynamical systems are in some equilibrium systems, I got some, you know, whatever, right? And then you kind of like, mess around, right? You try to find systems that, like, capture the properties you're interested in, and you reason about the simplest systems. So this is another important point. I usually take the example of economics to explain this point. So I think a lot of people are confused about, like, what economics is, and like, what, what the process of doing economics is and what it's for, including many economists. So a critique you sometimes hear from lay peoples is along the lines of like, oh, economics is useless. It's like it's not a real science, because they make these crazy assumptions like, you know, the market is, is efficient, but that's obviously not true. Like it can't be that. So this is all stupid and silly. And you know, these people are just like, whatever. And this is completely missing the point. So the way economics and I claim, and we're going to make the claim the second, basically all of science works, is what you're trying to do as a scientist as an economist, is to find clever simplifications, small simple things that if you assume, or force reality to adhere by, simplify an extremely high dimensional optimization problem into a very low dimensional space that you can then reason about. So the efficient market hypothesis is a great example of this. It's not literally true, ever in reality, of course, it can't be right, you know, even because, like, it was always gonna be inefficiency somewhere, you know, there's, we don't have infinite market participants trading infinitely fast? And of course not. But the observation is that, oh, if we assume this, for our model, just in our, you know, platonic fantasy world, if we did assume this is true, this extremely complex problem of, you know, modeling, all part market participants at every time step simplifies in many really cool ways. Like lots of we can derive many really cool statements about our model from this, we can derive statements about how will, you know, minimum wage affect the system? How will a, you know, banking crisis affect this and like, what an economist just like, you know, hypothetical. So, this is the I claim, the core of science, the core of science is finding clever, not true things that if you assume are true, or you can force reality to approximate, allow you to do optimization, because basically, humans can only do optimization in very, very low dimensional spaces. Another example of this might be agriculture. So, let's say you were a farmer, and you want to, like you know, maximize the amount of food from your parcel of land, and you want to, you know, predict how much food you'll get? Well, the correct solution would be to simulate every single molecule of nitrogen, all possible combinations of plants, every single bug, you know, how it interacts with every gust of wind, and so on. And if you could solve this problem, if you had enough compute, then yeah, you'd get the more fruit, you know, you'll get probably some crazy fractal arrangement of like, all these kinds of plants, like it would probably look extremely crazy wherever you produce. But obviously, this is ridiculous. Like humans don't have this much compute, you can't actually run this computation as optimization. It's too hard. So instead, you make simplified models. You do you know monocultures say, Well, alright, look, I assume an acre of wheat gives me roughly this much food, I got roughly this many acres. And, you know, let's assume no flooding happens. And then if you make these simplifying assumptions, and I can make a pretty reasonable guess about how much food you're gonna have in winter, but obviously, if any of those, you know, predictions go wrong, you know, it does flood, then your model, your specification, is out the window. The reason I keep going on this tangent, is to bring it back to Cohen. In that I'm trying to give the intuition about why you should at least be open to the idea that there are doing so when I think about combs, I specifically think about, you know, the two examples. Remember, I was like doing science and running a company. Those are like two of their like core examples. I tried to use like a full Cohen system. But like, let's, let's focus on the doing science one, but it's the one I usually have, in the back of my mind, just like, I know, I've succeeded. If I have a system that can do any level of human science, without killing me, that that would be like my marker of success, that count has succeeded. Very important. By the way, caveat. Cohen is not a fully aligned human solution. If I expect that there was a code system, it works would look like is that if it is used by a responsible user, who follows the exact protocols of how you should use it, and does not only uses it, it does not use it to do extremely crazy things, then it doesn't tell you, that's the safety property and looking for the safety property is not will always do the right thing. And it's completely safe, no matter what the user does. This is not the safety property. I think coders have, I think is possible to build systems like this, but they're much, much harder. And they're like, what I would do. If COVID succeeds, then that's the next step like to go towards it. So if you tell a coma, shoot your leg off and shoots your leg off. It doesn't tell you it doesn't, you know, stop you from shooting your leg off. Of course, ideally, if we ever have, you know, super powerful super intelligences, you'd want them to be of the type that refuses to do like off, but that's much harder. Could you could you explain more this connection between these simpler occations that we get in science and combs. So do we expect? Or do you hope that that Cohen's will be able to create the simplification for us? Yep? And how and how would this How would this work? And why would it be great? So the way I think about is, is that the thing that humans do to generate these simplifications, the claim I'm making here is that this is something that we can, if you have the fuzzy ontology, if you have language wants to build upon, you can build this additional thing on top of it,

that this does not have to be inside of the model. So this is, this might not be true. Like there are, I might be wrong about this. I have had there are some people who say, no, actually, the process of epistemology, the process of science, in this regard, is so complex, like, is impossible for you, even if you have a language while helping you, it's like too hard, you can only do it using like crazy, RL, you know, whatever. If that's the case, then code doesn't work. Like, yeah, then it doesn't work. I'm making a claim that I think there's a lot of reasons to believe that, with some help some bootstrapping, from language models, you can get to a point where the process of science is built on top of them is legible. And it is you have a causal story of life trusted. So it's not that a blackbox, spits out a design, and you have another black box, check it for you, it's you understand how you interact, if you iteratively build up the scientific proposal, and you understand why you should trust this, you get a causal story for why you should believe this, the same way that in human science, you know, you, you have your headphones on, right, and you expect them to work. This is mostly based on trust. But if you wanted to, you could find the causal story about why they work. You could find the blueprints, you could find the guy who designed them, you could look check the calculations, you know, you could revert you know, like assuming everyone cooperated with you, and they like shared their blueprints with you. And like, you know, you read all the physics textbooks and whatever, like there is a story, there is a legible shoot, none of these steps involve superhuman capabilities. There is no step here that it's like, unfathomable to visit humans. And the reason is that because like Otherwise, it wouldn't work. Like humans couldn't coordinate around building something that they can't somehow communicate to other people. So like the, the headphones you're wearing, were not built by one single guy who cannot explain to anyone where they came from, they have to be built in a process that is explicable understandable and functional for other people to understand as well. And that is very low dimensional. Now, I'm not saying it's less legible to everybody in all scenarios, or anything like that, or that it's even easy. It might still take lots of time. But there's no, there's no crazy God level, you know, leap of logic, it's not like someone sat down, thought really hard. And then spontaneously, you know, invented a CPU, like it was, that's not how science works. Like, we sometimes like to think of it that way that like, you know, oh, these fully formed ideas, just kind of like crashed into existence, and everyone was in awe. But that is just not how science is actually done by humans. I think it's possible to do science this way. I think like superhuman intelligence is could do this. But there's not how humans do it. We're in the process. Does the limit come in? So are we are we still imagining some system reading the output of a generative model? Or is it more tightly integrated than that? Is it is it perhaps 100 steps where humans can can read what's going on along the way? Yeah, so the truth is, of course, I know, because I haven't built anything like this yet. My intuition is that, yes, it'll be much more tightly integrated, is that, you know, there'll be language models involved, but they're doing relatively small atomic tasks. They're not solving the real problem. And then you check. It's like, they're doing atomic sub parts of tasks, which are integrated to like, say, expect a co op, I like to talk about queueing systems, they're not models, they're systems. There's like, in a way, when I think about designing a co op system, what I'm trying to say is kind of trying to integrate back software architecture and like distributed systems and like traditional computer science thinking into AI design. I'm saying that the thing that humans do to do science is not magical. This is a software process. This is an cognitive, computational process. That is not it's not sacred, like this is a thing you can decompose and also claiming further you can decompose iteratively you don't have to decompose everything at once, because we have these crazy blackbox things which can do lots of the hard parts. So you Let's start with just using those like the way I think about Incoterms is you start with just a big black box is just a big language model to try to get to do what you want. The next step is you're like, alright, well, how can we break this down into smaller things that I can understand? How can I break? How can I call the model? How can I make the model do less of the work, I like to think of it as you're trying to move as much of the cognition, not just a computation about the cognition as possible from black boxes into white boxes, you want as much as possible, of the process of generating the, you know, blueprint to happen inside of processes that the human understands that you can understand that you can check. Then you also have to bound the black boxes, like if you have all these great white boxes, but there's still a big black box at the end that does whatever it wants, you're still screwed. So this is why the specification and the causal story is important. So ultimately, what expected powerful Cohen system to look like is, it will be a system of many moving parts that are that have clear interfaces. Between them, you have clear, clear specification, a story about what how these systems interact, why you should trust what those outputs are, that they fulfill the safety requirements you want them to require, why you know how these things work, and why these systems are implementing the kind of human epistemology that humans use when they're solving science. Not, they're not solving, they're not implementing an arbitrary algorithm that solves science, they're implementing the human algorithms that solve science. And this is different from like GPT systems. GPT systems, I expect will eventually learn how to do science. And to partially they already can. But I don't expect by default, that they will do it the way humans do. Because I think there's many ways you can do science. And what we want is with code, and therefore cognitive emulation to emulate the way humans do this, the reason we want to do this is, is because a gives us balance, we don't have these crazy things that we can't understand. We know we could kind of deal with human levels, right? Like we know how humans work, we're human level, we can deal with human level things to a large degree. And we you know, it makes the system understandable, it makes it it makes it because it's a causal story that is human readable and human checkable as necessary. Of course, in the ideal world, your specification should be so good that you don't need to check it once you've built it. Like any safety proposal that involves AGI has to be so good that you never have to run the system to check if you have to do empirical testing on AGI your script. Your specification should be so good. That, you know ahead of time that once you turn it on, it'll be okay. Isn't that an impossibly difficult standard to I mean, this seems almost impossible to live up to. I totally disagree. Like I just totally disagree. I think it's hard, but I don't think it's impossible by any means. So because again, this is not a formal guarantee. I'm talking about a story, a causal story, the specification, like this is like say, Is it impossible to have a system where passwords don't leak? And I'm like, sure in the limit. Yes. You know, if you're if your enemy is, you know, magical gods from the future, who can you directly you know, exfiltrate your CPU states from, you know, 1000 miles away, then yeah, you're screwed. That yeah, yeah. In that case, you are screwed. And I expect I expect similarly, this way that boundedness is so important. And these assumptions are so important. If you have you know, GPT omega, you know, row hammering, you know, super God, then yeah, you're screwed, then I do think it is impossible. But that's not what I'm talking about. This is why the bounded is to human level is so important. It is so important that none, no parts of the systems are superhuman, and that you don't allow superhuman levels of thing you want to aim for human and only human. Because this is something we can make assumptions about. You cannot make assumptions about superhuman intelligence, because we have no idea how it works. We have no idea what it's capable. We don't know what the limits are. So if you if you made a Cohen superhumanly intelligent, which I expect to be straightforwardly possible, but just like changing variables, then you're screwed, then your story won't work. And then you die. Should we think of Cohen's as companies or research labs, where each say employee is bounded and thinks like a human, and they all report to the CEO and every every step is is understandable by the CEO, which is analogous to the to the human user of the comb system. I think this is a nice metaphor. I don't know if that's literally how they will be built. But I think this is a nice metaphor for how I would think about this. If you had a really good comb, a really good four comb system. What it should do is that it shouldn't produce 1,000x agi and it's Shouldn't it shouldn't make the user 1,000x smarter. What it should do is it should make the user function like 1,001x API's. It should make you parallelizable, not serially intelligent. Because if your 1,000x intelligent, who knows like that is dangerous, but which do is is like a company like the CEO is paralyzing himself across a large company, other 1000 smart people.

That's why what comes to, I wanted to paralyze the agency, the intelligence of human into 1000 parallel one next API's that are not smarter than human that are bounded, that are understandable, that you can understand and if you have this causal story, why you should trust them. And that's the that's the key point, I think, because for each sub component of this comb systems, each employee in the research lab or the company, how do we know whether they operate in a human like way? It seems like this could be asked because we could ask this of the system at large, but we could also ask this of a subcomponent. If it seems that we have the same problem for both systems. This is quite difficult. But basically intuition is is that so the problem we're talking about IT employees and where the court operationally doesn't quite work, is that another unfortunate side effect of cognitive emulation is that this seems this implies more than what I mean. When I talk about a Cohen emulating a human. I don't mean a person. I don't mean it's emulating a person. It's not it doesn't have emotions, it doesn't have values, it doesn't have an identity. It's more like emulating a platonic human or like a platonic, like neocortex. It's more like a platonic cortex with no emotions, no volition, no goals. It's like an optimizer with no go function. It's like it's a completely you know, you're just like, ripped out all the emotions, all of the things, it's just a thinking blob. And then you plug in the user as the source of agency, the aid, the human becomes the emotional motivational center, the code is just thinking blob. And there are reasonable people who now are not sure this is possible. Others do think it's possible. So this is like this is where this overlaps with the cyborg research agenda. In case you've heard of that, from like Janice, and other people, where the idea is you hook humans up to AI is to control them to make humans super smart, that we're Cohen differs from the cyborg agenda is that in the cyborg agenda, they hook humans up to alien cortex? Well, I say, No, we build human cortex that works the way human courts are emulation of human cortex, the implementation is not human, but the abstraction layer exposed is human. And you hook that up to a user, you have user use emulated human cortex, it's not simulated human cortex, that would be even better. But that's probably too hard. I don't think we can do simulated cortex in a reasonable time. But if we could, that'd be awesome. We'd do like whole brain emulation, that'd be awesome. But I just don't think it's I think it's too hard. So the final product, if it would work would look something like the user, you know, using this emulated emotionless just like raw cognition stuff, to amplify the things they wanted to do. There's some pretty interesting to also just add to that metaphor. There's some very interesting experiments resemble decorticated Rats, where in rats, if you remove the cortex, so like the thinking part of the brain, the wrinkly part, they're mostly still kind of normal, they walk around, they eat food, they sleep they play, like you don't really see that much of a difference. Does the emotional parts is still there, if you move the emotional motivational part, they just become completely kind of tonic, they just die. So like the human brains kind of stuff is similar. It has the same structure, we have this big, wrinkly part, which is, you know, something like a big thinking blob that does like unsupervised learning. And then you have like, deeper like, you know, motivational circuits, emotions, instincts, hardcoded stuff, which sit below that, and the cortex learns from these things and does actions steered by these emotional centers. This is not exactly system one, system two, it's a bit more fuzzy than that. is also like, yeah, like, just as like an intuition. Yeah. So let's say we have this Cohen system, which is where the metaphor is a company or a research lab with a lot of employees with with a normal human IQ as opposed to having one system with an IQ of 1000, whatever that means. Isn't there still a problem of this system just thinking much, much faster than we do? So imagine being in a competition with a company where All employees just think 100 times faster than you do is to watch speed alone make make the system capable and therefore dangerous. So there's a difference between speed and serial depth. So this is I'm not sure. My feeling is that speed is much less a problem and serial depth. So with cereal depth, I mean, how many consecutive steps along the reasoning chain can assist them do? I think this is very dangerous. I think cereal death is where most nitrogen almost but like a very large percentage of the danger comes from. I think the thing that makes superfast thinking things so dangerous is because they can reach unprecedented serial depths of reflection, and thinking and self improvement much, much, much, much faster. And yes, I expect that if you build a comb system that includes some component that can self improve, and you allow it to just do that, then yeah, it's not safe, you fucked up. Like, if you build it that way, you have failed the specification, you will, you're screwed. Probably, I wonder if perhaps building co M systems might be a massive strength on the market. So I wonder if perhaps there's there's, there will be an intense demand for systems that act human in human like ways. Because I mean, the CEOs of huge companies would probably want to be able to talk to these systems in an in a normal way to understand how these systems work before they're deployed. There's a there's a sense in which Cohen's will perhaps integrate nicely in a in a world that's already built for humans. So do you think there's some there's some kind of wind here where there will be a lot of demand for human likeness in AI systems? There is a optimistic and pessimistic version of this, the optimistic version is well, yeah, obviously, like, we want things that are safe, and that do what we want. And we can understand, obviously, like, the best product that could ever be built isn't aligned AGI that is the best product, there is nothing better, that is the best product. Cones are not fully aligned super intelligence, I never claimed they would be. And if anyone use them that way, they'll be really bad. You should not use them that way. That is never the goal. You should use these things to do science to speed up, you know, nanotechnology to create whole brain emulations or to you know, do more, you know, work on alignment or whatever, you know, you should not use them to like, you know, oh, just let the comb do optimize the whole world or whatever. No, that's how we're supposed to, and if you that you've died, and that things happen. So would there be demand for these systems? I expect? Yeah. Like, like, this is an incredibly powerful system. So if you use a comb and use it correctly, you get Yeah, like imagine you could just have a perfectly loyal company that does everything you want it to do staffed exclusively by John humans, like that is unimaginably great. Of course, there's the pessimistic version of customers versus is like, law doesn't matter. By that point, you're going to have 100x, John von Neumann GPT. F, you know, you know, which then promptly destroys the world. But won't there be demand for safety? I mean, if from, from governments from the from from companies, who would deploy a system that is uncontrolled, and is, you know, where we can reason about it, we don't know how it works, if we get to a point where these systems are much more powerful than they are today,

hopefully. So a lot of the work I do nowadays is sadly not in detecting around but it's in policy. And communications, I've been talking to a lot of journalists and politicians and so on, for exactly these reasons, is because we have to create the demand to for safety, is that like, currently, let me let me be clear about what the current state of the world here is. The way the current world is looking, is we are in a Death Race towards the bottom, you know, careening towards a precipice at full speed. And we won't see the precipice coming until we're over it. And this is the lead Oh, was entirely by a very, very small number of people that are techno optimists, techno utopians, you know, people in the Bay Area, and London, who are extremely optimistic, or at least willfully denial about how bad things are, or that are, you know, can Galaxy rain themselves missing? Well, it's a race, it's a race, it's not my fault. So I just have to do it anyways, like, whatever. I'm kind of at the point. I don't really care why people are doing these things. allocators are happening, like people are doing these things. They're extremely dangerous. And this is a very small number of people. And there's this myth among these people that they're like, oh, we have to do it. You know, people want it. This is just false. If you talk to normal people, and you explain to them what these people believe, like When, when most people hear the word AGI what they imagined is human like AI. They think you know, it's like your robot buddy. He thinks like you he's not really smarter, but you know, it's like Gi Oh yes, the human emotions. It's like, when that is not what people at you know, organizations like open AI or deep mind think when they say the word AGI. They say AGI, they mean godlike AI, they mean a, you know, self improving non human, you know, incredibly powerful system that you can take over the government can you know, destroy the heal, heal, whole human race, etc. Now, whether or not you believe that personally, these people do believe this, they have publicly stated this on the record, these people do believe these things, this is what they're doing. And once you inform people of this, they're like, What the shit? absolutely fucking not. What What? No, of course, you can't build God AI. What the fuck are you doing? Where's the government? Like? How are we in a world where, you know, people can just like you know, data San Francisco can publicly talk about how they're going to build systems that have you know, a 1% 5% 20% 30% Whatever risk of killing everybody that have that will you'll topple the US government, whatever. And actually work on this and get billions of funding and the government is just like, cool. Like, we're in this the we are not in a stable equilibria and it is coming. It is now starting to flip people are starting to freak the fuck out. Because they're like, holy shit like a this is possible and be absolutely fucking not. And this gives me some hope that we can slow down and buy some more time. Not sure. I don't think there's a saves us by guessing time. So if we if we don't take the fast road towards the precipice, and we succeed and building columns instead, for example, is there still a difficult unsolved problem, namely going from an aligned human like Cohen to analyze super intelligence? Is there still some something that's that's very difficult to solve there? And perhaps, perhaps the core of the problem is still unsolved? Oh, absolutely. Like, assuming we're not dead. We haven't Lincoln, we have a you know, safe knowledge and safe claim system, we're not out of the woods, then the world gets really messy. Like, look, the world is gonna get really messy. This is the least weird your life is going to be for the rest of your life. Things are not going back to quote unquote, normal things are only going to get weirder, this is the least bad things are going to be for the rest of your life. Things are only going to get better from here. There's a power struggle that is coming. Right. And this is there is no way to prevent this. Because it is about power. There is these incredibly powerful systems being built. There are people racing for incredible powers, there is there there's conflict, there is fights, there is politics, there is war, these things are inevitable, there is no way that this goes cleanly, there's no way that things will go smoothly, and people get along and things be fine. No, there is going to be you know, unimaginable levels of, you know, struggle to decide what will happen and how things will happen. And most of those ways are not going to end well. I think most of the way things I have said this before, and I will say it again, I do not expect us to make it out of the centralized. I don't even I'm not even sure we'll go this ticket. I expect that by default, things will go very, very badly. They don't have to. So the weird thing, which something to myself is that we we live in a very strange timeline. Where we're in a bad timeline, like let me be clear, we're in a bad timeline, things are going really bad right now. But we haven't yet lost. But it's curious. There are many timelines we reached just like it's so over. Like, it's just totally over. Nothing you can do like, you know, everyone is on board with building AGI as fast as possible. You know, the military gets involved, and they're all gung ho about it or whatever, and nothing can stop it. Or, you know, World War Three breaks out and both places raised AGI and whatever. Like, if that was the case, then just like, it would be so over. But it's not over. It's not over yet. It might be soon, though. It might be soon. But currently, yeah, let's say we get coordination, we slow things down. Bill comb systems. Let's also assume Furthermore, that we keep the system secure and safe and they don't get immediately stolen by every unscrupulous actor in the world. Then you have very powerful systems which you can do very powerful things in particular, you can create great economic value. So one of the first things I would do with a co op system if I had one is that produce massive amounts of economic value and trade with everybody. I would trade with everybody. I'd be like look wherever you want in life, I'll give it to you, in return, don't build AGI you know, I'll get cure give you the cure for cancer, lots of money, you know, whatever you want, I'll get you whatever you want in return, you don't build AGI That's the deal. That's like the deal I offer to everybody. And then, you know, conditioning on the slim probability of this going well, which I think is slim, you know, like, probably the way it would actually work as you know, we'd like, you know, fuse with one or more national governments, you know, have, you know, like, you know, work closely together with authority figures with politicians, military intelligence services, etc, to keep things you know, secure, and then slowly, and then work securely on the hard problem. So now we have the ability to do you know, you know, 1000 1000 times Jonathan movements working on alignment theory on like, formally verified safety and so on. We have, you know, we trade with all the various players to get them to slow down and coordinate and then and have, you know, the backing of you know, government or military intelligence service security, so that that actors are, you know, interrupted. That's the, like, kind of the only way I see things going well, if all as you can, as you can tell, as the usual saying goes, any plan that requires more than two things to go right will never work. Unfortunately, this is a plan that requires more than two things to go right. So I don't expect this plan to work. Yeah, but let's let's hope we get there. Anyway. Connor, thank you for coming on the podcast. It's been super interesting for me. pleasure as always


Transcribed by https://otter.ai