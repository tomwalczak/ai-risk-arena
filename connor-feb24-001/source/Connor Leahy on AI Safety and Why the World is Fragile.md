Welcome to the future of Life Institute podcast. My name is Gus Stocker. On this episode of the podcast I talked with Connor Lee Bonner is the CEO of conjecture, which is an organization dedicated to scalable AI alignment. On this episode, we talk about AI safety. And Connor lays out his case for how AI could become dangerous to humanity. We then discuss two potential solutions to this problem, slowing down AI development and regulating it. And we talk about why these solutions might not be enough. Deer is Connerly. Great. Connor,

thank you for coming on. That'd be back.

What is AI safety? How do you how do you frame this problem? Because there are myriad a myriad of different framings of the AI safety problem. There is different terminology, what what do you find most useful here?

So over time, I've become more and more pragmatic, and trying to limit the scope of what we're talking about here. Because, yeah, if you think do things too expensively, you know, it just brings a lot of baggage people like to it. I think if it just currently, what I usually think about the air control problem is kind of like we call it or the alignment problem, the sense that I want an AI system to do what I want it to do, whatever that means. And a even more pared down version of it is the problem of controlling a strong system using a weaker system.

Where the weaker system in this context is a human.

Yes. Yeah. And yeah, perhaps

you could, you could talk about basically a good way to, to introduce the topic is to is to think about how could AI go wrong? Right. So what are some concrete scenarios? And I know there are probably a massive list in your head right now. But how could it go wrong?

I mean, again, if someone asked me, Connor, can you please write down the top 10 ways I could kill a million people for under $10,000? I'd be like, well, I won't confirm or deny that. I could do that. But if I could, I wouldn't do that. So there's a similar thing here where like I can give, you know, various scenarios of various concreteness. But like, I don't know, if you ever had this great post and less wrong, it's like Tylenol and terrorism or something. It's called. It's a great post, highly recommend it. Think Davis Kingley wrote it. I'm sorry, if that was not the case. I don't remember. It's called like, Tylenol, terrorism and dangerous information, I think is what the post is called. Points. This is one of my favorite posts, I've been in for hazards. And points, this point where there is a thing of 70s or 80s, I'm probably gonna get some of the details wrong, but the gist will be correct. There was a string of poisonings where unknown person not to this day and don't know who did it, took open Tylenol bottles in shops and put poison to them. And so several people died, that was actually really bad. So this was actually truly terrible event. And so, obviously, people freak the fuck out. And like, there's a bunch of like copycats, as well. Copycats is another interesting topic that we don't have time to talk about. But that's a whole other mimetic danger topic. And, but the interesting thing here is, is that, that never happened before. Like, like, pills of this kind have existed for a long time. You know, it's like bottles, like they didn't have seals back then. So for decades, they didn't have these like seals they have now actually, nowadays, when you have no bottle pills, like these, like seal, just like, pull off, you're not supposed to get this damage, you're like, not supposed to take the pills. This is to a large degree. Why is that because of these poisoning attacks that happen. So there is this massive change in culture overall. And suddenly, after the first person did, there were dozens of attacks like these, there's all these copycats, or dozens of attacks like these, where people tried to poison the medication and stuff like this in random acts. And this was an extremely effective form of terrorism. Like, I mean, I genuinely like I think this is like super scary, like, this is a super scary attack. So we don't know when ever took credit for those attacks, we actually, which is also like, very strange. Like, we still don't know what happened there. But the interesting thing is, well, it happened once, and then suddenly, it kept happening. And like, you know, now that we have seals is like, not much less of a problem. But it really seemed like just like one person came up with an idea. And then it spread. And so this is a great example of an internal hazard. That one of the things that I've really learned as someone who has a bit of a security mindset, you know, as a bit of a hacker as a kid, I will break things and like, you know, think of how to get around things and whatever. How could I, you know, accomplish silly things. I'm gonna get around security systems, how can I, you know, do things that maybe are not legal, and so on? I never did of course, I was a good kid. Of course, never got a job, but you realize at some point Then like a lot of things that are obvious to maybe you or me, are actually not obvious to a lot of other people. And especially not bad people. Most bad people are like, shockingly stupid, like truly, genuinely, shockingly unintelligent, especially terrorists, like stuff like that are often just like shockingly uncreative and unintelligent, and giving them ideas might seem like, well, you know, I've already come up with this. So, you know, like, surely someone else no one else can, but often again, so I'm not going to get my most likely scenario, because my most likely scenarios involve human actors doing stupid things. And like, I can think of concrete human actors where I'm like, yep, that's, they would do that. They're definitely dumb enough to do that. But you know, maybe it's not the name. But let me give you a let me give you a slightly the class, the general class of scenarios. So there are scenarios in my mind about like super intelligence and like, take off and Nanotech and like all that crazy stuff, but none of those remain line, I truly think those are actually distractions, from what I think like the the minimum viable catastrophe looks like, for me, the minimum viable catastrophe looks much more like if you talk to people who work in like intelligence services, or in like security, and stuff like this, and you talk to them about how operations actually happen, and like how to defend against things. So recently, I was talking to a senior person in government. And I'm going to give a concrete example, because this is a fixed problem. So this is no longer they, they have solved this problem. So it's no longer unsafe. There's like 10 years ago, or like five or 10 years ago, and they were in like the office of the government, Microsoft, which government or which person but and they were talking to several, like, you know, high ranking officials, I think like the head of state was there as well. And he looked at this big window, this massive, you know, open window. And he's like, what's stopping some teenager from flying a drone through that window strapped with dynamite and killing all of us? Nothing. And they all just like they all made excuses like, oh, that can happen. As it turns out, no, no one had any protection against that. No one thought about it, they could just anyone could have done that. And so the lesson to learn from this, so by the way, they do have anti drone defenses. Now. That's why I'm saying that's not a problem anymore. So that that government now has anti drone defenses. So this is no longer a threat. It's still a threat, but like, you know, they're aware of it was like five or 10 years ago, the one of my fundamental parts of my model about AI danger is that the world is unstable. The world currently is extremely, not unstable, as is the wrong word. It's fragile. It's actually very resistant to small or medium shocks. It is not at all resistant against big shocks. So I don't think we've actually seen a big shock since World War Two. Even World War Two is like a, you know, it's a big one, but it's not the biggest that you could imagine, like there have been bigger ones in history. So it's like the Black Death or something, I would consider, like an even bigger shock than World War Two. But since then, I don't think the world has actually seen a truly big shock. Like, even like the 2008 financial crisis is like a medium at most, you know, it's like, Sure, it was like COVID Also, like, medium, it's like, you know, like, historical, if you look at, like historical context, 2000 years context, like, you know, COVID was like, you know, it was as fast as Spanish flu. And it wasn't fast, like death, you know, like, we've had bigger ones that humanity get through just fine. So the world I think, is fundamentally fragile. And that humans generally build defenses against like, it's, again, the Black Swan thing. They trade volatility for blow up risks, you know, so we'll couch things enough that, you know, from day to day, mostly, everything's fine, you know, day to day volatility gets evened out, no problem, year to year, mostly gets, you know, okay, you know, some war down in Africa happens, you know, some terrorism over in the Middle East, you know, but like, you know, for the most part, if you live in the Western world, or like, you know, in Tokyo or something, you're fine, you don't notice it. We don't buffer well against, you know, decade or century level events. And an example of that is, you know, after COVID hit and gave us a warning shot, as far as I'm concerned is what I you know, really bad pandemic could look like, like a black death level pandemic that look like governments are now cutting back on their pandemic spending and striking down all bills to increase for future events. Hmm. Suspicious. So the world was very unstable. I think it is at the point where, well, a lot of I was talking to a senior official at a government who also has a lot of connection to security services and such. And we were talking about this and like, you know, what holds the world together? Like why don't people do all these crazy attacks we were

coming up with, is it simply because People are too nice or too good. And most people are either nice or good or incapable. And so we're relying on the vast majority of people simply not attempting these horrific, horrific connections.

I mean, it's a goodness, a goodness, it's just like most people don't actually want to hurt other people like not really, or at least not randomly, you know, they, most people want society to be stable. Most people want other people to be held healthy and happy. But you know, maybe you'll hate like one guy, and you'll be like, fuck this guy want to kill this guy. But like, it's very rare for people to like, actually want to kill many people or like actually, or like harm or mean many people. It's actually rare. It definitely exists, but it's like, quite rare. And there's the other and then then another one is agency intelligence, optimization. We're actually well pretty well protected against people who want to maim and torture, they're not smart. They live in prison. You know, like, low IQ sociopath, like this, this meme of is like, you know, sociopaths are the suave, intelligent, you know, a manipulative, you know, vampires. No, those are just the only ones you see, most of them just land in fucking prison. And you never see them again, because they're just complete psychos, you know, they start torturing animals as kids, you know, start beating women by the time to 14, you know, you know, start, you know, robbing people by the age of their, you know, their aid or whatever. Like, it's like, if you want to really see the dark world as it exists, you should look up like juvenile sociopathy or psychopathy psychopathy. It's like actually shocking. And so I think I think a lot of people who don't work in law enforcement aren't or like psychiatry aren't aware of is just how bad it is, like how there are some people who are just so incorrigibly evil, like truly incorrigibly evil, that there's just nothing you can do, but just like throw them in a cage and just leave, leave them there, because they just tend not rehabilitate. This is Reagan rare. Like this is not like this is like a small percentage of the population. But they do exist. And like ignoring that these people exist is I think, actually very dangerous, because it gives you an accurate model about how reality works.

And so the point here would be that AIS would be different because AIS would be capable and perhaps or they perhaps would not share our human reluctance to hurt other people.

Exactly. Imagine, just hypothetically, we have a system, an AI system. No, it's not, it's not really smart than humans have no super intelligence doesn't even have to human level intelligence, not even human. You know, it's not that smart. It but it's like pretty smart. You know, it's like, you know, it's like, you know, 90, IQ 100 IQ, do some thinking to do some planning, but also it's read every book ever written. It's perfect memory. It can be run, you know, at superhuman speeds, and many copies in parallel. And you give this into the hands, you know, maybe this thing can't do planning very well. Or it has like some problems or whatever, right? So then you just give it to a human or a group of humans. And now suddenly, imagine you had access to a group of perfectly loyal, you know, they never snitch, they never get tired. They never break. They never return. You know, sociopaths, the complete sociopaths, they will do anything you want. They're not evil, per se. AIS wouldn't be evil per se. They wouldn't be like statistic. But they will be sociopaths sociopathic, they would have no qualms. If you said, Hey, maximize my chance of becoming president and they calculate Ooh, assassinating him is a good idea. Well, they would, why would the AI hesitate? It's just optimizing a goal function. So when humans think about optimizing for goals, it's extremely implicit, that we have these constraints that we don't use these like taboo areas, like if you want to become president, you can even think of killing the president because you'd be like, No, I've never do that. You could also argue on consequentialist grounds that like oh, it wouldn't work, or you can trouble blah, blah, blah. But to a certain to a large degree. Also, you just wouldn't do that. Like, I would just not do that. I like like purely like the ontologically. I would just not to do that. But if you have a system, it's only optimizing. Well, so the danger, I see from Ai, in the short term is not, you know, super intelligence or, or like, you know, emerge and things those might also happen like this is those are strictly worst scenarios. And the scenario I'm describing, the scenario I'm describing is the least worst scenario that still ruins everything. But what I'm thinking about is systems that are perfect sociopaths that are just optimizing. There's a great post unless Ron called optimality is the tiger and agents are teeth, which is related to this as well. Where you these can be systems are not agents. It's not necessarily systems or agents. It's not necessarily that they're, you know, interacting with the world or whatever. These can just be like, very simple AI systems. They're just very intelligent, right, as they're just optimizing for something. And to optimize for something. They might just cold heartedly conclude, ooh, well, I've simulated how to, you know, kill the president first run this piece of code. And with an internet connected device, or you know, and then that spawns some kind of agent that, you know, or you have some kind of agentic process that does some kind of actions in the environment, they're like necessary to assassinate the President or whatever.

So in the short term, you're mostly worried about these tool AI systems, such as, for example, GPT, three would be an example of this, used by humans with, with goals that that conflict with those of broader society. So say, for example, we want to know,

it's worse than that. Okay? That's, that's, that's still one of the positive scenarios. There are even worse scenarios than that. It's even worse, because that is a scenario, it's a minimum viable thing. Like this is the one most people agree with. I'm like, Hey, imagine you have, like, you know, some intelligence service or whatever, from a hostile nation, they have access to a group of never, never sleeping sociopaths. They'll be like, Oh, shit. So this is the existence proof that the world is unstable. Like every single person I've talked to from intelligence services, or like, security, if I told them, hey, imagine your adversary had 100, perfectly loyal sociopaths that will do everything and are also as smart as von Neumann. How would you defend against that there'll be like, reflect, like, like that. Like, there is no defense against that, that is like insane. So this is the existence proof that the world is unstable. Now, things get even worse. So I don't even thinking due to human, like AI, my default outcome scenario doesn't even involve a human doing this. This is like, if we get to this scenario, we're already one of the better timelines, my main line prediction is, is that we should die. Before we get to this scenario, what's going to happen is instead, as we build systems that, you know, continue to increase intelligence and generality, we have them, you know, training themselves or in the environment, or whatever, gathering new data from the internet, playing video games, simulation, whatever, right? You know, deep mind just released that paper of like an agent to teach yourself how to like, you know, collect diamonds in Minecraft and whatnot, you know, that kind of stuff. And we just, you know, just scale it up, scale it up, oh, suddenly, it has art and language and notice doesn't use hand axes anymore. And suddenly, something weird happens. So my prediction is, so my true prediction, which is more higher shock level, which is like, you know, might make sense to you or the audience, but it doesn't work as well on like, you know, government officials is then like, and then something weird happens, we have these systems that no one intended them to do anything, you know, intelligence didn't intend for humans to develop culture, or to didn't intend for people to develop any certain ideology, or preferences or opinions or whatever. But as they become more powerful, they're interacting with their environment, they're, you know, have these like discontinuous capability gains is like the, you know, like those, you know, sums of S curves, whatever. So, like, to be clear, I think all of this will look completely smooth from the perspective of loss. So if you look at, like, the last graphs of these things, I think there will be no anomalies, I do not predict any anomalies, I think things will go totally smoothly, as predicted, you know, and then, at some point, you know, you know, the difference between, like, you know, GPT, four, and GP three on last is like, not that massive, you know, but it could do a bunch of crazy new things that like GPT, three can't do, I think that's just gonna keep happening. And then people are gonna start using them. For benign tasks, you know, people are gonna start automating, you know, writing assistance. You know, like, your clerical work and, you know, stuff for coding, you know, stuff like that, like all this benign stuff. And I think this is all going to be completely benign, until very sudden leads. And then very suddenly, we have the systems that start taking actions, and we don't really know why or what they're doing, but it's like, we're like, Yeah, it's fine. Use some early CEF. You know, just kind of like, you know, it's fine. And the term you just used, what does that mean? Sorry, reinforcement learning from human feedback. This is a commonly used technique at the moment, which I have strong technical disagreements with. Basically, the idea is you take, like language models or systems like this, and you train them to optimize a model of what humans like. So yeah, like humans, look at various outputs of the model and rate them leave a thumbs up, thumbs down, make the model like output the thumbs up ones more thumbs down ones, ones less, sort of, and this is sometimes touted as alignment technique. I do not think that is a very fair description of what the technique actually does.

Because the problem here is that you you don't really know which goals you're encoding and the agent you don't know how the agent is, or how the AI model is understanding your thumbs down or thumbs up

exactly. Like this is encoding human preferences and the ontology of weights diffs, which is just such an alien way, like okay, I'll show you 170 If I billion, you know, list of numbers of like slightly changing floating point numbers, I'm saying like, okay, cool. This is your preferences here encoded, super legible? No, like, of course not like, What the hell am I supposed to do with this? This is an ontology. I understand. Why would you expect your ontology to fit into this? Like, there's like, some alien comes down from Earth, comes out from the space. And he's like, Ah, I've been looking at you guys. You guys really life for larval? Don't you like what the fuck is for larvae? Like, yeah, I only got you guys. And then he goes off to do whatever that is. That's kind of like how Orly Jeff is. It's like, you can't

understand what you've encoded by looking at the weights of the network. And that's the research paradigm trying to interpret these weights and trying to extract what information is encoded. But that's, it's it's definitely lagging behind progress in the models themselves.

Exactly, exactly. So we, there's a fun little experiment, we did a conjecture, where we looked at two different models that were trained slightly differently. This is not actually our lhf, we thought it was our all HF turns out wasn't at least assuming the open air is telling the truth, which, you know, basically, we started two models to GPT models. And one of the models, is you asked it for a random number. And you looked at the output probability over like digit, it was like, pretty random, actually. Not perfect, like 42 was like slightly more likely than others. But overall, it was like pretty smooth distribution. Then if you look as other models instruct model, and you asked for a random number, it would put like, almost all its probability mass onto like two numbers. It was like I had like two favorite numbers. So this is really interesting. So what I interpret kind of is going on here, I don't think opening I like, tried to tell the model to like these numbers. No, I don't think that's what happened at all. But expect what happened is, it's just, you know, retraining on just like, you know, you know, being useful to humans, or showing a bunch of examples, whatever. And for some reason, something about, you know, the thumbs up things, had some weird correlations, or some weird connections somewhere, something that for some reason, you know, just made it really like these numbers, just like, really upvote these numbers, this was not intentional. So the interesting thing here is thought, oh, it's has bigger numbers, that's like, you know, kind of funny, you know, it's kind of harmless, and like this has been solved, the newer models, like they have much more data and and like, they don't have this problem. So, you know, it's more of an interesting item. The interesting thing here is not the exact example is that like, what else are they encoding? You know, what else are in those wait updates? Who knows? We don't know,

it's an example of how seemingly random goals could arise in an AI model. And suddenly, exactly, suddenly, the the model starts acting in a way that to us look looks weird. But it's because we've encoded some goals that we don't understand. And then the AI safety problem, the alignment problems or control problem is then when these goals begin diverging from the goals of humanity, or from the goals of the lab or company developing the AI, that is where things go off the rails, and the way we've been talking about it here. And the framing of the fragile world, I think, I think it's a great way to frame it, because it underlines how serious this problem is, and perhaps how intractable it is, do you do you think AI safety can be can be solved by humanity? Given the fact that we have we have a mixed record of containing dangerous technologies, we've we have extremely strict standards for nuclear energy production and for biological laboratories and still there are accidents there are these these have happened. So if if the failure rate, or if the security is extremely high, and the failure rate extremely low for this to succeed, can we succeed? can of course, there

is no law of physics that forbids us from solving the problem, building a super in the line, super intelligence, and having a wonderful future. There was nothing whatsoever that forbids this from happening. This is completely allowed to happen by physics. This is a path that is open to us. This is we have to be clear about this. This is a past episode. If not, it's quite interesting. We live in a timeline where we have not yet obviously lost. We're not in a timeline, but like everything's out of control. You know, AIs are already spinning up and like taking over governments like you know, or like, you know, you know, weird sociopaths are on par or whatever. That's not the case. The paths are clearly still open. We can still win. But we have to be realistic here. Truth matter is I don't expect us to solve it. Don't expect us to race race challenge in most timelines, I expect us to fail. This is a type of problem that humanity is especially bad at solving. This is a like, it's like I sort of call it like level two epistemology problem. This is a problem. That is not like a normal scientific problem where we are like, iterate over and over. And like, failure isn't catastrophic. And, you know, it's not a problem if we can, like, you know, if we don't have, you know, we don't get everything right on the first try. People are like, mostly aligned on the same on the same page and whatever. That's not where we're at. In our ad at all. This is a much, much, much harder problem. This is a problem where if we, if we get it wrong on the critical first tried, that's it, that integration, we might be able to iterate on, like proto versions of it, right? Like, we're, this is less like the nuclear bomb, and more like the nuclear bomb presented, if it would ignite the atmosphere. And, you know, there was the thing in Los Alamos where they weren't sure if it was going to ignite the atmosphere, there was like a possibility that it could ignite the whole atmosphere. And it did like, some crazy, like mathematics, like three days before the test, and it still gave them like a 30% chance it might ignite the atmosphere. And they still did it. Like imagine, imagine being in that room, and being like, Well, alright, it's only 30% that we will kill everyone forever. But I mean, the general surest shouting, so let's do it anyways, like, this is the state of mankind, like, like, imagine, imagine being at the state where you can both believe that that is 30% chance of the world, you know, ending. But the scary alpha eight is yelling at me, so I shouldn't do it. Like that's, that's the state of mankind? Or perhaps you're too curious not to do it, that will be even more even more sinister. And in some sense, I think for a lot of scientists, that was the case, the case, I think, for a lot of them. So there's this truly darkly funny quote, I don't remember exactly. But it's from von Neumann, where he's like, are the you know, the goal of science, you know, for scientists is to do science, no matter the consequences, it doesn't matter if it causes harm, it must be done. Because, you know, we must do it. And I'm like, what? Like, like, so, in my personal life, one of the mantras I live my life by one of the things I like to repeat to myself to like, orient myself through life is the same is the saying, Don't be stupid. Now, this is a very different saying from the smart, just be smart is hard. I can't guarantee I'm smart. So dear listener, I cannot guarantee to you that any of the things I'm saying is smart. I can't guarantee you that I can't guarantee that I won't. I'll be smart in all scenarios. But a pretty, like, I'm pretty good at not being stupid. No, I'm not perfect at it, I still am still stupid sometimes. But like, if you actually orient yourself around, recognizing when you're being stupid, and just not doing that, you can already outperform like, everyone by like a crazy margin. Because most people do stupid things all the time. It's crazy how much times people spend, just like loading their guns and shooting themselves into the foot over and over again. Like,

and the same goes for humanity on a civilizational level, you think? Yep,

absolutely. If humanity would literally just shooting stop shooting itself in the foot. I'm not even saying, you know, you know, in, you know, become enlightened and ascend into the, you know, a truly, you know, beautiful society, which, by the way, is also possible. Everyone's just being stupid. But even if we would just not be stupid, and not shoot ourselves in the foot over and over, and over and over and over and over and over and over and over again. If you could just do that, man, man, would I feel better about the future, but like, prediction markets are still illegal in the US. Like, we are shooting ourselves in the foot so hard, every single day, all the time. It's, it's just truly patenting. Like, if aliens came down from space to be like, Oh, what the fuck did you guys do?

What would it mean for humanity to not be stupid in terms of AI safety? What should we do? What is what is the scenario? What is a? How do we survive this if we were to not be stupid? So

that's a great question. So I'm gonna I'm gonna give an answer for what would we do if we were not stupid? And what will we do if we were smart? That's what I want to hear. Yeah, so first answer what we do for not stupid. Everyone, we just kind of look at each other and like, huh, the CGI thing, you know, seems like because we like the the atmosphere how guys? Yep, we should probably not do it, right? Yep. And so they didn't. That would be the non stupid version. Like, you know, back to Alan Turing. People have predicted this, like this is not like some crazy new thing or whatever. It's like, it's obviously predictable. Alan Turing already predicted all of this basically and like, you know, I ally, you know, he good and stuff like this. So let's,

let's sorry, but let's pause here for a second actually, do you think it's possible to slow down progress in AI capabilities research? So you think it's likely that we will coordinate Do you think it's, why don't we just hand off the torch to less scrupulous companies or labs? If say, say open mind. DeepMind and OpenAI began collaborating on slowing down progress in their research, wouldn't it just be then the next most, you know, advanced company taking the lead?

So the original question asked me is, what will we do if humanity will stop being stupid? So you're now asking, Okay, what happens if you continue to be stupid? Because obviously, if you were not stupid, the other labs would just also not do it. So those are two different questions. Just want to make clear. So you're asking, Okay, what do we do if we continue to be stupid? I can answer that question. I have many models here. I expect us to be continued to be stupid to be fair. And so So assume we continue to be stupid. Then like? So there's two there's two parts of this, I guess, two main parts put into this. First part is, will others catch up? How much will it catch up? Etc, etc. Second, does this question even make sense? First answer. I actually genuinely think that no one other than like the, like, top labs irrelevant. I don't think China is irrelevant out. Like, I think it's a terrible meme. They're not going to catch up. They're so far behind. Who cares? Like, I think this is actually genuinely a terrible meme. This is like a really, like insidious, insidiously bad meme. Is that so it's relevant for longer timelines? If we're dealing with longer timelines. I mean, I recommend everyone just, you know, start solving politics because Oh, no. If we're in short, timelines, doesn't matter. Like like it? Well, politics matters, because like China, Russia, whatever, like Russia, obviously doesn't matter. They obviously don't have the capabilities. And like, people have these like, really weird like orientalist, like mindset around China. Like there's like boogeyman that has like this capability. Like, if you actually look at China, like actually, like, really look at it, and like how science is done there. And like the bureaucracy, and like, whatever. It's China's one of the worst places in the world to do science. Like, they're like, doing science, China's a nightmare. It's so bureaucratic, it's so slow. It's so you know, ideology, you know, suffuses every part of it. There, like, so much of it is marketing. Like, just like Chinese government markets itself as good at science super, super hard. And they have a whole large population, which includes lots of brilliant people who are succeeding, despite the Chinese government. Like To be clear, a lot of the smartest people in the world are Chinese, like, there's a lot of truly brilliant Chinese people. And, you know, they succeed despite the Chinese government, not thanks to it, you know, it's just because of their, you know, the presser appearance of the human soul and mind, these people can do great work. And I would never, and most of those, go to the US do their good work. Because, you know, if you're a smart, brilliant Chinese person, you know, student or whatever, why would you want to stay there with all this bureaucracy and politics and all this bullshit, you know, if you go to the US, you make tons of money, you can do your research much better. So, so

if we can, if we can rule China out as taking the lead, if the if the US were to begin not advancing AI capabilities, what about just say, the second tier of American AI companies say, perhaps a Facebook or Google Brain or something like this?

Yep. So there's several factors that go into that. One is, is that they don't actually want to kill everybody. Like, like, and they are mostly, you know, like, not insane. Like, they might be like, not super convinced. And they might be like normal amounts of irrational, but they're not like, politics amount of irrational. So like, you can talk to them. Like, you can just talk to Mark Zuckerberg, like he is weird, and whatever, but like, he's a person you can talk to. And also, these are all people who do listen to the US government. You know, if the US government said, No more AGI there would be none. Of course, that's not what would actually happen. My actual models are more that government takes over and starts ruining everything, because governments aren't capable enough to do this, like actual non stupid things. So this brings us back to not being stupid. So in the real world, this is not impossible. Governments have levers to do this. It is a thing that is like within their like jurisdiction and like power, but it is not within their executive ability. I think, to at least not in short timelines are the State effectively, I hope I'm wrong about this. And this is something we can like, figure out. So actually, it's worse than that. I am actually very skeptical about governments trying to slow down AGI I think this is not a good idea. And or Well, it's tricky. And but the reason I think this is because of my model of governments and how incompetent and like contracted and like, you know, internally inconsistent they are, in a sense, having labs in control is not ideal, but it is there are more concentrated points of coordination, it's easy to coordinate with DeepMind that it is with the US government. And whether if we have longer timelines, and five years, even maybe on the five year timeline, it's not gonna matter, because governments are gonna get involved, obviously, like, obviously, so there's no way, you know, like intelligence services are just gonna let the shit go on. If like, if we have like, you know, 10 year timelines, or 20 year timelines, no way. Intelligence Services. I mean, I'm pretty sure all these organizations are already infiltrated by intelligence agencies. I wouldn't be surprised if intelligence agencies were listening to us right now. I have been cited in the CIA paper. Did you know that? I didn't know that. I have. Yes. That's probably fine. Right. But, yeah, so one thing that government, especially US government is very good at, is reacting to national threats. That is like, the only thing that makes the US government get it shipped together, is when the Pentagon says this needs to get done, then the US government can actually do things. So if this continues to babble along as long as like feature is then are kind of maybe economics thing, then I don't expect governments to do much. If this becomes like a national threat type scenario, then I expect the behemoths to lumber into action. And I expect it to break everything by default. Unless we, you know, somehow direct to this and like, you know, help the behemoth make a non stupid choice, I don't think you can make a smart choice. I think it's like, genuinely possible for like governments to be smart. I think it's just like, too complicated. But I think it's possible to make governments be not stupid.

And would that be by a piece of legislation governing how, how they how the governments would govern AI? Or what are you thinking in terms of helping governments not be stupid?

The truth is, the government is composed of people. It's composed of, you know, lots of civil servants, politicians, and bureaucrats and generals, and you know, Intelligence Service agents, so on, these are all people. And these are all people you can talk to. These are all I've been surprised by how much I have, like, you know, just like contacted some, like senior government officials, and they were quite happy to talk to me. And I could just answer the questions. And that was quite nice. And I'd like to do more of this. I have this is a thing I've updated very heavily on last year, is how much again, make trying to make these people act smartly. In the scenario at scale, maybe individuals, like I've met some pretty smart people in government, actually, some not. I met some pretty, pretty smart people. You can't do that skill, but you can help people be not stupid, because it's also in their interest to not be stupid. So I think the things for example, I am doing and that'll be very interesting, if any of your readers out there listeners out there are, you know, working government intelligence services, whatever, I try to understand these problems better and how to do this, my email is open, please let me know. I would be happy to help. I think this is important to like inform people like you know, how to give people better bottles of these kinds of things, how we can reason what kind of things I don't think there'll be like one piece of legislation that fixes all of this or whatever. I think the way government really works is like not like that like it's a lot more backroom deals dinners you know, who knows who who owes who will favor you know, what are the lines of You know, various parties and whatever. And I think the ideal thing I would want is that just to have everyone kind of be on the on the not stupid side just be like yeah, let's let's like not be stupid about this let's like all take this seriously, let's like fund alignment research like Jesus Christ, please. Like let's just like have DARPA pull your push $10 billion into into alignment research. Why not? Like, this is like for government. This is like, you know, they can they can make this happen. Like, the number of people that need to sign off to get like $100 million. until like, until like a project is like shockingly low. It's like It's like depending on the government. It can be like two people. So let's just do that. Like there's like 200 people maybe in the whole world working full time on alignment. If governments just like said even just said, this is a national Priority, even if they don't like put funds into anything, basically all like massive amounts of academia will just like lumber into motion. And because it becomes high status, now it becomes the thing to be working on, it becomes a legitimate real scientific problem, you know, like, you know, rubber stamp, you know, this is a, you know, you are a high status person for working on this. Cool. And just that would cause, I think, a massive shift in like, how many people take this prom seriously, and like, how academics are taking prom? Seriously, there is a massive risk in doing all these things. Because whenever you get lots of people involved, you know, politics gets important. So ideal world would be of course, you know, oh, turns out alignment is super easy. And we just salt in our basements. And here it is, and everyone's happy. I don't expect that's how things are gonna go. So if we're realistic news, do some real real politic here, then what we need to do is to be realistic, that people will get interested this is of interest to everyone. This is of interest to governments, intelligence agencies, academics, everyone. Let's help people not be stupid. Let's talk to let's be friendly. You know, let's say

that DARPA began funding alignment research to the tune of $100 million, or whatever you pick a number. And, you know, could this be counterproductive? Because the money would be used to increase capabilities? Isn't? Isn't that a pretty life danger? If we look at four critics of open AI will tell a story in which open AI was founded with safety in mind, but then increased capabilities of AI and thereby increased AI risk. And perhaps the same thing could happen with a government grants into alignment research.

Of course, this is the default thing that happens. Everyone's stupid. Remember, like being smart would be just the government, nationalize everything creates the alignment Manhattan Project solves the problem for done. That's the smart solution. We're never gonna get that that is like, don't even think about that. It's not possible. Yeah. So

it's not it's not it's not that interested, interesting to discuss what we would do in an era very smart world. So what we're trying to avoid being stupid instead, yes, we're

trying to Okay, so like, so I'm thinking about purrito improvements here. I think we're not stupid. We are currently in a world where there is no government funding for alignment. This is stupid. Like, it's not just like, not smart. It's also stupid. If we have like, okay, $10 billion alignment research, and it all goes to, you know, something irrelevant, or something dangerous. I'm like, okay, that's not good. But this is a less stupid world and expect a world that is already at this point, will be more amenable to interventions that help build safety and alignment. And the ones we're currently in, of course, because it's backfire. Yes. Obviously, like anything involving, you know, lumbering behemoths, you know, tends to involve, you know, blowback risks, you know, when you when a giant monsters attack in Tokyo, and you summon Godzilla, like, you know, Godzilla could probably beat the monster, but you're gonna be a lot of reconstruction costs, you know, by default, even if Godzilla was the good guy.

So how could government intervention go wrong here? What are ways in which the US government could break things

that have gotten I don't even want to give them ideas? And that's a plenty of things. I mean, obviously, as you just said, The obvious one is just, you know, funding capabilities work, obviously. The second one, which I think is just extremely likely to happen, it's just they only fund military applications, if they don't actually fund safety, they just found like, Okay, how do we maximize, you know, military applications, and whatever. And that obviously kills you. Another one, like I actually talked to someone about this a while back, you said some very interesting, so I was talking about interpretability research with them, which is, you know, very common topic in AI alignment research. And she basically said that, so he worked for the military industrial complex. And he said that the number one thing, currently holding military back from deploying AI is at wide scale is lack of interpretability and accountability. So every increase in interpretability increases the military adoption of AI. This is I think, something that a lot of people in like the safety world do not consider when they consider the cost benefit analysis of interpretability research. I do. I still think it's worth it. But it is something that you should have in your calculus. So these are like some obvious ways that the government can mess it up. Another obvious way they can mess it up is to politicize it. It becomes a left wing right wing, you know, Red Blue Team issue, that seems that's another way things could be stupid. Like the non stupid thing is, well, obviously this is not red or blue. This is like you know, we're all it's all in all of our interest to you know, be able to control our tech algae, like, no one benefits from this not the case. So let's just not be stupid about that. Unfortunately, this is the kind of thing humans tend to be really stupid about, you know, like, you know, climate change or whatever. Yeah,

let's, let's go back to the question of government grants for AI alignment research. Another thing that could go wrong here is that AI alignment research becomes a buzzword, and it becomes it comes to mean something else than what they originally meant. And it becomes a way to attract funding to your, to your existing projects and so on. Is there a way to avoid this? Is there a way to be strict about what you're what you're trying to fund? Without it drifting into becoming too broad? And coming to me in something else?

That would involve being smart?

Is there any hope here for for what could we do to to constrain what we're trying to find?

Oh, there's, I mean, there's lots of marginal things you can do here. But actually, I think this is a massive mistake that a lot of funders have been making here is that, so this is actually a genuine critique I have of like EA and like, you know, ASAP funding is they're extremely risk averse. Like, they build themselves as like, oh, you know, we're funding the crazy stuff. No one else was funding or like, you know, we're really to do things, whatever. But they're not actually, like DARPA is way less risk averse than, like, you know, open fill or whatever, right? Understand the balls, because DARPA has way more money. So like, you know, this is not a this is like an understandable thing that, you know, maybe openfit would be more conservative, because they have much less resources, and DARPA, you know, could do all kinds of crazy things. But like, I would expect that, like, when DARPA funds things, like, they found a lot of crazy, stupid bullshit. And like, they love it, like open fill funds, like, you know, one person that turns out to be controversial or like, does something stupid or whatever. And then people are like, huh, using my donated money to fund this guy. Like, that seems like an inappropriate use ABA, like, like, no good deed goes unpunished. Like if DARPA funds some guy making an invisibility cloak or whatever, right, you know, DARPA, I mean, whatever, they do weird mill tech stuff, that doesn't work. But if a philanthropic organization, you know, Bill and Melinda Gates Foundation funds, like one company that turns out to be shady, or whatever, everyone's Of course, immediately under case no good deed goes unpunished. It's really funny how, if you were a rich billionaire, and you're just selfish, you don't really get criticized for that very much. It's kind of like you're like, yeah, obviously. But it's like the it's called the Copenhagen interpretation of ethics. If you're a billionaire, a try to solve a problem that everyone and you failed to solve the problem, you get way more shit than all the billionaires that did nothing. This is a this is, again, humanity shooting themselves in the foot, so abysmally hard, that you should get credit for trying to solve a problem, even if you fail. But the exact opposite happens, there's a large reason of why people are so low agentic is because trying and failing is gives you more social negative than not even trying. So this is a massive problem. And so when I think about government grants, yeah, I expect most of it to go to bullshit. I expect most of it not going to work. But you know, it'd be good. If I wanted government grants the way it would want it to go as DARPA type DARPA is very different from like, how like other grant making works, if we do the other grant making also fine. Look, I also think there's no way that would help. But ideal case would be like DARPA, like high risk, like weirdness, because we don't know how to solve Alignment. Alignment is not like a low risk, like, man, we just need this amount of money to build the alignment machine, and then we're fine. No, no, this is blue sky research. Like there's like, you know, we if you look at like, current alignment approaches, you know, some are, like, pretty simple and reasonable. Others involve like, you know, a retro causal a, you know, a causal decision theoretic multiversal, you know, decision theory, simulations or whatever, right? And he like, is either like, is either of these going to work? I don't know, probably not. But surance L, someone should fund them, like someone should try. Is

there a way to earn money by solving alignment?

I mean, depending on what you mean by that, obviously, yes. I mean, in the sense that if you solve alignment, you make infinite money, obviously, like you solved everything. Like the thing holding back humanity's progress. I think the limiting factor, the bottleneck on human economic progress, wasn't the only bottleneck but the biggest bottleneck intelligence is that if everyone was twice as smart, whole man, could you imagine your message was like the median human had like 200 IQ? Could you imagine? What society would be like? Oh, the policy is the efficiency or the science, the, you know, the, the or the social, like systems we could build if needed the coordination technology we could implement, like, Oh, could you imagine? And this is just like a modest increase in intelligence? No, this is like, if every, you know, you know, if everyone was like, this is still a human range, like, like, 200 IQ is like, still within human range, like, there are people that are that smart, you know, so like, that's still like, not as, as good, like, anywhere near as good as things could be. If we have, like, you know, AI or, like, super intelligence, that is, like, you know, running things and developing technology and coordinating and, you know, doing economic activity and whatnot, we can have, you know, everything like, you know, if you have an AGI it does what you want it to do, I mean, price to tell to cure cancer, tell it to, you know, trade infinite money on the stock market, tell it to just no more wars, please just like go negotiate with everybody and solve all politics.

Yeah, I was, I was simply interested in what the best funding model for solving this problem is, whether it's nonprofit, or government grants, or whether it could be a for profit company, or some new legal construction, like limited profit, open AI style. Great

question. This is the thing, I've often always thought about what? And so I mean, if we were a smart society, we would have, you know, like, you know, impact grants,

what is an impact grant.

So I don't know exactly all the exact details, they will do this, whatever. This is a new funding instrument, where basically you can like, you sell impact certificates, I think that's what it's called, actually, maybe I'm thinking of impact mark is not grants, one of the other and like, basically, you're creating a new charity, it will work the following way, involves the following people, and now you sell shares in the impact, you say, like, you know, I'm sharing spelling, saying 100 shares or million shares, they're like the philanthropic benefit of this existing, so then people will think this is good. And this should exist can buy the shares to fund your operation. So like, this would be an example of like, something that a smart society would have, it would have, like, this would be like, super common. And then the

price of these certificates arise, when there's more impact from the, from the company, whose shares we're talking about.

Yeah, that's yeah. And then you can also issue more like, like this, this just an example of like, a simple system that like, again, showing just like how humans are, you know, so far away from, like, smarter society to have even better systems, like, even better, like, a common funding, you know, credit, attic funding, and like, you know, common, you know, goods funding kind of research. And

let's just take one objection to impact grants, which is to question whether we can measure impact appropriately for these grants to work and objectively in a sense, so we can, so there's this information that's that's out there, objectively that we can trade on? Do you think that's possible? Or do you think that's likely to happen

in a set in a smart society, of course, it's possible, but we're not in a smart society. So like, this is an example of a technology that only works in smart society, it doesn't work in us, I don't think impact markets work in our society. They just don't. It's not that they fundamentally can't work. They just don't work for practical contingent reasons that couldn't be overcome in the future. But in the way things currently are not. It's still work. So what do you actually want to be pragmatic? About funds? This is like, you know, so for listeners who don't know, I run an AI alignment research company conjecture, and we are a for profit company. And that's not a coincidence. The reasoning for that is, quite simply that that is the best form, I think, currently to be able to raise large amounts of money. And to be able to have an ongoing supply of money to fund research into this kind of stuff. If you don't have like some kind of crazy billionaire backing or something. Even there, there's problems like diversification and stuff. The truth is, is that if you look at how our society allocates, resources, money, power, etc. The currently, again, this is a contingent truth, this is not a ideological statement, this is just a contingent truth about how reality currently is set up. But this could like change in five years, is that currently, the vehicle through which it is most likely to go from $0 to a billion dollars in a short period of time, as a startup? This is a contingent truth about how our markets are currently set up, you know, 50 years ago, that was not true. You know, and maybe five years from now, that won't be true. But currently, VC markets are startups that are build, you know, software based products that, you know, are useful, you know, scale very, very quickly to very, very many users are the most effective way to gain very, very large amounts of resources where this you know, unless you happen to have some kind of weird other scenario, but like, those aren't scalable for the most part because Also our markets are way more robust. And way more diversified sources of funding and so on, then, for example, having one billionaire, the Europe patron, that might, you know, might be great. And you know, if some billionaire wants to come by and you know, has a billion dollars happy to talk, but practically, you know, as we saw, for example, with the FTX scenario, there are blow up risks, let's say.

Yeah, and so perhaps the main objection to the for profit model is that the incentives won't be properly aligned to, to do the actually, societally, societally beneficial thing, you will be pushed into doing the profitable thing as opposed to the, to the good thing.

Yeah, but that's not a property of for profits. That's a property of how we as society assign credit, if we had impact markets, and we had, you know, been Neverland, you know, billionaire patrons, or whatever, then we will be assigning credit differently as study money is fundamentally a credit assignment mechanism. It is a reinforcement mechanism is a mechanism that gives sub parts of a computational graph ability to do more computation to do more actions, we have reward people, ideally, by you know, it's criticized was a fundamentally hard problem, like credit assignment is extremely, extremely hard. And it's, you know, massive rabbit hole there. And the fundamental like, the fundamental like, you know, insight from like, mercantilism first and later, capitalism is how we do this credit assignment, there was other economic systems that do credit assignment differently. And the fundamental progress of capitalism is idea of how we assign credit to like, people capital, or, you know, labor or whatever, in certain ways you can agree or disagree with this is the right way to do this. But it has been a very efficient, it's very, it's the way our things currently work. It's the efficient, most efficient system we currently have. Now, capitalism is in many ways natural, like, you know, give people money for things you want, trade is a very natural fundamental thing to build a system or credit assignment on top of that perfect, for example, capitalism has large problems, pricing externalities, and like commons, this is a failure mode of capitalism. So I would expect a very advanced alien society would not be capitalist, they will definitely not be socialist, but they would be like some third thing, you know, they would have some kind of, you know, prediction markets, you know, Commons trading, you know, based systems, you know, some like Robin Hanson, design, the economy or whatever, you know. And so, the fact that there are these incentives are contingent, a, see a not stupid society would have different mechanisms, they would have different incentives. But there's a great, there's a great essay is like, I forgot who wrote wrote it, and it was like, incentives aren't the problem you are, like, if you have bad incentives, and you act on them? Well, you're bad. Like, you know, you did the bad thing, like, Sure, there's like, some amount of exculpation here in the sense that like, you know, you can say like, well, you know, I didn't have full freedom here. There was incentives, blah, blah, blah. But ultimately, you took the action dude, like, you know, it's true that there are systems that are so corrupt or so your 1984 oppressive or whatever, they just fucked, and that case, yeah, you're you're fucked, obviously. Like, you know, like, what do you want me to say? Like, yeah, if you're, if you're controlled by some like crushing market force, or authoritarian regime or something, and every time we try to resist, you get shot or you, you you starve, then yeah, you just die. You just fail. Like, yeah, obviously, it's surprising that we have as much freedom as we do. Like people are allowed to spend their resources in weird ways. You know, people are allowed to be eccentric, to a large degree, not infinitely and very differently to Ben ampio. And the fact Elon Musk is allowed to exist, is like, I mean, just look at the guy like, like, you must be a pretty, you know, high IQ society to allow something like this to go on. Like, I mean, there's genuinely I mean, I'm being a bit snarky, but like, you so weird, you so erratic, and he like does so many like crazy things, like potentially dangerous things that like, as he has so much power, and he's still allowed to have this power, it's like, you know, in like, in an authoritarian regime, this shit wouldn't fly. You know, if he was Chinese, that shit ain't gonna fly, you know, they're not gonna allow something like this, this. And so, in a sense, I'm saying that this is the best we have in the stupid society is just like capitalist freedom. It's not perfect. It's very bad. Actually. It's quite stupid. But it is the You know, what's the alternative? Right? Like, you know, if the alternative is, okay, you do a nonprofit, you have no money, you die. You start game over. The alternative is you have a patron. Okay? So first of all, assume you have a patron. Second, well, the patron probably got his money through capitalism. So he's using his weirdness capitalism points on you by proxy. And now you're also tied to him. So now you have the blow blow up risk of your billionaire buddy being you know, now he has incentives, that the incentives that the billionaire expresses on, you are also extremely powerful. And like, you know, maybe, you know, Dustin Moskovitz, or whatever is a great guy, but like, you know, there's a lot of them who are not,

what do you think of companies tying themselves to the masked by having windfall clauses where, for example, if they successfully develop AGI, they then have some clause stating that they will distribute the profits from this venture in a, say, more fair way, after they've returned money to investors?

Definitely cute marketing stunt.

But not substantially. Important.

Look, learn from the very simply right, like, you know, I run a company, random CEO, you know, and whatever, right? And sure, there's like, there are things you can do, you can have, like a board, you can have like shareholder meetings, you do all these kinds of things. But like, if me and my co founders were like, Hey, we're gonna fuck up this company. Because nothing can stop you. Like, it's like, if someone finds the philosopher's stone, and he has a handgun, you're like, what are you going to do? You can like, you can complain about oh, no, wait, you have a contract that you said you would give me the philosopher's stone and like, okay, she to like, like, if like, if the person with a philosopher's stone and a handgun is sufficiently on the line, you're just screwed. So like, sure you can sign these contracts, and maybe that makes people feel better. But it's very funny. Because like, if you if you asked this question to someone from like, the Middle Ages, or something, they would laugh in your face, they would be like, what the king signed the contract with you like, so what, like, who's gonna stop the king, like, and so I think it's this this thing about power is that it depends on how the situation goes, of course. So like, there are so turned on the cynical dial just a little bit from here. I do think people doing this for being to a large degree, actually quite genuine. Not all of them. Some people, this is really just safety, washing. Some of them are being quite genuine, they're really trying to make things work. And I respect them for that. It's really nice. So I think the biggest value of these things is more than just like, signaling that they tried, and like they're trying, but of course, any genuine signal of honesty will quickly be co opted by anyone who's not honest.

But it's, it might be an interesting first start, it might be a milestone or symbol, where we, we say that this is not some, this is what we intend to do. And perhaps then the actually trying to do that thing comes later on after you've you've you've signaled that this is what you want

to do. I mean, sure, these are all coordination mechanisms signaling. So yeah, I'm seeing from those perspective. Yeah, it's, of course, it's valuable. Like, you know, I'm, I'm being very cynical right now, you know, because also, podcast mode, right. But signals matter. You know, signals matter. Reputations matter. Honor matters, like these things do matter. But like, we should not delude ourselves here, right, that like people lie. No, I'm sorry if this is shocking to any listeners, but people lie, like a lot all the time. And they change their minds. You know, a lot of people promise very big things when good times comes. But once you know, war comes suddenly, you know, you see how people really are. So I'm interested in these mechanisms from the perspective of what they say about the people. Like I know, some of these organizations, and I know like some of the thought process that went into these things and like, Wow, you really tried, like, that's, that's, like, really heartwarming, like, I actually feel better about you as a person. I trust you more now. There's other people where I'm like, oh, yeah, this is just a marketing stunt. I do not trust you more. So the interesting thing for me is what it says about the people or like, or what it signals about the people and also what it doesn't signal about people. I'm not optimistic about the legal mechanisms, like I just like I would love if they would work. Like, the problem is legal mechanisms, like these involve having enforcement power. And like, if you have AGI Yeah, it was gonna force that. Exactly. Points.

All right, perfect. Let's Let's end it here and then perhaps moving to the to the semi or pseudo lightning round as I as I call it


Transcribed by https://otter.ai