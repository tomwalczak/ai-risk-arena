All right, welcome to the future of Life Institute Podcast. I'm back here with Connor lady and Connor is of course the CEO of conjecture. Runner. Welcome to the podcast

back in.

Alright, so how does the strategic landscape of AGI look like right now? Tell me if totally if you think this kind of structure is approximately true? Do you have open AI and deep minded front, followed by the American tech giants like Apple and Google and matter? And then perhaps you have American startups, and then you have Chinese tech giants and startups in terms of capabilities is that the right ordering,

sort of, I would say Deep Mind is behind open AI by a pretty significant margin right now. I think anthropic might actually be ahead of DeepMind at this point, not 100%, clear, deep mind keeps the cards much closer to the chest. So it might have some really impressive internal things. I've heard some things that effect but I don't have evidence of them. So it seems to be opening I is clear front center ahead of everyone else. I expect anthropic will catch up, it seems like they're trying very hard to train their GPT four model right now, like equivalent model right now. expect they will succeed. tech giants, I mean, really depends, like meta is like, pretty far behind Google is DeepMind. Apple doesn't do anything as far as I can tell. So I would, for example, think some startup such as character are ahead of all of them. Like character AI is a company that was founded in New Hampshire, zir. And others. Notice, you're being the person who invented the transformer. And, you know, their their posts focus on chatbots and such, but their models are quite good. They're quite good. So they're the other. Yeah, that's kind of how it was said, Yeah, I don't feel that Chinese tech giants and startups are very relevant. And they're really far behind. And I don't expect them to catch up anytime soon.

Are we simply waiting for the tech American tech giants to make moves here, I mean, Apple has a lot of money, they have a lot of talent, they have machine learning chips on all of their iPhones, you could you could easily see and enhanced Siri GPT, four style

sharp. But, you know, Google, which is supposed to be the best of these couldn't even keep the guy who invented transformers around because they're so dysfunctional. And then one of the first things I was, you know, I was told by experience investors and such when I founded a startup is that like, incumbents are not competition. They're all incompetent. It's like, oh, like, sure all the things are possible, they have the resources to do these things. But there is a lot of reasons why it can be very hard for large organizations to execute on these kinds of things. And other good examples barred. So like the chatbot that Google produced, it was severely delayed. It, you know, have lots of problems. And it was just like, extraordinarily underwhelming compared to what a much smaller group at opening I was capable to do in smaller amounts of time, you know, Google's encode read now where the, you know, the CEO is personally involved and like, everyone's freaking out, and they still doesn't mean they can catch up, like, you know, just because a lot of people in the boardrooms say something should be done, and they have a lot of money. That's not enough. It's, there are some things that are actually hard. And, you know, training, complex cognitive engines, like GT four is among those things. Other ones are, for example, like chip production. Like, you know, China did a huge thing about how they're gonna produce their domestic chip production, they're gonna catch up to TSMC. And that has now been like, slowly like choking in a way because it's just not succeeding. Because no one in the world other than TSMC can get these ultra violet machines to work for whatever reason.

So what we're doing right now and kind of describing incumbent technology giants, as incompetent might that be mistaken, because perhaps they're hiding the They're Real, they're not waiting to release until they have something that's very polished, that will be very Apple like thing to do, perhaps Deep Mind has something that they're not releasing because they are safety conscious, or is this simply is this kind of wishful thinking? Wishful thinking? Okay, so So, so the situation is that this is interesting, because then the situation is kind of how it would look, if you're a naive observer, you're just seeing open AI making lots of progress. And the most legible big players are the most advanced big players is that so basically, the strategic landscape is transparent in that way. It is quite transparent. Like,

the truth of the matter is this field is not good with secrecy. This is not like defense contractors, where people have a culture of secrecy and like keeping things close to chest, like Apple is like the only company on this list that like is good at that. DeepMind also, anthropic is also trying but you know, make Just if you ask, you know, who's better, you know, Lockheed Martin, or like, you know, Airbus fighter jets or whatever I'm like, Okay, I genuinely don't know, like, you know, that's like, actually hard to know. And like, people will actively make it hard for you to know these things. But like, all these people have an incentive to make the public how good they are. And they do so quite aggressively, when it benefits them. And, you know, Google, you know, scrambled after chat GPT to catch up with Bart, and they put their best effort forward, and it was a flop. And you know, same thing like with Ernie and China, and stuff like this, like, I think they don't Galaxy bring yourself like, it's just what it looks like.

And also the AI researchers in the most advanced organizations they want there, they want to publish research so that they perhaps can can move to another organization they have, they have interests and incentives that are not not particularly aligned with the company they work for. So there are these there are these publishing norms where your resume as an AI researcher, is your published papers, and this this make it basically difficult to to prevent new advances from from spreading out to a lot of companies? Yep, that's exactly correct. But if that's the case, why can't google catch up then,

because there is an additional aspect to it, which is execution and tacit knowledge. So especially with large language model training, a massive amount of difference in good language model from a decent language model is weird voodoo shit, or it's just like, someone just has a feeling like, oh, you know, you have to turn down the atom betta to decay parameter why? Gnome said, so, you know, like, there is a theoretical catch up, you know, where it's like, you know, you need to come up with the right architecture, or the right algorithms, whatever. But there's also engineering like, just like research and like logistics, like, you know, setting up a big compute data center is hard and takes a lot of money and time and specialized effort. And also, then you need, so there's like a logistical aspect to it. There's also there's a will and like, you know, executive capacity that like, can an organization commit to doing something like this, and like, have someone lead it who can like actually manage the complexities involved with it. And there's huge aspects of tacit knowledge of just like, the stuff that isn't written down. And there's a lot that is not written down, and that the test tested knowledge might be particularly important in chip production, which could be why you know, the giants are the people in front of companies in front of chip production are, you know, they are very difficult to copy. That is exactly correct. Like, this is also what people on the inside of lecture production will tell you, there is a absolutely phenomenally large amount of tacit knowledge that goes into producing high end computer chips. And that like, there's a lot of task sounds like only TSMC has, and like, they're not writing it down. And even if they want it to, they probably couldn't.

Why is it that for example, with defense companies or with Tip production, there's a there's a protection of intellectual property in such companies that we don't see in in AI companies in the most kind of secure or safe secrecy in AI companies is simply to not say what's happening, you don't see that, that this company secrets are protected by intellectual property in the same way. Yep.

I think this is completely contingent, historical, cultural fact, I think there was no, it didn't have to be this way. I think it was literally just coincidence. It's literally just coincidence, that just the personalities of people that like founded this field, and like the academic norms, that they came from a very academic area, there is much less military involvement, and much less, you know, industry involvement initially. And there's much more like academics have much more bargaining power in that, like, because of the high level of tacit knowledge, there's a larger bargaining power that academics have here, because, you know, if no, she's here wants to publish, like, he can go wherever he wants. So if you don't allow him to just go somewhere else, or like whoever, you know, any high profile person, but I think this is totally contingent, from the perspective of people in ML and things like this is the way things have always been, will always be can only be, this is obviously wrong, because, you know, you're telling me the people who build like, you know, stealth fighters are not incredible scientists and engineers, like, you know, give me a break, like, just because you're great scientists engineer doesn't mean they're, like, you know, compelled by their very genomics to like, you know, want to publish like now this is just a cultural content. This is a contingent truth about how the culture happened to have evolved as the

race to AGI heats up will we see more more more closeness, so close to data or closed algorithm Do you see labs not publishing as many papers? Will these kind of open source norms from the AI researcher community begin to fall apart?

I hope so. We're seeing some of it, that's for sure. And I hope it accelerates.

Okay. As you see it, we are already in a race towards AGI Correct? Yep, obviously. So maybe 10 years ago, when people were debating AI, people will debate whether human level AI is possible. And whether we could, whether we could get there within a couple of centuries and so on, perhaps, is a time now to retire the term AGI and to talk about just specific predictions. Because it's people mean a lot of different things. Perhaps if you if you asked Connelly in 2010, whether Chad GPT, or GPT. Four was an AGI system, what would you have responded?

I mean, depending on how well you described it, I would have said yes, but I do think these things are AGI so I still do. But just like people just don't like my definition of the word AGI. So I don't use that word very much. I agree that the word AGI is not particularly good. It's people use it to mean very, very different things. Like by reasonable definitions of AGI that were used, like 10 years ago, obviously charged up to AGI obviously, so, like most of the definitions of AGI from my 10 or 20 years ago, are you know, vaguely conducive human life things in some scenarios, right. And like, you know, and like, you know, reasonable human level performance, or like a pretty wide range of tasks, obviously, Gchat GPT, GP four have reached this level. And obviously, they have the ability to go beyond that. But as other definitions that they don't fulfill, you know, like, strictly better at humans at literally everything, like short term CBT is not that, but also low, like what you doing, perhaps that's not super interesting, there will always be 2% that where humans are just better. Sure, or people are just, you know, testing it wrong, or just not bothering to do it or whatever. It's like, the the real definition of AGI that I am most interested in personally is more vague than that. And it's something like a system that has the thing that humans have the chimps don't. It's like, you know, a saying that, you know, you know, human brains are basically scaled up chimp brains, you know, by like a factor of like three or four or something, they're all the same structure is all the same kind of things eternally, blah, blah, blah. But humans go to the moon chimps don't. So, you know, there's some people who are like, Oh, okay, but like, you know, there's always gonna be some tasks that a specialized, you know, tool like, you know, AGI couldn't fold proteins as good as alpha fold or whatever. And like a, it's just a short AGI, maybe a handful of proteins as good as alpha fold, but he can invent Alpha fold. So, the relevant thing I'm personally interested in is just like, a thing that is powerful enough to learn and do science and to pose potentially existential risks to humanity. Like there's a thing I personally care about. And when I talk about AGI, that's generally what I'm referring to, but I agree it's a bad meme. Like it's a bad word. Because other people are, as I've said before, some people in here AGI to things like you know, friendly human robot buddy who's like, you know, sort of smartest you but not really smarter, you know, but other people think, you know, AGI is godlike super thing that can do everything, live intermediate things. Yeah, you know, we can we can have an we'd have a semantic fight about

this, but so perhaps a way to resolve these issues is to make predictions of do would you be willing to make a prediction about when for example, a an academic paper created by an AI model would get published in say, a reasonably high quality scientific journal

that's under defined is like, does the system have a human to intervention is allowed here? Do I give it a prompt doesn't have to man you know, navigate the website and upload the paper itself? Say

say you give it a you can give it a detailed prompt and the system simply has to create the paper and nothing else?

Okay, so do does this have to have actually occurred or be possible because possible yesterday, actually occurred? I think there's no one's gotten around to it. I just No one's bothered to do this. Do you think this could be done now? Yeah, absolutely. Like obviously. So like the the SoCal affair, you know, already got papers published, you know, you know, decades ago, which complete nonsense. I think you could have done this with GPT. Two, probably, if you if you allow, like non stem journals, you may be knee deep T three for STEM journals. But like, have you read ml papers, like so many of them are so awful. Like, this is like not that hard.

So you're thinking this is basically already here now? Oh, yeah, absolutely.

But I think this is not capturing the thing you're interested in. The thing I think you're probably interested in is that like, can you do science? We're not interested in candidate trick reviewers into thinking something Good. So the question of when will the fate and publish the scientific paper, which are what I expect? You know, correct me if I'm wrong, but expect you're looking for the question, when you can do science, you're not looking for the question, How stupid are peer reviewers?

True? So how can we make this question interesting then is it? Is it when can a can an AI system publish a scientific paper that gets cited a lot? Or? Or is that also simply kind of a way of gaming the system? Or?

So there's various ways we can think about this. And, and I'm gonna give the unsatisfying, but I think correct answer, which is, by the time it could do that, it's too late. If you have a system that can fulfill whatever good criteria you can actually come up with, that actually means you can do actual science, it's too late. And I expect at that point, if we have not aligned such a system, you're not great and fun things, then, you know, end times are upon us. And we do not have much time left, if any, if you asked me to bet on these things, or like, you know, do you know with my real money, I

just like wouldn't, because I like I don't expect the better pay out? Do you expect AI's to publish credible scientific papers before they can empty a dishwasher? Credible are correct, those are different. Correct? Interesting scientific papers.

I expect that to happen probably before the dishwasher. Yeah, I can make a concrete prediction, I expect the world to end before you know, like, more than 10% of cars on the street are autonomous. Okay.

So what we have here is, is a scenario in which we are close to to transformative AI, we could call it or perhaps deadly AI? If we are very close, does this mean that the game board is kind of settled in a sense that the big players are the players that are going to take us all the way there? So for example, we could ask, Is it open AI that ends up creating transformative AI? Seems

pretty likely, in the current trajectory, if nothing changes, if government doesn't get involved, if, you know, culture doesn't shift, if people don't revolt, then yeah, I expect you know, open AI DeepMind, anthropic 80%. One, you know, 70%, one of them, and like, you know, let risk percentage like smeared over, you know, like other actors, or like actors that have yet merged. All

we were you getting hyped up on an exponential curve that's about to flatten off. So we'll be for example, because we run out of data, or we run out of accessible computer or something of that nature.

This is not something you see, you see coming, I don't see any reason to expect this. My general predictions are usually predict what if you don't know what the future is going to help predict that what just happened will happen again, and this is what I'm seeing? We're at the beginning of, you know, we're now in take off, you know, expenditures are happening. And will this flatten off at some point? Yeah, sure. I just expect that to be post apocalypse, let's take

a tour of the landscape of different alignment solutions or AI safety solutions. So the current paradigm that's that's used by opening AI, for example, is that you train on human created data. And then you do reinforcement learning from human feedback, kind of fine tuning the model afterwards, if nothing changes, if we are very close to transformative AI, if perhaps transformative AI is will be developed by opening I could this succeed as as a last option? Do you do think that reinforcement learning from human feedback could take us to something at least somewhat safe systems? No, this there's no chance of this paradigm working now.

Like it's not it's not even an alignment solution. It's not a proposal I don't think the people working on it will even claim that. Like, I'm pretty certain that if you asked like Paul Christianna, or something like is our lhf a solution to alignment, they will just see we're just saying no.

And for context, poor Christianna basically invented reinforcement learning from humans feedback. Yeah,

he was one of the core people involved in that. And I don't think he maybe some people involved in the creation of a claim this, I don't know. But I would expect that if you ask the people create these methods, is this an alignment solution? They would say no, and they don't expect this to work? Like, I don't think that Arlie CEF in any way addresses any of the actual problems of LeMans. It is a core problem of alignment is how do you get a very complex, powerful system that you don't understand, to reliably do something complicated that you can't fully specify in domains where you cannot supervise it? It's, you know, the principal agent problem writ large. RHF does not address this problem. It doesn't even claim does justice problem. There's no reason to expect that early tip should solve this problem. This is like, it's like, you know, clicker training in Alien. You know, it's like, you know, there is every time you do an hourly tiff update, so you can, if For those not familiar, you can imagine it's simplified as So the, you know, the model produces some text and you give it a thumbs up or a thumbs down. And then you do a gradient update to like, you know, make it more or less likely to do the stuff. You have no idea what is in that gradient. There is no idea what is learning, what is updating on, you know, let's say, you know, your model threatened some users or whatever, and you're saying, like, oh, that's bad. So give a thumbs down. Well, what does the model learn from this? Well, one thing it might learn is don't threatened users. Another thing you might learn is don't get caught threatening users. Or it could learn to use less periods, or, you know, don't use the word green, or like, like, like, who knows, like, in practice, and what's going to learn is a superposition of like, all of these like, or like, tons of these possible explanations. And it's going to, you know, change itself, like the minimum amount to like, fulfill this criteria, or like move in that direction. And in that domain, but you have no reason to expect this to generalize. Maybe it does show, maybe it does, but maybe it doesn't, like you have no reason to expect it to, there is no theory, there is no prediction, there is no safety. It is like, you know, it's like the, you know, Siegfried and Roy and the tiger, right? It's like, well, you know, we've raised it from birth, it's so nice, and then it falls you like why? Who knows, Tiger had a bad day, I don't know,

perhaps the counter arguments or something like this is that it's only when we begin interacting with these systems in an empirical way that we can actually make progress. The 10 years of alignment research before, say, 2017 didn't really bring us any closer. It was only when, when open AI began interacting with real systems that that we understood how they even worked, and therefore perhaps gained information about how to align them do buy the dope, and because,

I mean, like, which part of that is true, like a saying that like, no progress happened, when it was like, you know, three people in the basement working out with no funding is like, What the hell are you talking about a like, like a given it was three people in the basement, they made a lot of progress on predicting things that would happen on D confusing concepts on, you know, building an ontology for things that don't yet exist. This was extremely impressive, given the extremely low amount of effort put into this. And, you know, sure, they didn't, like, you know, they didn't solve alignment short, but like, has any progress happened alignment since then, it's like, not obvious to me that there's been more people using the word. There's a lot more papers about it. But like, and like, there's stuff like Arlo, Jeff and stuff, I don't consider Arlidge actually progress in a sentence regression. It's like, the fact that anyone and this is not meant to be as a critique per se, as the people who did orally check. I think they were fully aware that, hey, this is not alignment. It's just an interesting thing. We want to study a little bit, which I think is totally fair. So legitimate, you know, and like arledge has its purpose, it's, you know, it's a great way to make your product nicer, right? As like a capabilities method or lhf. That's totally fine. Like, you know, just Don't delude yourself into thinking that this is, you know, I don't buy this whole like, well, if it makes the model behave better, in some subset of scenarios, this is progress towards alignment, I think this is a really bad way of thinking about the problem. It's like saying, Well, if I hide my password file, two folders deep, then that security because you know, there are some scenarios where an attacker would not think to look two folders deep. And I'm like, sure, in like some trivial sense. That's true. But like, that's obviously not what we mean, when we talk about security. It's like if you if you encrypt the password file, but your encryption is bad. I'm like, okay, that's progress. But your encryption is bad. I'm like, Alright, cool. This is obviously safety. I accept this the safety, but now I have problems with your encryption, you know, system, that is progress and alignment that we can argue about. You moving a thing, your password dot txt, two folders deep, I don't consider progress. You weren't even trying to solve the problem, you were trying to do something different. You know, you don't think that Microsoft, being Chad or certainly was less aligned than Chad TBT. For not in the ways I care about, like, it's you. I think this is a stretching of what I use the term alignment for. So like, you can make that statement and this I think this is a completely legitimate way of defining the word alignment. If you want to define it that way. That is an okay thing to do. But it's not the thing I care about. I do not expect that if I had an underlined existential risk AGI and I did the chat GPT equivalent to it that that saves you. I think that gives you nothing you die anyways. Nature doesn't grade on a curve. Like just because you're You know 10% better if you don't meet the mark, you still die? Doesn't matter. It doesn't matter if I'm, you know, your smiley face was a little 10% larger than the next guy smiley face. If you're only painting largely you incrementally larger smiley faces, it doesn't matter.

So what about extrapolations from reinforcement learning from human feedback? For example, having AI's work to give feedback on other AIs? Could that maybe scale to something that would be more interesting for you? Why

would you expect that to work? Like, where does the safety come from here? There is no step in this process, where you actually are addressing the core difficulty of how do you deal with a system that will get more smart that will reflect upon itself that we'll learn more that is fundamentally alien with fundamentally alien goals encoded in ways we do not understand can access or modify, that is extrapolating into larger and larger domains that we cannot supervise? No part of this addresses this problem. It's like you can play shell games with where the difficulty is, until you confuse yourself sufficiently to thinking it's fine. This is a very common thing, how does the science all the time, especially if we're talking cryptography, there's the same, everyone can create a code complex enough that they themselves can break it? And this is a similar thing here, where I think everyone can create alignment scheme sufficiently complicated that they themselves think it's safe. But so what, like if you don't, if you just confuse where the safety part is, that doesn't actually give you safety?

What would be evidence that you're wrong, right here, for example, if it turns out that the GPT model that's that's available right now, is not used to create, you know, havoc in the world is not used to scam people and turns out to to not be dangerous in the sense that we expected would this be evidence that perhaps opening is doing something, right?

So a proof of this would be is that no one on the entire internet can find any way to make a model say something that if there's no prompt that can be found that makes it say something overnight, doesn't

want it to say, perhaps not not say something bad, it's

not specifically about bad words, or something is actually quite important. This is important, because what opening is trying to do is to stop the model from saying bad things. That's what they were trying to make it do. And they failed. That's the interesting thing. If they have an alignment technique, that actually worked, like SEC might have a chance to work on super intelligent system, it should be able to make your less smart systems

never in any scenario, say a bad thing. So it should be much more it should work in almost all cases, or basically all cases for.

So importantly, it has to be all cases. Because if it is not all cases, that I unless you have some extremely good theoretical reason why actually this is okay. But by default, these are black boxes. I don't accept any assumptions unless you give me a theory, a causal story about why I should relax my assumptions that I'm like, Well, if it's breakable, it will break. And this is the security mindset, the difference between security mindset and ordinary paranoia is ordinary paranoia assumes things are safe until proven otherwise, security mindset assumes things are unsafe until proven otherwise. And sure, you can't apply security mindset to literally everything all the time, because you go crazy, right? So but when we're dealing with existential threats of extremely powerful superhuman, optimizing systems, systems, whose whole purpose is to optimize reality into weird, you know, edge cases, to to fight to break systems to glitch to to, you know, enforce power upon systems, this is exactly the type of system you have to have a security mindset for. Because if you have a system is looking for a hole in your walls, you can't end it, you have one small hole, that's not good enough. If you have a system, which is you know, randomly probing your wall and you have one small hole. Yeah, maybe that's fine. If it's small enough, that's okay. But it's not okay. If it is deliberately looking for the small hole, and it's really good at finding them.

What about the industry of cybersecurity, for example, you would assume that they have a security mindset, or at least they should have, but accidents happen all the time data is leaked, and so on. So isn't that evidence that we can survive that we can survive situations where we should have had security where where there are holes in our security? So it's not actually true that systems have to work 100% of the time. The fact

that we survived has not anything to do with a security measure. It has to do with the seeing systems being secured not being substantial. If those systems had been existentially dangerous, he is yes, I expect we will be dead. It's

only because of the limited capabilities of these systems that can be hacked and then have been hacked and so on. Exactly. Let's let's take another paradigm of AI safety which is mechanistic interpretability and this is about Understanding what this black box machine learning system is doing trying to reverse engineer the algorithm that produced the neural network weights. Is this is this a hopeful paradigm, in your opinion?

I think it's definitely something we're worth working on. It's something that you know, me people conjecture work on as well. I think the way I think about interoperability, it's not as real as a alignment agenda, like alignment, like interoperability doesn't solve alignment, it might give us tools with which we can construct an aligned system, it might allow, like, the way I think about in my ontology is that I think of mechanistic interpretability, as attempting to move cognition from Black Box neural networks into white boxes. Again, as I've said, before, black box is observer dependent, you know, neural networks are not inherently black box, it's not like an inherent property of the territory, it's property of the math, if you have extremely good interpretability in your head, and extremely good theory, and extremely, you know, a lot of compute in your head and whatever, then a neural network would probably look like a white boss to you. And if that is the case, fantastic. Now you can like, you know, bound lots of things and maybe, like, stop it from doing bad things, and whatever. And so, I expect this. So this is like, like, this is the default thing I tell people to do. They don't know what to do. If they're like, I don't know what to do is alignment or safety. What I'm just like, okay, just work interoperability, just like just like, try just like just like, bang your head against it and just see what happens. Not because I think it's easy, I expect it to be very hard. I also think a lot of the current paradigm of mechanistic interpretability is not great. I think a lot of people are making simplifying assumptions I think they shouldn't be making. But in general, I'm in favor, and I think this is good.

It's one problem, or perhaps the main problem with interpretability research is just the question of whether it can move fast enough. So we're just beginning to understand the some interesting clusters of neurons in in GPT. Two, but right now GPT, five has been trained. And so can it keep up with the pace of progress? Do you think it can?

I mean, the same applies to like literally every other thing? My my default answer is no, I don't expect things to go well, like, again, I expect things to go poorly. I do not expect us to solve alignment on time, I expect things will not slow down sufficiently, I expect things will continue and expect us to die. I expect this as the default outcome, or the default world we are currently living in. This doesn't mean it has to happen. There is this important thing that, you know, the world being the way it is, is not overdetermined. It didn't have to be this way. The way the world currently is the path we're currently on is not determined by God. It is because of decisions that humans have made individual humans have made decisions and institutions and so on and made decisions. And, you know, done things in the past that have led us to the moment we are in now. This was not overdetermined. It didn't have to be this way. It doesn't have to continue to be this way. But it will, if nothing changes. So I do expect to rebuild it to work on time. No, don't expect them to work on time, though. Do expect rLf to work ever. No, I don't expect any of these things to work. That doesn't mean it's impossible. If we take action if things change if we slow down, or if we make some crazy breakthroughs or whatever. I think I think interpretability is like I think there's a lot that can be done here. You know, I think there's a lot of theory, there's a lot of you know, things that can be done here. Will they happen in really? Like are they possible to happen? Yes. Will they happen in time? Probably not.

Then there is the research done by Paul Christiano and AI to kowski at the alignment Research Center and the Machine Intelligence Research Institute. This is something that's that's for me at least difficult to understand, as I see it, it is it is attempting to make to use mathematics to prove something about the background assumptions that in which alignment is situated. What do you think of this research paradigm? Should we have? I mean, is there any hope here,

so I feel like both Paul and Eliezer would scream if you put them in the same bucket? I think their research is actually very different. So just to say a few words on that. The straw man, I'm gonna I'm specifically straw Manning, because Eliezer and Paul are some of the most difficult people to get their true opinions. Right. So basically, every single person I know, completely mischaracterizes Paul, even people who know him very well. Like whenever someone very knows recall very well. I asked him to pull beliefs. They tell me x and then I asked Paul, he tells me something different. So like, I think, I don't think this is malicious. I think it's just hit Paul. And you can see these opinions are very subtle and very complex and communicating them as hard. So I am perfectionist, I am definitely misunderstanding What Paul and Eliezer truly believe I can just give my best strawman. So my best straw man I can give for Christianna view is that he works on currently, something called elk, which is eliciting latent knowledge is kind of like this attempt to add that plus another thing that I'm aware of where he's trying to like, think of worst case scenarios. How can you get like true knowledge out of like neural networks? How can you get their true beliefs about system and not necessarily network like any system like arbitrary system, even if they're deceptive and also related to that he does some like semi formal work about like proofs and like calls out calls with tracing through neural networks and stuff like this. i This is a straw man, this is definitely not an accurate scripture for you actually does. But this is the closest I can get while Eliezer. Well, he's currently on hiatus. He's currently on sabbatical. So I don't think he's currently doing any research, actually. But historically, what Miri there in the organization that he founded does is kind of building formal models of agency, like trying to de confuse agency and intelligence are far more fundamental level than building like, you know, just doing some code and trying to build an AI and then figuring out how to align it it's way more thinking for first principles. What does an agent What does optimization what would it mean for systems to be aligned? Or corrigible? Can we like express these things formally? How can like systems know that? You know, they are their successes will be aligned? How can you prove things about themselves? How would they coordinate or like work together a lot of work on like decision theory on embedded agency, stuff like this. So I think a lot of the MIRI paradigm is a lot more subtle, then and then I understand the mirror paradigm better than I do, Paul. I think a lot of it is very subtle, but actually, I think a lot of the mirror work is very good. I think it's very good work. I think it's really interesting. But that's just my opinion. So when people talk about like formal mathematical theories, blah, blah, blah, they often refer I think, to like something that I think Eliezer said in the sequences where it's like, the only way to get aligned, AGI is like formally proof checked full thing, you know, solve all of alignment in theory, and then, you know, build AGI. I don't know, if he still believes this, he probably does. But I'm just saying I just don't know, I haven't. I don't think I've asked him maybe I've asked him, but I don't remember the answer. And I don't think Paul believes this. I like Paul, last time I talked to him again, this is straw man, please hold me to this, Paul, sorry for misrepresenting you here. My understanding is that he has like, you know, 30% P do even in on the current path or something like that, which obviously isn't going through formal methods. So by that I deduce that he doesn't expect this to be necessary. If that's wrong, I apologize. That's just my impression that Paul's quite open to non formal things and neural networks and that kind of stuff, wherever you guys are, kind of has this belief that like, if we, if it's just neural networks, we're super screwed. We're just super, super screwed. There's nothing we can do. It's way too hard. So like, a lot of the memory perspective, I think isn't like aligning neural networks is so hard that we have to develop something that is in neural networks that is easier to align, and they have to use that instead. And this has been not super successful. As far as I can tell. My view on this is not sure about Paul's agenda. bit pessimistic. I'm pretty pessimistic about elk. It seems too hard. pretty pessimistic about that. don't really understand the other stuff he's working on. can't really comment on that. I definitely disagree with him on some points about interpretability and P do and such. I think he's too optimistic about many things. But every time when I bring this up, he actually then has good counterpoint. So maybe he has some good counter points I just don't know about for Eliezer. I agree that in a good world. That's what we would do. Like I think in a good world, where people are seen and like coordinated. And we take lots of time, we would do much more theory like things not necessarily exactly what Marita did, I think some of the exact details of like, Emir's research agenda, or like not what I would have done, but like the general like class of things like let's do confused agency, let's do confuse alignment, core stability. And then like try to go formal models, and I try to understand that I think this is super sensible. I think this is like a super sensible thing to do. It didn't work in this one specific instance, given the constraints they had. I don't think that means the entire class of methodologies, you know, ontologically flawed and like cannot possibly work. Just like you know, they try If they found some things that I find interesting, and other things didn't work out, like, Bro, that's how science works. And perhaps Perhaps it could have worked if we had started in 1950 working on this and had developed it alongside the, you know, the general mathematics of computation or something like that. Yep, I think this is completely feasible. I think it's possible to just like if things had gone slightly differently, or just, you know, if Mary had, you know, one more John von Neumann get involved and get really intuitive the early days, like, you know, I think it's not obvious that this is 100 years away, or something like this. It might be, but it's not obvious to me, like things always feel impossibly far away until they're not. People thought, you know, flying machines were hundreds of years away the day before it happened. Same thing with like, you know, nuclear fusion and like fear fission and like stuff like that. So like, it's, I feel like nearly gets a bad rap. Is that Sure, they made some technical bets, and they didn't quite work out. But I think that's like, fair, so I'm pretty sympathetic to the Yudkowsky view, even so I am. So my personal view is kind of like, we're at the point, this is a strategic decision, like, Okay, if I had, if I knew I had 50 years, I'd probably work on the relay stuff. But I'm like, Alright, I don't have 50 years. So I'm, like, the kind of code and stuff I work on is more of a compromise between the various positions where we're like, all right, there's a spectrum between fully formal everything white box, and nothing formal, completely black box, let's try to move to as far towards the formal things as possible. But no further kind of that makes

any sense to us. So perhaps introduce Cohen's these cognitive emulations.

Yeah. So cognitive emulation, or Cohen is the agenda that I and conjecture are primarily focused on right now. It is a proposal or it for a more research agenda for how we could get towards more safe, useful, powerful AI systems by fundamentally trying to build bounded, understandable systems that emulate human like reasoning, not arbitrary reasoning, not like they just solve the problem by whatever means necessary, but they solve it in the way humans would solve a problem in a way that you can understand. So when you use a co M system, and so these are system talk models, like I always say, it's to be like design neural network. This is like a, it may involve neural networks, it probably does involve neural networks, but it'd be like a system of many sub components, which can include neural networks, but also include non neural network components, that when you use such a system, at the end, you get a causal story, you get a reason to believe that you can understand using human like reasoning, why it made the choices it did, why it did the things it did, and why you should trust the output to be valid in this regard. Yeah, and for listeners who were enticed by that description, they should go back and listen to the previous podcast in this series were caught on and me, we discussed this for an hour and a half. All right, so as we see more and more capable AI systems, do you expect us to also see more and more public attention?

Do you expect the public attention to kind of scale with with capabilities? Or will public attention lag behind and only come in? Right at the very end?

Both? In that I think we're at the very end, and we're starting to see the attention now.

Okay, do you think that this will on net be a positive or negative, so will public attention to AI make AI safer?

At the current point, I see it as positive. This is not obvious, it could still go negative very easily. But the way it currently see things is that all the everyone is racing headlong into the abyss. And at least what the public so far in my experience has been able to do is to notice, hey, wait, what the fuck don't do that, which is great progress. Right. But you can tell, it is truly maddening. How many smart you know, ml professors and whatever, are so incredibly, utterly resistant to the possibility that what they're doing might be bad or might be the interest is incredible the level of rationalization that people will capable of, I mean, it's not critical, like it's actually very expected, this is exactly what you expect. They rely on a man to understand something when his salary depends on him not understanding it. And even the people who claim to understand it and say all the right words, like you know, they still do it, like, you know, openly I can say all the nice words about alignment they want or anthropic or defined or whatever, they're still racing to AGI and they don't have an ultimate solution. So I don't like speculating about people's like, internal lines are like why are they doing it? Are they good? Would are they aligned or I don't really care, what I care about is what they do. And for me the writing's on the wall, just people are just careening towards the abyss. And if no intervention happens, if nothing changes, they will just go straight off that cliff with all of us in town. And I think the public, you know, even so they don't understand many things. And there's many ways in which they can make things worse, do seem to understand the very, very simple concept of don't careen off into the abyss. Stop that right now.

So here's an argument. Open AI releases GPT. Four, and this draws a lot of attention. And therefore we get more research sources into regulation and and AI safety research and so on. And so it's actually a good thing to release a model. That's, that's very capable, but not a super intelligent AGI is this is this galaxy brain. Is this 40 chests? Or do you think there's something there? Sure, there's

something there. But it is also just obviously, for the chest. Like, it's like, okay, if you had a theory with which you can predict where the abyss is? Sure, okay. There is no such theory. You have no idea what these systems can do. When people get their hands on, you have no idea what happens when you augment them with tools, or whatever, you have no idea what these things can do. There's no bounds. There's no limit or whatever. So every time you release a model, every time you build such a model, you're rolling a die. So maybe this sounds fine. Maybe next time sign. But at some point, it won't be it's Russian roulette. Sure, you know, you can you can play some Russian Roulette most of the time, it's fine. You know, five out of six people say Russian Roulette is fine. What

about possible counterproductive overreactions? From more public attention to AI, for example, imagine that, we decided to pause AI research, but AI safety research gets lumped in with AI capabilities research. And so even though we're not making any progress on capabilities, we're not making any progress on safety either. And when we lift this pass, we are in the same place as when we instigated it.

Honestly, I'd be extremely surprised if that happened. Like I'm trying to imagine how that would actually play out in the real world. Like, people won't even accept like a moratorium on training things larger than GPT for which, like, the easiest thing to implement the easiest thing to monitor that effects like, you know, a teeny tiny sliver of all AI research, like there are so few people that could ever would train a GP for size model. And, you know, that's such a teeny tiny sliver of AI research and not even that is like, feasible in the current political world is like very hard to get done. It's like, an overreach so large that you know, Miri doodling, you know, type theory on their whiteboards gets shut down. I'm like, Oh, that's not the world we live in. Like, if we were in that world, it'd be like, Okay, interesting. Let's talk about it. But this is just not the world we live in. What about AI

becoming a military technology, and only the only militaries can work on it? And perhaps they work on it in ways that turn out to be dangerous? Yep,

I am concerned about this. I think this is one of the ways things can go really badly. I used to be more virulent ly against this I am now. Now in another sense, I look at where we're currently heading. And I'm like, Alright, currently we have 100% chance. What are the other options? Right? And so look, I'm not going to defend, like many of the atrocities committed by militaries across the world or whatever, right? I'm not gonna say that there's not problems here. I'm not gonna, like, say, deny that there's some really fucked up people involved in these in these organizations or anything like that. Of course, there are, but also, at least in the Democratic West, you know, don't want to speak about other nations. But like, there is such thing as oversight. Like, there are court martial means it this is an actual thing actually happens. In a sense, the military is authoritarian in a good way. Like, the military is very authoritarian, there is hierarchy, there is authority, there is accountability, there is structure, like they're like, the US military does a lot of bad things, but it least to some degree, they are accountable to the American public, like not perfectly, there's lots of problems here. But like, if a senator wants a hearing, to investigate something going on the military, they can usually get it, which you know, is not perfect, you know, huge problems or whatever, but I'm like, that's something and like, people do look like, you know, politicians do care, you know, they might make very stupid mistakes, and you might make stupid things. I might make things worse, like the DoD could scale up, you know, like, you know, GPD for very easily, they could make something much bigger than that, you know, if they, if they did a Manhattan Project, and they, you know, put all the money together to create you know, GPT You know, you know, GPF, you know, just like and of the world system, then they, they could and that would be bad. So I think it can make it worse, but it's not obviously. So it's not like it could also be that they, you know, a like, also it's just like super incompetence slow, you know, bureaucratic mess. And the military is very conservative, like, very, very conservative about what they deploy volunteers, they want extremely high levels of security, they want extremely high levels of reliability before they use anything. Like if we built AI systems to military standard of like reliability, like, like the military requested that, like every AI system is like, you know, as reliable as a flight control system. I'd be like, let's not fucking great. Like, that sounds awesome. Of course, that is a rosy view, like probably, when I think it's not a question if military gets involved, I think it's a question of when. And when this happens, it probably as the law of undignified failure goes, if the thing can fail, it will always fail and the least dignified way possible. So probably, it won't get to this level. But I think we should not dismiss out of hand that. I mean, first of all, I think it's ridiculous to accept that the military will not get involved. I think this is just impossible at this point, unless we get you know, paperclip tomorrow, like on things unless things go so fast that anyone can react, military will get involved, and we should work with them. We should be there to like, be like, Alright, how can we help the military handle this as non stupidly as possible? And I do think that a lot of people that work in the military do care and would like things to be safe and work well. So is it worse than you know, Sam Altman, you know, all like Dr. Strangelove style, you know, is running you know, things as fast as possible. Is it worse if you know the military nationalizes the whole thing and grinds into this bureaucratic monstrosity? not obvious to me. Not saying I know obviously it is good, but it's obviously not good. All right. Connor,

thank you for coming on the podcast.

pleasure as always


Transcribed by https://otter.ai