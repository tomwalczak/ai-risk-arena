{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f418b28d-060a-4e1c-b804-d76264ca0e0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:15:31.499674Z",
     "start_time": "2024-04-11T08:15:23.830030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.1.8 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (0.1.8)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain==0.1.8) (6.0.1)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain==0.1.8) (2.0.29)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain==0.1.8) (3.9.3)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain==0.1.8) (4.0.3)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain==0.1.8) (0.6.4)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain==0.1.8) (1.33)\r\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain==0.1.8) (0.0.32)\r\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.24 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain==0.1.8) (0.1.41)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain==0.1.8) (0.1.45)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain==0.1.8) (1.26.4)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain==0.1.8) (2.6.4)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain==0.1.8) (2.31.0)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain==0.1.8) (8.2.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (23.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (1.9.4)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.8) (3.21.1)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.8) (0.9.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.8) (2.4)\r\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.24->langchain==0.1.8) (23.2)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain==0.1.8) (3.10.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.8) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.8) (2.16.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.8) (4.9.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.8) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.8) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.8) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.8) (2024.2.2)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.8) (3.0.3)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.8) (1.0.0)\r\n",
      "Requirement already satisfied: langchain-openai==0.0.6 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (0.0.6)\r\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain-openai==0.0.6) (0.1.41)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain-openai==0.0.6) (1.26.4)\r\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain-openai==0.0.6) (1.17.0)\r\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain-openai==0.0.6) (0.6.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (6.0.1)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (1.33)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (0.1.45)\r\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (23.2)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.6.4)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (8.2.3)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (4.2.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (0.27.0)\r\n",
      "Requirement already satisfied: sniffio in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (1.3.0)\r\n",
      "Requirement already satisfied: tqdm>4 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (4.66.2)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (4.9.0)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.0.6) (2023.12.25)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.0.6) (2.31.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (3.4)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (1.2.0)\r\n",
      "Requirement already satisfied: certifi in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (2024.2.2)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (1.0.5)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (0.14.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.4)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (3.10.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.16.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai==0.0.6) (3.3.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai==0.0.6) (2.1.0)\r\n",
      "Requirement already satisfied: python-dotenv==1.0.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (1.0.0)\r\n",
      "Requirement already satisfied: chromadb==0.4.18 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (0.4.18)\r\n",
      "Requirement already satisfied: requests>=2.28 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (2.6.4)\r\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (0.7.3)\r\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (0.110.1)\r\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (0.29.0)\r\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (3.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (4.9.0)\r\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (3.4.0)\r\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (1.16.3)\r\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (1.24.0)\r\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (1.24.0)\r\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (0.45b0)\r\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (1.24.0)\r\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (0.15.2)\r\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (0.48.9)\r\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (4.66.2)\r\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (7.4.0)\r\n",
      "Requirement already satisfied: importlib-resources in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (6.4.0)\r\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (1.62.1)\r\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (4.1.2)\r\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (0.12.3)\r\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (29.0.0)\r\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (8.2.3)\r\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (6.0.1)\r\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (4.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.22.5 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from chromadb==0.4.18) (1.26.4)\r\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb==0.4.18) (0.37.2)\r\n",
      "Requirement already satisfied: certifi>=14.05.14 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (2024.2.2)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (1.16.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (2.8.2)\r\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (2.29.0)\r\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (0.58.0)\r\n",
      "Requirement already satisfied: requests-oauthlib in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (2.0.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (3.2.2)\r\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (2.1.0)\r\n",
      "Requirement already satisfied: coloredlogs in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (15.0.1)\r\n",
      "Requirement already satisfied: flatbuffers in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (24.3.25)\r\n",
      "Requirement already satisfied: packaging in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (23.2)\r\n",
      "Requirement already satisfied: protobuf in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (4.25.3)\r\n",
      "Requirement already satisfied: sympy in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (1.12)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.18) (1.2.14)\r\n",
      "Requirement already satisfied: importlib-metadata<=7.0,>=6.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.18) (7.0.0)\r\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.18) (1.63.0)\r\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.18) (1.24.0)\r\n",
      "Requirement already satisfied: opentelemetry-proto==1.24.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.18) (1.24.0)\r\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.45b0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (0.45b0)\r\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.45b0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (0.45b0)\r\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (0.45b0)\r\n",
      "Requirement already satisfied: opentelemetry-util-http==0.45b0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (0.45b0)\r\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (68.2.2)\r\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (1.16.0)\r\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (3.8.1)\r\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.18) (1.6)\r\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.18) (2.2.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from pydantic>=1.9->chromadb==0.4.18) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from pydantic>=1.9->chromadb==0.4.18) (2.16.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.18) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.18) (3.4)\r\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb==0.4.18) (0.22.2)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.18) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.18) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.18) (13.7.1)\r\n",
      "Requirement already satisfied: h11>=0.8 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.18) (0.14.0)\r\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (0.6.1)\r\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (1.0.0)\r\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (0.19.0)\r\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (0.21.0)\r\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (12.0)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.18) (5.3.3)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.18) (0.4.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.18) (4.9)\r\n",
      "Requirement already satisfied: filelock in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.18) (3.13.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.18) (2024.3.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.18) (3.18.1)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.18) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.18) (2.15.1)\r\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb==0.4.18) (4.2.0)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.18) (10.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.18) (1.3.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb==0.4.18) (1.3.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb==0.4.18) (1.2.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.18) (0.1.2)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/macbookpro/opt/anaconda3/envs/ai-safety-arena/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.18) (0.6.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.1.8\n",
    "!pip install langchain-openai==0.0.6\n",
    "!pip install python-dotenv==1.0.0\n",
    "!pip install chromadb==0.4.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98356761-e795-4338-9aef-8d05b212388f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:15:50.956757Z",
     "start_time": "2024-04-11T08:15:50.952541Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "106d34e5-61c0-4667-81ea-18bd8437d645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:06.618973Z",
     "start_time": "2024-04-11T14:17:06.598094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d7dbba34-6940-430b-8057-419f77abf163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:09.015199Z",
     "start_time": "2024-04-11T14:17:08.944252Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-turbo\",\n",
    "                 temperature=0.7,\n",
    "                 model_kwargs={\n",
    "                    \"frequency_penalty\": 0.0,\n",
    "                     \"presence_penalty\": 0.0,\n",
    "                     \"top_p\": 1.0,\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "729fece9-860b-4fd9-9e40-10f2962026bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:11.592018Z",
     "start_time": "2024-04-11T14:17:11.577003Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_to_file(filename, text):\n",
    "    try:\n",
    "        directory = os.path.dirname(filename)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        with open(filename, 'a') as file:\n",
    "            file.write(text)\n",
    "        print(\"Text successfully written to\", filename)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cd30f70b-2af9-4a99-8018-f7ebbe34108f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:14.291166Z",
     "start_time": "2024-04-11T14:17:14.287058Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_dict_to_json(data, filename):\n",
    "    directory = os.path.dirname(filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c78957f9-1683-4c12-ac90-136b14419ee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:16.344374Z",
     "start_time": "2024-04-11T14:17:16.340455Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_results_from_json(int):\n",
    "    if int == 1:\n",
    "        with open(\"./sources/json/first_cycle_results.json\", 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    if int == 2:\n",
    "        with open(\"./sources/json/second_cycle_results.json\", 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    if int == 3:\n",
    "        with open(\"./sources/json/third_cycle_results.json\", 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8360efe1-f307-466d-aa59-312cf30f0cdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:22.226198Z",
     "start_time": "2024-04-11T14:17:22.220334Z"
    }
   },
   "outputs": [],
   "source": [
    "first_cycle_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"\"\"\n",
    "    Read the following transcript and extract all the arguments made about AI safety. Make sure they are self-contained.\n",
    "\n",
    "  You must stick as close as possible to the transcript - use the author's own words and tone of voice.\n",
    "\n",
    "  You must write each argument in a valid YAML format, surrounded with backticks.\n",
    "  You must separate each argument with a new line.\n",
    "\n",
    "\n",
    "  The simplest possible argument must at least contain three claims:\n",
    "\n",
    "  ```yaml\n",
    "  claim: \"Top-level claim\"\n",
    "  premises:\n",
    "    - claim: \"First independent premise supporting the top-level claim\"\n",
    "    - claim: \"Second independent premise supporting the top-level claim\"\n",
    "  ```\n",
    "\n",
    "  And here's an example of a more complex argument, which also includes examples to illustrate lower-level claims:\n",
    "\n",
    "  ```yaml\n",
    "  claim: \"Top-level claim\"\n",
    "  premises:\n",
    "    - claim: \"First independent premise supporting the top-level claim\"\n",
    "    - claim: \"Supporting premise for the top-level claim\"\n",
    "      example: \"Example supporting this premise\"\n",
    "    - claim: \"Another supporting premise for the top-level claim\"\n",
    "    - claim: \"Second independent premise supporting the top-level claim\"\n",
    "      premises:\n",
    "        - claim: \"Supporting premise for the second independent premise\"\n",
    "        - claim: \"Another supporting premise for the second independent premise\"\n",
    "        - claim: \"Independent premise supporting the second independent premise\"\n",
    "          example: \"Example supporting this independent premise\"\n",
    "  ```\n",
    "\n",
    "  Here's how to read this structure:\n",
    "\n",
    "  The top-level claim is the main argument.\n",
    "  Directly nested under the claim are independent premises. These provide justification independently of other premises.\n",
    "  An example can be nested directly under a claim to provide further context or support.\n",
    "  Just like the top-level claim, each premise can itself be supported by further individual premises, or examples, creating a nested structure.\n",
    "\n",
    "  # Here is the transcript:\n",
    "\n",
    "  {transcript}\n",
    "  \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f4ae46fd-4e4d-4780-9b11-f839d2a02e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:23.702811Z",
     "start_time": "2024-04-11T14:17:23.696224Z"
    }
   },
   "outputs": [],
   "source": [
    "second_cycle_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"\"\"\n",
    "  Based on the following transcript, make the arguments clear and distinct. \n",
    "  You may need to merge similar arguments to create a better, more logical argument.\n",
    "\n",
    "  Create the best, strongest possible version of the arguments, here's what to do:\n",
    "\n",
    "  - Make sure the arguments is self-contained\n",
    "  - Make the arguments understandable on their own, out-of-context\n",
    "  - Remember, arguments are not a description or an explanation\n",
    "  - Premise must always give a reason to believe the claim above\n",
    "  - Avoid using pronouns in premises\n",
    "  - A claim can have a maximum of two child claims (premises), rewrite if needed\n",
    "\n",
    "  # Argument format \n",
    "\n",
    "  You must write each argument in valid YAML format, surrounded with backticks.\n",
    "  Separate each argument with new line.\n",
    "  You must stick as closely as possible to the transcript. \n",
    "  Above all, you must express the argument in the words of the author, stick as close as possible to the tone of voice and phrases used in the transcript.\n",
    "\n",
    "\n",
    "\n",
    "  Here are some examples:\n",
    "\n",
    "  A simple argument might look like this: \n",
    "\n",
    "  ```yaml\n",
    "  claim: \"Top-level claim\"\n",
    "  premises:\n",
    "    - claim: \"First independent premise supporting the top-level claim\"\n",
    "    - claim: \"Second independent premise supporting the top-level claim\"\n",
    "  ```\n",
    "\n",
    "  And here's an example of a more complex argument, which also includes examples to illustrate lower-level claims:\n",
    "\n",
    "  ```yaml\n",
    "  claim: \"Top-level claim\"\n",
    "  premises:\n",
    "    - claim: \"First independent premise supporting the top-level claim\"\n",
    "    - claim: \"Supporting premise for the top-level claim\"\n",
    "      example: \"Example supporting this premise\"\n",
    "    - claim: \"Another supporting premise for the top-level claim\"\n",
    "    - claim: \"Second independent premise supporting the top-level claim\"\n",
    "      premises:\n",
    "        - claim: \"Supporting premise for the second independent premise\"\n",
    "        - claim: \"Another supporting premise for the second independent premise\"\n",
    "        - claim: \"Independent premise supporting the second independent premise\"\n",
    "          example: \"Example supporting this independent premise\"\n",
    "  ```\n",
    "\n",
    "  Here's how to read this structure:\n",
    "\n",
    "  The top-level claim is the main argument.\n",
    "  Directly nested under the claim are independent premises. These provide justification independently of other premises.\n",
    "  An example can be nested directly under a claim to provide further context or support.\n",
    "  Just like the top-level claim, each premise can itself be supported by further individual premises, or examples, creating a nested structure.\n",
    "\n",
    "\n",
    "  # Arguments to improve:\n",
    "\n",
    "  {all_arguments}\n",
    "\n",
    "  # Transcript\n",
    "\n",
    "  {transcript}\n",
    "  \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7aa75b11-6b3b-493c-8c32-01ef58a562cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:26.711954Z",
     "start_time": "2024-04-11T14:17:26.706788Z"
    }
   },
   "outputs": [],
   "source": [
    "third_cycle_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world expert at creating accessible, persuasive explanations.\"),\n",
    "    (\"user\", \"\"\"\n",
    "  Based on your own knowledge and the transcript, create a structured explanation for the following argument. Use the context from the transcript for the explanation.\n",
    "\n",
    "  # Argument to use for the explanation\n",
    "\n",
    "  {argument}\n",
    "\n",
    "\n",
    "  The structured explanation must be directly based on the argument. You can also use the provided transcript for context.\n",
    "\n",
    "  You must follow this YAML format:\n",
    "\n",
    "  ```yaml\n",
    "  counteragument_to: (what would be the argument, to which this argument is a counterargument? use your own knowledge. use bullet points)\n",
    "\n",
    "  strongest_objection: (what is the strongest, good-faith, honest objection that a thoughful person might have? use bullet points)\n",
    "  consequences_if_true: (if true, what would be the consequences? write in causal language,  use bullet points, max 3)\n",
    "\n",
    "  link_to_ai_safety: (how is this linked to AI safety? 1 sentence.)\n",
    "\n",
    "  simple_explanation: (explain this clearly to a college student in max. 4 sentences, speak persuasively as the author of this argument. don't use bullet points)\n",
    "\n",
    "  examples: (max 3 examples, use bullet points)\n",
    "\n",
    "  ```\n",
    "\n",
    "  # Here is the transcript:\n",
    "\n",
    "  {transcript}\n",
    "  \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "274af6ed-0d16-4e5b-ae57-868d7418efa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:22:12.279536Z",
     "start_time": "2024-04-11T08:22:12.272852Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_text_from_sources_and_make_chunks(directory):\n",
    "    folder_names = []\n",
    "    raw_texts = []\n",
    "    for entry in os.listdir(directory):\n",
    "        folder_names.append(entry)\n",
    "        print(folder_names)\n",
    "    for folder_name in folder_names:\n",
    "        loader = DirectoryLoader(f'./sources/{folder_name}/source', glob=\"**/*.txt\")\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=10000,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len)\n",
    "        raw_text = loader.load()\n",
    "        entire_text_from_a_source = \"\"\n",
    "        for doc in raw_text:\n",
    "            entire_text_from_a_source += doc.page_content\n",
    "        chunks = text_splitter.create_documents([entire_text_from_a_source])\n",
    "        formatted_chunks = []\n",
    "        for chunk in chunks:\n",
    "            formatted_chunks.append(chunk.page_content)\n",
    "        temp = {\n",
    "            \"name\" : folder_name,\n",
    "            \"path\": f'./sources/{folder_name}',\n",
    "            \"chunks\": formatted_chunks,\n",
    "        }\n",
    "        raw_texts.append(temp)\n",
    "    return raw_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8cffef20-d5b3-474c-995c-636f00c05cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:22:27.776139Z",
     "start_time": "2024-04-11T08:22:14.374332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['david_deutsch_transcripts']\n",
      "['david_deutsch_transcripts', 'json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "sources_dicts = load_text_from_sources_and_make_chunks(\"./sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bafcb6e1-e0fb-41bd-8469-b463d105ee1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:23:15.750986Z",
     "start_time": "2024-04-11T08:23:15.742372Z"
    }
   },
   "outputs": [],
   "source": [
    "def first_cycle_of_extracting_arguments(dicts):\n",
    "    first_cycle_chain = first_cycle_prompt | llm\n",
    "    for dict in dicts:\n",
    "        dict[\"arguments\"] = []\n",
    "        for chunk in dict[\"chunks\"]:\n",
    "            first_cycle_response = first_cycle_chain.invoke({\"transcript\": chunk})\n",
    "            dict[\"arguments\"].append(first_cycle_response.content)\n",
    "        print(\"over\")\n",
    "        text = '\\n\\n'.join(dict[\"arguments\"])\n",
    "        filename = f\"{dict['path']}/steps/first_step.md\"\n",
    "        write_to_file(filename, text)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "16983b7e-dc71-418e-b5e4-f340b1846e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:30:43.919323Z",
     "start_time": "2024-04-11T08:26:00.234801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/first_step.md\n",
      "over\n",
      "Text successfully written to ./sources/json/steps/first_step.md\n"
     ]
    }
   ],
   "source": [
    "dicts_with_extracted_args = first_cycle_of_extracting_arguments(sources_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9d0d68af-6a24-40fd-aa63-7ae2cdfb0470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:31:22.861422Z",
     "start_time": "2024-04-11T08:31:22.855702Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dict_to_json(dicts_with_extracted_args, \"./sources/json/first_cycle_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2acb9d74-4988-4f3e-9cb3-9a53956ec7b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:31:24.641759Z",
     "start_time": "2024-04-11T08:31:24.633681Z"
    }
   },
   "outputs": [],
   "source": [
    "def second_cycle_of_extracting_arguments(dicts):\n",
    "    second_cycle_chain = second_cycle_prompt | llm\n",
    "    for dict in dicts:\n",
    "        dict[\"improved_arguments\"] = []\n",
    "        for i, chunk in enumerate(dict[\"chunks\"]):\n",
    "            second_cycle_response = second_cycle_chain.invoke({\"all_arguments\": dict[\"arguments\"][i], \"transcript\": chunk })\n",
    "            dict[\"improved_arguments\"].append(second_cycle_response.content)\n",
    "        print(\"over\")\n",
    "        text = '\\n\\n'.join(dict[\"improved_arguments\"])\n",
    "        filename = f\"{dict['path']}/steps/second_step.md\"\n",
    "        write_to_file(filename, text)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9b45be49-f2ce-4e39-95e4-bd7ed762809b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:36:46.573109Z",
     "start_time": "2024-04-11T08:31:27.330814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/second_step.md\n",
      "over\n",
      "Text successfully written to ./sources/json/steps/second_step.md\n"
     ]
    }
   ],
   "source": [
    "dicts_with_extracted_and_improved_args = second_cycle_of_extracting_arguments(dicts_with_extracted_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "41ae0410-b3c8-4c53-8451-8dbe915c6bde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:04.364822Z",
     "start_time": "2024-04-11T08:37:04.358169Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_improved_arguments_in_dict(dicts):\n",
    "    for dict in dicts:\n",
    "        dict[\"isolated_arguments\"] = []\n",
    "        for improved_arg in dict[\"improved_arguments\"]:\n",
    "            if improved_arg:\n",
    "                splitted_args = improved_arg.split(\"```yaml\")\n",
    "                splitted_args_cleaned = []\n",
    "                for arg in splitted_args:\n",
    "                    arg_clean = arg.split(\"```\")[0]\n",
    "                    if (arg_clean != \"\"):\n",
    "                        splitted_args_cleaned.append(arg_clean.strip())   \n",
    "                if splitted_args_cleaned != None:\n",
    "                    dict[\"isolated_arguments\"].append(splitted_args_cleaned)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a6176109-6d8a-4e58-861b-e8707aeb0b0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:38:15.049848Z",
     "start_time": "2024-04-11T08:38:15.045485Z"
    }
   },
   "outputs": [],
   "source": [
    "dicts_with_isolated_improved_arguments = split_improved_arguments_in_dict(dicts_with_extracted_and_improved_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8a1eec61-b2d6-43c5-bde6-26e2bcbd1725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:15:09.442636Z",
     "start_time": "2024-04-11T12:15:09.373322Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dict_to_json(dicts_with_isolated_improved_arguments, \"./sources/json/second_cycle_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0bbb2a99-f23f-4c28-9812-386bdf73ded8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T13:13:02.090399Z",
     "start_time": "2024-04-11T13:13:02.082830Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_embeddings_with_smaller_chunks(directory):\n",
    "    folder_names = []\n",
    "    all_chunks = []\n",
    "    dbs = []\n",
    "    for entry in os.listdir(directory):\n",
    "        if entry != \"json\":\n",
    "            folder_names.append(entry)\n",
    "    for folder_name in folder_names:\n",
    "        loader = DirectoryLoader(f'./sources/{folder_name}/source', glob=\"**/*.txt\")\n",
    "        text_splitter = TokenTextSplitter(chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len)\n",
    "        raw_text = loader.load()\n",
    "        entire_text_from_a_source = \"\"\n",
    "        for doc in raw_text:\n",
    "            entire_text_from_a_source += doc.page_content\n",
    "        chunks = text_splitter.create_documents([entire_text_from_a_source])\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata = {\"folder\": folder_name}\n",
    "            all_chunks.append(chunk)\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "        db = Chroma.from_documents(all_chunks, embeddings)\n",
    "        dbs.append(db)\n",
    "    return dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "71a58393-7906-47ea-8070-6082914bf81c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T13:13:09.966428Z",
     "start_time": "2024-04-11T13:13:03.882656Z"
    }
   },
   "outputs": [],
   "source": [
    "dbs = create_embeddings_with_smaller_chunks(\"./sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e2a48a0d-bf72-40fb-85f9-5033db492991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:54:08.800908Z",
     "start_time": "2024-04-11T14:54:08.790963Z"
    }
   },
   "outputs": [],
   "source": [
    "def third_cycle_of_extracting_arguments(dicts):\n",
    "    third_cycle_chain = third_cycle_prompt | llm\n",
    "    for i, dict in enumerate(dicts):\n",
    "        dict[\"explanations\"] = []\n",
    "        if i < len(dbs):  \n",
    "         db = dbs[i]\n",
    "        else:\n",
    "         db = None\n",
    "        for i, chunk in enumerate(dict[\"chunks\"]):\n",
    "            explanations = []\n",
    "            for arg in dict[\"isolated_arguments\"][i]:\n",
    "                docs = db.similarity_search(arg, k=3)\n",
    "                print(docs)\n",
    "                context = \"\"\n",
    "                for doc in docs:\n",
    "                    context += \"\\n\" + doc.page_content\n",
    "                explanation = third_cycle_chain.invoke({\"argument\": arg, \"transcript\": context})\n",
    "                explanations.append(explanation.content.split(\"```yaml\\n\")[1].split(\"```\")[0].strip())\n",
    "            dict[\"explanations\"].append(explanations)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d5de267f-8b42-48fa-8ad5-a5a5341d60fb",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-04-11T15:27:54.098431Z",
     "start_time": "2024-04-11T14:54:12.984960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\"ulie works by things like empiricism, induction, Bayesianism, Bayesian epistemology. And so AI, so first I guess my first question is AI, current AI, following that?\\n\\nDavid Deutsch Current AI was inspired by that but that's not what it's doing. It's not doing induction any more than anything else is. Induction is impossible. Also, current AI was also inspired by the architecture of the brain, neural nets. There are these programming hardware, computer hardware devices that are modelled on how neurons work. Now I think that's a coincidence. I mean it's possible that the neuron architecture makes things like pattern recognition and extrapolation and so on a bit more efficient. But because of computational universality we know that that can't be fundamental and in fact, you know, you can download a neural net based computer program onto your home computer, which doesn't have a neural net in it, and it'll still work, even though it's a bit slower.\\n\\nLulie If AGI cannot come from AI, what would create AGI in your view?\\n\\nDavid Deutsch I can only say very little about that. From Popper's epistemology we can infer a few things about what it must look like, but far from enough to make one. So one thing is that there cannot be a specification of a program for AGI in the sense of saying what properties its output must have either for a given input, because, for example, an AGI may choose not to answer. It may choose never to answer, it might choose to become a hermit. Are you saying that current views of AGI are all about the output? Yes, they're all about either the output itself or more often how the output must be related to the input. So if we can't judge an AGI based on the output, how can we judge it? Yes, we can't judge an AGI or a human. There can't be a reliable test of whether a human is thinking. What about the Turing test is something that has been invented after Turing. It's been based on a misconception about passage in Turing's 1950 paper called, the paper was called Can Machines Think? and unlike most titles which are questions, the answer was yes rather than no. He included a section on a thing called the imitation game, he called it the imitation game, where an AGI, he just assumes it is an AGI, is pretending to be a human, and he is saying, suppose it could pretend to be a human sufficiently well for the sceptics who think that AGI isn't possible, not to be able to tell the difference between it and an actual human. What would happen? What would these skeptics say about that?\\n\\nWell, if they said, well, it's still not thinking, it just seems to be, then they're vulnerable to the criticism, but that is all the information you have about whether a human is thinking.\\n\\nLulie Wait, so what, so, sorry, why is the test not working or, I didn't, I don't, I didn't\\n\\nDavid Deutsch quite follow. It's not saying that something that can pass this test is necessarily an AGI, or that something that can't pass the test is, necessarily isn't an AGI. That's not what this game is for. What's it for? It's for persuading people that machines could think. How does it do that? By imagining a computer program that could fool people into thinking it was a person. There must be such a program because of computational universality. Ah, it was an argument about universality. Well it just assumed universality. Turing had proved the existence of universality, conjectured, but basically proved the existence of universality 14 years earlier, in 1936, and he was just thinking of that among many consequences of computation, which was a fairly new concept at the time. So I still don't get what the thought experiment intended to persuade the reader, in case the reader was sceptical that a machine can think.\\n\\nLulie So it did that by imagining a situation in which there is a computer and the computer and so a human can have a conversation with another human and that's fine and that would be fairly persuasive that I'm talking to a person and because computers can produce any output you can imagine a computer that produces exactly the same output as that hypothetical human.\\n\\nDavid Deutsch So his imitation game started out with that as a premise that such a computer can exist, that such a computer program can exist because of universality. And then he imagined playing this game in which you have the computer and a human both talking at long distance with the sceptic, the human sceptic who thinks that machines can't think. So the sceptic would then be unable to tell the difference\", metadata={'folder': 'david_deutsch_transcripts'})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithError(\"Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPSConnectionPool(host='api.smith.langchain.com', port=443): Read timed out. (read timeout=10.0)\\n\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=' perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans’ algorithm exactly. Yet that would have achieved nothing except an increase in the error rate, due to increased numbers of glitches in the more complex machinery. Similarly, the humans, given different instructions but no hardware changes, would have been capable of emulating every detail of the Difference Engine’s method — and doing so would have been just as perverse. It would not have copied the Engine’s main advantage, its accuracy, which was due to hardware not software. It would only have made an arduous, boring task even more arduous and boring, which would have made errors more likely, not less.\\n\\nBabbage knew that it could be programmed to do algebra, play chess, compose music, process images and so on\\n\\nFor humans, that difference in outcomes — the different error rate — would have been caused by the fact that computing exactly the same table with two different algorithms felt different. But it would not have felt different to the Difference Engine. It had no feelings. Experiencing boredom was one of many cognitive tasks at which the Difference Engine would have been hopelessly inferior to humans. Nor was it capable of knowing or proving, as Babbage did, that the two algorithms would give identical results if executed accurately. Still less was it capable of wanting, as he did, to benefit seafarers and humankind in general. In fact, its repertoire was confined to evaluating a tiny class of specialised mathematical functions (basically, power series in a single variable).\\n\\nThinking about how he could enlarge that repertoire, Babbage first realised that the programming phase of the Engine’s operation could itself be automated: the initial settings of the cogs could be encoded on punched cards. And then he had an epoch-making insight. The Engine could be adapted to punch new cards and store them for its own later use, making what we today call a computer memory. If it could run for long enough — powered, as he envisaged, by a steam engine — and had an unlimited supply of blank cards, its repertoire would jump from that tiny class of mathematical functions to the set of all computations that can possibly be performed by any physical object. That’s universality.\\n\\nBabbage called this improved machine the Analytical Engine. He and Lovelace understood that its universality would give it revolutionary potential to improve almost every scientific endeavour and manufacturing process, as well as everyday life. They showed remarkable foresight about specific applications. They knew that it could be programmed to do algebra, play chess, compose music, process images and so on. Unlike the Difference Engine, it could be programmed to use exactly the same method as humans used to make those tables. And prove that the two methods must give the same answers, and do the same error-checking and proofreading (using, say, optical character recognition) as well.\\n\\nBut could the Analytical Engine feel the same boredom? Could it feel anything? Could it want to better the lot of humankind (or of Analytical Enginekind)? Could it disagree with its programmer about its programming? Here is where Babbage and Lovelace’s insight failed them. They thought that some cognitive functions of the human brain were beyond the reach of computational universality. As Lovelace wrote, ‘The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform. It can follow analysis; but it has no power of anticipating any analytical relations or truths.’\\n\\nAnd yet ‘originating things’, ‘following analysis’, and ‘anticipating analytical relations and truths’ are all behaviours of brains and, therefore, of the atoms of which brains are composed. Such behaviours obey the laws of physics. So it follows inexorably from universality that, with the right program, an Analytical Engine would undergo them too, atom by atom and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestrials can comprehend something beyond quantum mechanics, which we can experience love, and an AGI couldn't experience love because it has to do with our hardware, not just memory and speed, but specialized hardware. And I think that falls victim to the same argument. The thing is, this specialized hardware can't be anything except a computer. And if there's hardware that is needed for love, let's say that somebody is born without that hardware, then that hardware, that bit of the brain that does love or that does mathematical insight or whatever, it's just a bit of the brain and it's connected to the rest of the brain in the same way that the other part of the brain is connected to the rest of the brain, namely by neurons passing electrical signals and by chemicals whose concentrations are altered and so on.\\n\\nSo therefore an artificial device that computed which signals were to be sent and which chemicals were to be adjusted could do the same job and it would be indistinguishable and therefore a person augmented with one of those who couldn't feel love could feel love after that augmentation. So those are and I think that a GIS and humans have the same range in the sense I've defined.\\n\\n2 0:06:14 Okay, interesting. Okay, so I think the software question is more interesting than the harder one immediately but I do want to take issue with the idea that the memory and speed of human brains can be arbitrarily and easily expanded, but we can get into that later. We can just start with this question. Can all humans explain everything that even the smartest humans can explain right so if I took the village idiot, and I asked him to create the theory of quantum computing. a reference about 21 to 24% of Americans on the National Belt Literacy Survey, they fall in level one, which means that they can't even perform basic tasks like identifying the expiry date of a driver's license, for example, or totaling a bank deposit slip. So are these humans capable of explaining quantum computing or creating the Deutsch-Jozsa algorithm? And if they're not capable of doing this, doesn't that mean that the theory of universal explainers falls apart?\\n\\n1 0:07:15 Well, there are people who... So, these tasks that you're talking about are tasks that no ape could do. However, there are humans who are brain damaged to the extent that they can't even do the tasks that an ape can do. And there comes a point when installing the program that would And then installing the program that would, you know, be able to read the driver's license or whatever would require augmenting their hardware as well as their software. So if, if a person that we don't know that might we don't know enough about the brain. of the population then it's definitely not hardware. So I would say that for those people it's definitely software. If it was hardware then getting them to do this would be a matter of repairing the imperfect hardware. If it's software it is not just a matter of whether the existing software is, what word can I use instead of wants to, but he will never be able to speak Mandarin Chinese, and there's nothing about his software either, except that, well, what word can we use to say that he doesn't want to go through that process? I mean, he does want to learn it, he does want to learn it, but he doesn't want to go through the process of being programmed with that program. But if his circumstances changed, he might well want to.\\n\\nSo, for example, many of my relatives a couple of generations ago were forced to migrate to very alien places where they had to learn languages that they never thought they would ever speak and never wanted to speak. And yet very quickly, they did speak those languages. Again, was it because what they wanted changed? In the big picture, perhaps you could say what they wanted changed. So if your driving license blind people wanted to be educated to read driving licenses, in the sense that my ancestors wanted to learn languages, then yes they could learn that there is a level of dysfunction, below which they couldn't, and I think those are hardware limitations on the borderline I like the question of, could apes be programmed with a fully human intellect. I think the answer to that is yes, but it would require intricate changes at the neuron level and that's so that\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\"ulie works by things like empiricism, induction, Bayesianism, Bayesian epistemology. And so AI, so first I guess my first question is AI, current AI, following that?\\n\\nDavid Deutsch Current AI was inspired by that but that's not what it's doing. It's not doing induction any more than anything else is. Induction is impossible. Also, current AI was also inspired by the architecture of the brain, neural nets. There are these programming hardware, computer hardware devices that are modelled on how neurons work. Now I think that's a coincidence. I mean it's possible that the neuron architecture makes things like pattern recognition and extrapolation and so on a bit more efficient. But because of computational universality we know that that can't be fundamental and in fact, you know, you can download a neural net based computer program onto your home computer, which doesn't have a neural net in it, and it'll still work, even though it's a bit slower.\\n\\nLulie If AGI cannot come from AI, what would create AGI in your view?\\n\\nDavid Deutsch I can only say very little about that. From Popper's epistemology we can infer a few things about what it must look like, but far from enough to make one. So one thing is that there cannot be a specification of a program for AGI in the sense of saying what properties its output must have either for a given input, because, for example, an AGI may choose not to answer. It may choose never to answer, it might choose to become a hermit. Are you saying that current views of AGI are all about the output? Yes, they're all about either the output itself or more often how the output must be related to the input. So if we can't judge an AGI based on the output, how can we judge it? Yes, we can't judge an AGI or a human. There can't be a reliable test of whether a human is thinking. What about the Turing test is something that has been invented after Turing. It's been based on a misconception about passage in Turing's 1950 paper called, the paper was called Can Machines Think? and unlike most titles which are questions, the answer was yes rather than no. He included a section on a thing called the imitation game, he called it the imitation game, where an AGI, he just assumes it is an AGI, is pretending to be a human, and he is saying, suppose it could pretend to be a human sufficiently well for the sceptics who think that AGI isn't possible, not to be able to tell the difference between it and an actual human. What would happen? What would these skeptics say about that?\\n\\nWell, if they said, well, it's still not thinking, it just seems to be, then they're vulnerable to the criticism, but that is all the information you have about whether a human is thinking.\\n\\nLulie Wait, so what, so, sorry, why is the test not working or, I didn't, I don't, I didn't\\n\\nDavid Deutsch quite follow. It's not saying that something that can pass this test is necessarily an AGI, or that something that can't pass the test is, necessarily isn't an AGI. That's not what this game is for. What's it for? It's for persuading people that machines could think. How does it do that? By imagining a computer program that could fool people into thinking it was a person. There must be such a program because of computational universality. Ah, it was an argument about universality. Well it just assumed universality. Turing had proved the existence of universality, conjectured, but basically proved the existence of universality 14 years earlier, in 1936, and he was just thinking of that among many consequences of computation, which was a fairly new concept at the time. So I still don't get what the thought experiment intended to persuade the reader, in case the reader was sceptical that a machine can think.\\n\\nLulie So it did that by imagining a situation in which there is a computer and the computer and so a human can have a conversation with another human and that's fine and that would be fairly persuasive that I'm talking to a person and because computers can produce any output you can imagine a computer that produces exactly the same output as that hypothetical human.\\n\\nDavid Deutsch So his imitation game started out with that as a premise that such a computer can exist, that such a computer program can exist because of universality. And then he imagined playing this game in which you have the computer and a human both talking at long distance with the sceptic, the human sceptic who thinks that machines can't think. So the sceptic would then be unable to tell the difference\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans’ algorithm exactly. Yet that would have achieved nothing except an increase in the error rate, due to increased numbers of glitches in the more complex machinery. Similarly, the humans, given different instructions but no hardware changes, would have been capable of emulating every detail of the Difference Engine’s method — and doing so would have been just as perverse. It would not have copied the Engine’s main advantage, its accuracy, which was due to hardware not software. It would only have made an arduous, boring task even more arduous and boring, which would have made errors more likely, not less.\\n\\nBabbage knew that it could be programmed to do algebra, play chess, compose music, process images and so on\\n\\nFor humans, that difference in outcomes — the different error rate — would have been caused by the fact that computing exactly the same table with two different algorithms felt different. But it would not have felt different to the Difference Engine. It had no feelings. Experiencing boredom was one of many cognitive tasks at which the Difference Engine would have been hopelessly inferior to humans. Nor was it capable of knowing or proving, as Babbage did, that the two algorithms would give identical results if executed accurately. Still less was it capable of wanting, as he did, to benefit seafarers and humankind in general. In fact, its repertoire was confined to evaluating a tiny class of specialised mathematical functions (basically, power series in a single variable).\\n\\nThinking about how he could enlarge that repertoire, Babbage first realised that the programming phase of the Engine’s operation could itself be automated: the initial settings of the cogs could be encoded on punched cards. And then he had an epoch-making insight. The Engine could be adapted to punch new cards and store them for its own later use, making what we today call a computer memory. If it could run for long enough — powered, as he envisaged, by a steam engine — and had an unlimited supply of blank cards, its repertoire would jump from that tiny class of mathematical functions to the set of all computations that can possibly be performed by any physical object. That’s universality.\\n\\nBabbage called this improved machine the Analytical Engine. He and Lovelace understood that its universality would give it revolutionary potential to improve almost every scientific endeavour and manufacturing process, as well as everyday life. They showed remarkable foresight about specific applications. They knew that it could be programmed to do algebra, play chess, compose music, process images and so on. Unlike the Difference Engine, it could be programmed to use exactly the same method as humans used to make those tables. And prove that the two methods must give the same answers, and do the same error-checking and proofreading (using, say, optical character recognition) as well.\\n\\nBut could the Analytical Engine feel the same boredom? Could it feel anything? Could it want to better the lot of humankind (or of Analytical Enginekind)? Could it disagree with its programmer about its programming? Here is where Babbage and Lovelace’s insight failed them. They thought that some cognitive functions of the human brain were beyond the reach of computational universality. As Lovelace wrote, ‘The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform. It can follow analysis; but it has no power of anticipating any analytical relations or truths.’\\n\\nAnd yet ‘originating things’, ‘following analysis’, and ‘anticipating analytical relations and truths’ are all behaviours of brains and, therefore, of the atoms of which brains are composed. Such behaviours obey the laws of physics. So it follows inexorably from universality that, with the right program, an Analytical Engine would undergo them too, atom by atom and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestrials can comprehend something beyond quantum mechanics, which we can experience love, and an AGI couldn't experience love because it has to do with our hardware, not just memory and speed, but specialized hardware. And I think that falls victim to the same argument. The thing is, this specialized hardware can't be anything except a computer. And if there's hardware that is needed for love, let's say that somebody is born without that hardware, then that hardware, that bit of the brain that does love or that does mathematical insight or whatever, it's just a bit of the brain and it's connected to the rest of the brain in the same way that the other part of the brain is connected to the rest of the brain, namely by neurons passing electrical signals and by chemicals whose concentrations are altered and so on.\\n\\nSo therefore an artificial device that computed which signals were to be sent and which chemicals were to be adjusted could do the same job and it would be indistinguishable and therefore a person augmented with one of those who couldn't feel love could feel love after that augmentation. So those are and I think that a GIS and humans have the same range in the sense I've defined.\\n\\n2 0:06:14 Okay, interesting. Okay, so I think the software question is more interesting than the harder one immediately but I do want to take issue with the idea that the memory and speed of human brains can be arbitrarily and easily expanded, but we can get into that later. We can just start with this question. Can all humans explain everything that even the smartest humans can explain right so if I took the village idiot, and I asked him to create the theory of quantum computing. a reference about 21 to 24% of Americans on the National Belt Literacy Survey, they fall in level one, which means that they can't even perform basic tasks like identifying the expiry date of a driver's license, for example, or totaling a bank deposit slip. So are these humans capable of explaining quantum computing or creating the Deutsch-Jozsa algorithm? And if they're not capable of doing this, doesn't that mean that the theory of universal explainers falls apart?\\n\\n1 0:07:15 Well, there are people who... So, these tasks that you're talking about are tasks that no ape could do. However, there are humans who are brain damaged to the extent that they can't even do the tasks that an ape can do. And there comes a point when installing the program that would And then installing the program that would, you know, be able to read the driver's license or whatever would require augmenting their hardware as well as their software. So if, if a person that we don't know that might we don't know enough about the brain. of the population then it's definitely not hardware. So I would say that for those people it's definitely software. If it was hardware then getting them to do this would be a matter of repairing the imperfect hardware. If it's software it is not just a matter of whether the existing software is, what word can I use instead of wants to, but he will never be able to speak Mandarin Chinese, and there's nothing about his software either, except that, well, what word can we use to say that he doesn't want to go through that process? I mean, he does want to learn it, he does want to learn it, but he doesn't want to go through the process of being programmed with that program. But if his circumstances changed, he might well want to.\\n\\nSo, for example, many of my relatives a couple of generations ago were forced to migrate to very alien places where they had to learn languages that they never thought they would ever speak and never wanted to speak. And yet very quickly, they did speak those languages. Again, was it because what they wanted changed? In the big picture, perhaps you could say what they wanted changed. So if your driving license blind people wanted to be educated to read driving licenses, in the sense that my ancestors wanted to learn languages, then yes they could learn that there is a level of dysfunction, below which they couldn't, and I think those are hardware limitations on the borderline I like the question of, could apes be programmed with a fully human intellect. I think the answer to that is yes, but it would require intricate changes at the neuron level and that's so that\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" is correct and another thing isn't correct it will talk nonsense and yet it knows which of them is correct. I've seen several cases of that and I was very impressed because it means that it's knowledge of language. It's a thing that I don't actually understand how it can be as good at speaking English, writing English, as it is. It's much better than most people, but more interestingly, it's much better than the average or the typical text on the internet. So it makes mistakes, but it makes far fewer mistakes than a typical text on the internet. Do you think this is going to revolutionise the economy and I can't prophesy the applications of modern AIs, chatbots and so on. I have found, for what it's worth, as it were, I have found it useful but not revolutionary in my own work and in my writing and whatever it is useful but I can't see possibility for it to revolutionize what I do. Whether it can revolutionize the economy depends on something slightly different because for that it doesn't need to really have a fundamental new functionality. It could be that a lot of existing jobs, and people are scared that computer programming is one of them, where only a proportion of the job, let's say 10%, of a particular programming job involves human creativity, and the rest is basically hack work.\\n\\nwhich is a big if, because I'm not convinced of this either. If chatbots can reliably perform the hack work, then it might be argued, I think again wrongly, that if a given task can be done with only a tenth as much work, then we might need only a tenth as many programmers in the long run.\\n\\nLulie And unfortunately we got interrupted, so David never finished his thought about why programmer jobs might be safer. But if you have any questions about anything in this episode, leave them jobs might be safer. But if you have any questions about anything in this episode, leave them on the tweet, which I will link in the show notes about this episode. Thank you.2 0:00:00 Okay, today I'm speaking with David Deutsch. Now this is a conversation that I've been eagerly wanting to have for years so this is very exciting for me. So, first let's talk about AI. Can you briefly explain why you anticipate that AI's will be no more fundamentally intelligent than humans.\\n\\n1 0:00:19 I suppose you mean capable of all the same types of cognition as humans are in principle.\\n\\n4\\n\\n0:00:37\\n\\nYes.\\n\\n1 0:00:38 So that would include, you know, doing science and doing art and in principle also falling in love and being good and being evil and all that. So the reason is twofold and one half is about computation hardware, computation hardware, and the other is about hardware. We know that our, our brains are Turing complete bits of hardware, and therefore can exhibit the functionality of running any computable function program for any computable function. Now, when I say any, I don't really mean any, because you and I sitting here, you know, we're having conversation and we could say, you know, we could have any conversation. Well, we can assume that maybe in a hundred years time, we'll both be dead. And therefore, the number of conversations we could have is strictly limited. And also, some conversations depend on speed of computation. So, you know, if we're going to be solving the traveling salesman problem, then there are there are many traveling salesman problems that we wouldn't be able to solve in the programs we can run, apart from by speed and memory capacity. So all limitations on us, hardware limitations on us, boil down to speed and memory capacity. And both those can be augmented to the level of any other entity that is in the universe. Because, you know, if somebody builds a computer that can think faster than the brain, then we can use that very computer or that very technology to make our thinking go just as fast as that. So that's the hardware.\\n\\nAs far as explanations go, can we reach the same kind of explanations as any other entity, let's say, usually this is said not in terms of AGIs but in terms of extraterrestrial intelligences, but also it's said about AGIs, you know, what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestri\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" of an algorithm that was programmed in by somebody who knew what the transformation between the inputs and outputs ought to be. Namely, when you press a certain button it multiplies and so on. AI's, modern AI's, essentially construct their algorithm themselves by generalizing a large amount of data. In early ones, this led to kind of embarrassing glitches, like when they identified a heap of rifles as a cat, because somebody worked out what they were doing and how to fool them. But the modern ones use so much data and have been honed by humans so well that they rarely do this. Although I've recently been playing around with the chat GPTs and I find that if you ask it some questions off the beaten track, you can quite easily cause it to do all the old things of either either saying nonsense, contradicting itself, committing howlers, where it says the opposite of known facts and so on. And that's different from human knowledge which is explanatory.\\n\\nNeither the calculator nor the chatbot ever produces a new explanation. You can ask it for an explanation but all it's doing is distilling explanations that already exist and that's the thing that it doesn't do very well either.\\n\\nLulie What is the difference between an explanation and the thing that it does produce?\\n\\nDavid Deutsch If we knew the detailed answer to that we'd know how to make an AGI. basically an explanation accounts for what it's trying to account for, like a physical process or the reason for something, it accounts for that, it accounts for a known thing that is trying to explain in terms of the unseen, unknown reasons behind it, which usually cannot even be seen even in So Brett Hall's favourite example is that we can never see the centre of the sun. We could never go there, any instrument that we send would get destroyed long before it got to the centre of the sun. So the centre of the sun can't be in GPT's training data? As it were, yes, exactly. And the only thing that can be in GPT's training data is what is seen about the Sun, namely its surface.\\n\\nLulie Couldn't GPT derive things about the Sun based on other theories that we have?\\n\\nDavid Deutsch It can deduce things from existing theories, yes. So if you ask it about the centre of the Sun, it will find some existing theory of the Sun and make a deduction from that. So that's not induction, that's deduction. And on the other hand, if there was a mystery about the Sun, like a couple of decades ago...\\n\\nLulie Isn't it induction via deduction? So the induction was all of the training data,\\n\\nDavid Deutsch and then the deduction is taking that data and then forming theories about it? No, because it didn't induce the data. It distilled it into a more compact form and then it can deduce things from that, but those things are only ever as good as the original theories were, probably slightly worse because by compressing the data it slightly some cases it degraded it a lot. A few decades ago, I was going to say recently, but in fact not so recent, it was when I was a graduate student, the big problem in astrophysics was that the sun wasn't producing enough neutrinos. Enough for what?\\n\\n3\\n\\nEnough.\\n\\nDavid Deutsch For your breakfast cereal? producing as many as the theory predicted. And this theory was extremely robust because it was also the theory that we used to predict the sun's brightness. And it predicted the sun's brightness extremely well, and also the brightness of other stars and how they change with time and So when they first made neutrino observatories with rather crude neutrino detectors, first they didn't find neutrinos but then they found a few but nowhere near enough. And when they refined it they found that there were only a third as many neutrinos as as predicted by the theory. And so there were all sorts of explanatory theories proposed, which couldn't possibly have been induced from anything, because all the data said was there are too few neutrinos. And of course you can always say that neutrinos have been eaten by a space monster, but generally when we produce scientific theories, we don't just want a new explanation, we want a new explanation that doesn't upset old explanations, that doesn't make them into nonsense, like the space monster theory. So you asked me what the difference is between an explanation and just a predictive theory. And by the way, there is no such thing as a purely predictive theory. They all have some kind of explanation and when you're quote inducing things from data you're always using an old explanation and just piling on some tweaks which don't have an explanation in order to make your supposed new theory.\\n\\n\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" our knowledge of, of grammar is my was my favorite example, as we know why certain sentences are acceptable and why they're unacceptable, but we can't state explicitly, or in every case, why it isn't or why it is. And then there's explicit and inexplicit knowledge, there's conscious and unconscious knowledge. All those are bits of program in the brain, they're ideas, they are bits of knowledge in this, if you define knowledge as information with causal power, they are all information with causal power. They all contain truth and they all contain error and it's always a mistake to shield something, to shield one of them from criticism or replacement.\\n\\nNot doing that is what I call the fun criterion. Now you might say that's a bad name, but you know, it's the best I can find.\\n\\n2 1:14:11 So why would creating an AGI through evolution necessarily entail suffering? Because the way I see it, or it seems to me your theory is that you need to be a general intelligence in order to feel suffering, but by the point an evolved, simulated being is a general intelligence, we can just stop the simulation.\\n\\n1 1:14:31 And so where is the suffering coming from? Okay, so the kind of simulation by evolution that I'm thinking of, I mean, there may be several kinds, but the kind that I'm thinking of, and which I said would be the greatest crime in history, is the kind that just simulates the actual evolution of humans from pre humans that weren't people. So you have you have a population of non people which in this simulation would be some kind of NPCs. And then they would they would just evolve we don't know what the criterion would be we just have an artificial universe which simulated the surface of the earth and they'd be walking around and some of them might or might not become people. And now the thing is when you're part of the way there, what is happening is that you have the way that I, the only way that I can imagine the evolution of personhood or create the explanatory creativity happened was that the hardware needed for it was first needed for something else. I have proposed that it was needed to transmit memes. So there'd be people who were transmitting memes creatively but they were running out of resources before it managed to increase their stock of memes. So in every generation, there was a stock of memes that was being passed down to the next generation. And once they got beyond a certain complexity, they had to be passed down by the use of creativity by the recipient.\\n\\nSo, there may well have been a time and as I say I can't think of any other way it could have been, where there was genuine creativity being used, but it ran out of resources, very quickly, but not so quickly that it didn't increase the mean bandwidth. Then in the next generation, there was more mean bandwidth, and then after, you know, certain number of generations, there would have been some opportunity to use this hardware or whatever it is, you know, firmware, I expect, to use this firmware for something other than just trying blindly transmitting memes or rather creatively transmitting memes, but they were blind memes. So in that time, it would have been very unpleasant to be alive. It was already very unpleasant to, sorry, it was very unpleasant to be alive when we did have enough resources to think as well as do the memes. But I don't think there would have been a moment at which you would say, yes, now the suffering begins to matter because it's not just blind memes. I think the people were already suffering at the time when they were blindly transmitting memes because they were using genuine creativity. They were just not using it to any good effect.\\n\\n2 1:17:56 Gotcha. Would being in the experience machine be compatible with the fun criterion so you're not aware that you're in, you know, the experience machine it's all virtual reality, but you're still doing the things that would make you have fun in fact more so than in the real world.\\n\\n1 1:18:15 So, would you be tempted to get into experience machine, what do we can put a compatible with the fun criteria, I mean, is it just a virtual reality world in which things work better than in the real world or something?\\n\\n2 1:18:35 Yeah, so it's a thought experiment by Robert Nozick, and the idea is that you would enter this world, but you would forget that you're in virtual reality. So all, I mean, the world would be perfect in every possible way that it could be perfect or not perfect, but it would be better in every possible way it could be better. But you would think the relationships you have here are real, the knowledge you're discovering here is novel\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.\\n\\nPerhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.\\n\\nAGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.\\n\\nIronically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’\\n\\n‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.\\n\\nSome, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science.)\\n\\nThat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non-cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.\\n\\nPerhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.\\n\\nAGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.\\n\\nIronically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’\\n\\n‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.\\n\\nSome, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science.)\\n\\nThat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non-cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.\\n\\nPerhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.\\n\\nAGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.\\n\\nIronically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’\\n\\n‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.\\n\\nSome, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science.)\\n\\nThat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non-cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or practical aim. This isn’t good. Never mind the terminology; change it if you like, and there are indeed reasons for treating various entities with respect, protecting them from harm and so on. All the same, the distinction between actual people, defined by that objective criterion, and other entities has enormous moral and practical significance, and is going to become vital to the functioning of a civilisation that includes AGIs.\\n\\nThe battle between good and evil ideas is as old as our species and will go on regardless of the hardware on which it is running\\n\\nFor example, the mere fact that it is not the computer but the running program that is a person, raises unsolved philosophical problems that will become practical, political controversies as soon as AGIs exist. Once an AGI program is running in a computer, to deprive it of that computer would be murder (or at least false imprisonment or slavery, as the case may be), just like depriving a human mind of its body. But unlike a human body, an AGI program can be copied into multiple computers at the touch of a button. Are those programs, while they are still executing identical steps (ie before they have become differentiated due to random choices or different experiences), the same person or many different people? Do they get one vote, or many? Is deleting one of them murder, or a minor assault? And if some rogue programmer, perhaps illegally, creates billions of different AGI people, either on one computer or on many, what happens next? They are still people, with rights. Do they all get the vote?\\n\\nFurthermore, in regard to AGIs, like any other entities with creativity, we have to forget almost all existing connotations of the word ‘programming’. To treat AGIs like any other computer programs would constitute brainwashing, slavery, and tyranny. And cruelty to children, too, for ‘programming’ an already-running AGI, unlike all other programming, constitutes education. And it constitutes debate, moral as well as factual. To ignore the rights and personhood of AGIs would not only be the epitome of evil, but also a recipe for disaster: creative beings cannot be enslaved forever.\\n\\nSome people are wondering whether we should welcome our new robot overlords. Some hope to learn how we can rig their programming to make them constitutionally unable to harm humans (as in Isaac Asimov’s ‘laws of robotics’), or to prevent them from acquiring the theory that the universe should be converted into paper clips (as imagined by Nick Bostrom). None of these are the real problem. It has always been the case that a single exceptionally creative person can be thousands of times as productive — economically, intellectually or whatever — as most people; and that such a person could do enormous harm were he to turn his powers to evil instead of good.\\n\\nThese phenomena have nothing to do with AGIs. The battle between good and evil ideas is as old as our species and will continue regardless of the hardware on which it is running. The issue is: we want the intelligences with (morally) good ideas always to defeat the evil intelligences, biological and artificial; but we are fallible, and our own conception of ‘good’ needs continual improvement. How should society be organised so as to promote that improvement? ‘Enslave all intelligence’ would be a catastrophically wrong answer, and ‘enslave all intelligence that doesn’t look like us’ would not be much better.\\n\\nOne implication is that we must stop regarding education (of humans or AGIs alike) as instruction — as a means of transmitting existing knowledge unaltered, and causing existing values to be enacted obediently. As Popper wrote (in the context of scientific discovery, but it applies equally to the programming of AGIs and the education of children): ‘there is no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AG', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.\\n\\nPerhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.\\n\\nAGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.\\n\\nIronically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’\\n\\n‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.\\n\\nSome, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science.)\\n\\nThat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non-cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or practical aim. This isn’t good. Never mind the terminology; change it if you like, and there are indeed reasons for treating various entities with respect, protecting them from harm and so on. All the same, the distinction between actual people, defined by that objective criterion, and other entities has enormous moral and practical significance, and is going to become vital to the functioning of a civilisation that includes AGIs.\\n\\nThe battle between good and evil ideas is as old as our species and will go on regardless of the hardware on which it is running\\n\\nFor example, the mere fact that it is not the computer but the running program that is a person, raises unsolved philosophical problems that will become practical, political controversies as soon as AGIs exist. Once an AGI program is running in a computer, to deprive it of that computer would be murder (or at least false imprisonment or slavery, as the case may be), just like depriving a human mind of its body. But unlike a human body, an AGI program can be copied into multiple computers at the touch of a button. Are those programs, while they are still executing identical steps (ie before they have become differentiated due to random choices or different experiences), the same person or many different people? Do they get one vote, or many? Is deleting one of them murder, or a minor assault? And if some rogue programmer, perhaps illegally, creates billions of different AGI people, either on one computer or on many, what happens next? They are still people, with rights. Do they all get the vote?\\n\\nFurthermore, in regard to AGIs, like any other entities with creativity, we have to forget almost all existing connotations of the word ‘programming’. To treat AGIs like any other computer programs would constitute brainwashing, slavery, and tyranny. And cruelty to children, too, for ‘programming’ an already-running AGI, unlike all other programming, constitutes education. And it constitutes debate, moral as well as factual. To ignore the rights and personhood of AGIs would not only be the epitome of evil, but also a recipe for disaster: creative beings cannot be enslaved forever.\\n\\nSome people are wondering whether we should welcome our new robot overlords. Some hope to learn how we can rig their programming to make them constitutionally unable to harm humans (as in Isaac Asimov’s ‘laws of robotics’), or to prevent them from acquiring the theory that the universe should be converted into paper clips (as imagined by Nick Bostrom). None of these are the real problem. It has always been the case that a single exceptionally creative person can be thousands of times as productive — economically, intellectually or whatever — as most people; and that such a person could do enormous harm were he to turn his powers to evil instead of good.\\n\\nThese phenomena have nothing to do with AGIs. The battle between good and evil ideas is as old as our species and will continue regardless of the hardware on which it is running. The issue is: we want the intelligences with (morally) good ideas always to defeat the evil intelligences, biological and artificial; but we are fallible, and our own conception of ‘good’ needs continual improvement. How should society be organised so as to promote that improvement? ‘Enslave all intelligence’ would be a catastrophically wrong answer, and ‘enslave all intelligence that doesn’t look like us’ would not be much better.\\n\\nOne implication is that we must stop regarding education (of humans or AGIs alike) as instruction — as a means of transmitting existing knowledge unaltered, and causing existing values to be enacted obediently. As Popper wrote (in the context of scientific discovery, but it applies equally to the programming of AGIs and the education of children): ‘there is no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AG', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.\\n\\nPerhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.\\n\\nAGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.\\n\\nIronically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’\\n\\n‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.\\n\\nSome, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science.)\\n\\nThat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non-cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or practical aim. This isn’t good. Never mind the terminology; change it if you like, and there are indeed reasons for treating various entities with respect, protecting them from harm and so on. All the same, the distinction between actual people, defined by that objective criterion, and other entities has enormous moral and practical significance, and is going to become vital to the functioning of a civilisation that includes AGIs.\\n\\nThe battle between good and evil ideas is as old as our species and will go on regardless of the hardware on which it is running\\n\\nFor example, the mere fact that it is not the computer but the running program that is a person, raises unsolved philosophical problems that will become practical, political controversies as soon as AGIs exist. Once an AGI program is running in a computer, to deprive it of that computer would be murder (or at least false imprisonment or slavery, as the case may be), just like depriving a human mind of its body. But unlike a human body, an AGI program can be copied into multiple computers at the touch of a button. Are those programs, while they are still executing identical steps (ie before they have become differentiated due to random choices or different experiences), the same person or many different people? Do they get one vote, or many? Is deleting one of them murder, or a minor assault? And if some rogue programmer, perhaps illegally, creates billions of different AGI people, either on one computer or on many, what happens next? They are still people, with rights. Do they all get the vote?\\n\\nFurthermore, in regard to AGIs, like any other entities with creativity, we have to forget almost all existing connotations of the word ‘programming’. To treat AGIs like any other computer programs would constitute brainwashing, slavery, and tyranny. And cruelty to children, too, for ‘programming’ an already-running AGI, unlike all other programming, constitutes education. And it constitutes debate, moral as well as factual. To ignore the rights and personhood of AGIs would not only be the epitome of evil, but also a recipe for disaster: creative beings cannot be enslaved forever.\\n\\nSome people are wondering whether we should welcome our new robot overlords. Some hope to learn how we can rig their programming to make them constitutionally unable to harm humans (as in Isaac Asimov’s ‘laws of robotics’), or to prevent them from acquiring the theory that the universe should be converted into paper clips (as imagined by Nick Bostrom). None of these are the real problem. It has always been the case that a single exceptionally creative person can be thousands of times as productive — economically, intellectually or whatever — as most people; and that such a person could do enormous harm were he to turn his powers to evil instead of good.\\n\\nThese phenomena have nothing to do with AGIs. The battle between good and evil ideas is as old as our species and will continue regardless of the hardware on which it is running. The issue is: we want the intelligences with (morally) good ideas always to defeat the evil intelligences, biological and artificial; but we are fallible, and our own conception of ‘good’ needs continual improvement. How should society be organised so as to promote that improvement? ‘Enslave all intelligence’ would be a catastrophically wrong answer, and ‘enslave all intelligence that doesn’t look like us’ would not be much better.\\n\\nOne implication is that we must stop regarding education (of humans or AGIs alike) as instruction — as a means of transmitting existing knowledge unaltered, and causing existing values to be enacted obediently. As Popper wrote (in the context of scientific discovery, but it applies equally to the programming of AGIs and the education of children): ‘there is no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AG', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.\\n\\nPerhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.\\n\\nAGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.\\n\\nIronically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’\\n\\n‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.\\n\\nSome, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science.)\\n\\nThat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non-cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or practical aim. This isn’t good. Never mind the terminology; change it if you like, and there are indeed reasons for treating various entities with respect, protecting them from harm and so on. All the same, the distinction between actual people, defined by that objective criterion, and other entities has enormous moral and practical significance, and is going to become vital to the functioning of a civilisation that includes AGIs.\\n\\nThe battle between good and evil ideas is as old as our species and will go on regardless of the hardware on which it is running\\n\\nFor example, the mere fact that it is not the computer but the running program that is a person, raises unsolved philosophical problems that will become practical, political controversies as soon as AGIs exist. Once an AGI program is running in a computer, to deprive it of that computer would be murder (or at least false imprisonment or slavery, as the case may be), just like depriving a human mind of its body. But unlike a human body, an AGI program can be copied into multiple computers at the touch of a button. Are those programs, while they are still executing identical steps (ie before they have become differentiated due to random choices or different experiences), the same person or many different people? Do they get one vote, or many? Is deleting one of them murder, or a minor assault? And if some rogue programmer, perhaps illegally, creates billions of different AGI people, either on one computer or on many, what happens next? They are still people, with rights. Do they all get the vote?\\n\\nFurthermore, in regard to AGIs, like any other entities with creativity, we have to forget almost all existing connotations of the word ‘programming’. To treat AGIs like any other computer programs would constitute brainwashing, slavery, and tyranny. And cruelty to children, too, for ‘programming’ an already-running AGI, unlike all other programming, constitutes education. And it constitutes debate, moral as well as factual. To ignore the rights and personhood of AGIs would not only be the epitome of evil, but also a recipe for disaster: creative beings cannot be enslaved forever.\\n\\nSome people are wondering whether we should welcome our new robot overlords. Some hope to learn how we can rig their programming to make them constitutionally unable to harm humans (as in Isaac Asimov’s ‘laws of robotics’), or to prevent them from acquiring the theory that the universe should be converted into paper clips (as imagined by Nick Bostrom). None of these are the real problem. It has always been the case that a single exceptionally creative person can be thousands of times as productive — economically, intellectually or whatever — as most people; and that such a person could do enormous harm were he to turn his powers to evil instead of good.\\n\\nThese phenomena have nothing to do with AGIs. The battle between good and evil ideas is as old as our species and will continue regardless of the hardware on which it is running. The issue is: we want the intelligences with (morally) good ideas always to defeat the evil intelligences, biological and artificial; but we are fallible, and our own conception of ‘good’ needs continual improvement. How should society be organised so as to promote that improvement? ‘Enslave all intelligence’ would be a catastrophically wrong answer, and ‘enslave all intelligence that doesn’t look like us’ would not be much better.\\n\\nOne implication is that we must stop regarding education (of humans or AGIs alike) as instruction — as a means of transmitting existing knowledge unaltered, and causing existing values to be enacted obediently. As Popper wrote (in the context of scientific discovery, but it applies equally to the programming of AGIs and the education of children): ‘there is no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AG', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.\\n\\nPerhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.\\n\\nAGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.\\n\\nIronically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’\\n\\n‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.\\n\\nSome, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science.)\\n\\nThat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non-cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or practical aim. This isn’t good. Never mind the terminology; change it if you like, and there are indeed reasons for treating various entities with respect, protecting them from harm and so on. All the same, the distinction between actual people, defined by that objective criterion, and other entities has enormous moral and practical significance, and is going to become vital to the functioning of a civilisation that includes AGIs.\\n\\nThe battle between good and evil ideas is as old as our species and will go on regardless of the hardware on which it is running\\n\\nFor example, the mere fact that it is not the computer but the running program that is a person, raises unsolved philosophical problems that will become practical, political controversies as soon as AGIs exist. Once an AGI program is running in a computer, to deprive it of that computer would be murder (or at least false imprisonment or slavery, as the case may be), just like depriving a human mind of its body. But unlike a human body, an AGI program can be copied into multiple computers at the touch of a button. Are those programs, while they are still executing identical steps (ie before they have become differentiated due to random choices or different experiences), the same person or many different people? Do they get one vote, or many? Is deleting one of them murder, or a minor assault? And if some rogue programmer, perhaps illegally, creates billions of different AGI people, either on one computer or on many, what happens next? They are still people, with rights. Do they all get the vote?\\n\\nFurthermore, in regard to AGIs, like any other entities with creativity, we have to forget almost all existing connotations of the word ‘programming’. To treat AGIs like any other computer programs would constitute brainwashing, slavery, and tyranny. And cruelty to children, too, for ‘programming’ an already-running AGI, unlike all other programming, constitutes education. And it constitutes debate, moral as well as factual. To ignore the rights and personhood of AGIs would not only be the epitome of evil, but also a recipe for disaster: creative beings cannot be enslaved forever.\\n\\nSome people are wondering whether we should welcome our new robot overlords. Some hope to learn how we can rig their programming to make them constitutionally unable to harm humans (as in Isaac Asimov’s ‘laws of robotics’), or to prevent them from acquiring the theory that the universe should be converted into paper clips (as imagined by Nick Bostrom). None of these are the real problem. It has always been the case that a single exceptionally creative person can be thousands of times as productive — economically, intellectually or whatever — as most people; and that such a person could do enormous harm were he to turn his powers to evil instead of good.\\n\\nThese phenomena have nothing to do with AGIs. The battle between good and evil ideas is as old as our species and will continue regardless of the hardware on which it is running. The issue is: we want the intelligences with (morally) good ideas always to defeat the evil intelligences, biological and artificial; but we are fallible, and our own conception of ‘good’ needs continual improvement. How should society be organised so as to promote that improvement? ‘Enslave all intelligence’ would be a catastrophically wrong answer, and ‘enslave all intelligence that doesn’t look like us’ would not be much better.\\n\\nOne implication is that we must stop regarding education (of humans or AGIs alike) as instruction — as a means of transmitting existing knowledge unaltered, and causing existing values to be enacted obediently. As Popper wrote (in the context of scientific discovery, but it applies equally to the programming of AGIs and the education of children): ‘there is no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AG', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or practical aim. This isn’t good. Never mind the terminology; change it if you like, and there are indeed reasons for treating various entities with respect, protecting them from harm and so on. All the same, the distinction between actual people, defined by that objective criterion, and other entities has enormous moral and practical significance, and is going to become vital to the functioning of a civilisation that includes AGIs.\\n\\nThe battle between good and evil ideas is as old as our species and will go on regardless of the hardware on which it is running\\n\\nFor example, the mere fact that it is not the computer but the running program that is a person, raises unsolved philosophical problems that will become practical, political controversies as soon as AGIs exist. Once an AGI program is running in a computer, to deprive it of that computer would be murder (or at least false imprisonment or slavery, as the case may be), just like depriving a human mind of its body. But unlike a human body, an AGI program can be copied into multiple computers at the touch of a button. Are those programs, while they are still executing identical steps (ie before they have become differentiated due to random choices or different experiences), the same person or many different people? Do they get one vote, or many? Is deleting one of them murder, or a minor assault? And if some rogue programmer, perhaps illegally, creates billions of different AGI people, either on one computer or on many, what happens next? They are still people, with rights. Do they all get the vote?\\n\\nFurthermore, in regard to AGIs, like any other entities with creativity, we have to forget almost all existing connotations of the word ‘programming’. To treat AGIs like any other computer programs would constitute brainwashing, slavery, and tyranny. And cruelty to children, too, for ‘programming’ an already-running AGI, unlike all other programming, constitutes education. And it constitutes debate, moral as well as factual. To ignore the rights and personhood of AGIs would not only be the epitome of evil, but also a recipe for disaster: creative beings cannot be enslaved forever.\\n\\nSome people are wondering whether we should welcome our new robot overlords. Some hope to learn how we can rig their programming to make them constitutionally unable to harm humans (as in Isaac Asimov’s ‘laws of robotics’), or to prevent them from acquiring the theory that the universe should be converted into paper clips (as imagined by Nick Bostrom). None of these are the real problem. It has always been the case that a single exceptionally creative person can be thousands of times as productive — economically, intellectually or whatever — as most people; and that such a person could do enormous harm were he to turn his powers to evil instead of good.\\n\\nThese phenomena have nothing to do with AGIs. The battle between good and evil ideas is as old as our species and will continue regardless of the hardware on which it is running. The issue is: we want the intelligences with (morally) good ideas always to defeat the evil intelligences, biological and artificial; but we are fallible, and our own conception of ‘good’ needs continual improvement. How should society be organised so as to promote that improvement? ‘Enslave all intelligence’ would be a catastrophically wrong answer, and ‘enslave all intelligence that doesn’t look like us’ would not be much better.\\n\\nOne implication is that we must stop regarding education (of humans or AGIs alike) as instruction — as a means of transmitting existing knowledge unaltered, and causing existing values to be enacted obediently. As Popper wrote (in the context of scientific discovery, but it applies equally to the programming of AGIs and the education of children): ‘there is no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AG', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\\nDavid Deutsch Absolutely. I mean, I think probably more than now, it was a complete consensus that everybody was afraid. Let me think. I think everybody was appropriately afraid. Was it like the pandemic? It's unlike today.\\n\\nLulie Was it like the pandemic freak-out? Because you also got people actually freaking out around the pandemic and doing all sorts of things like covering their door handles in copper and not knowing whether masks work or not and so on and these battles with their friends about whether\\n\\nDavid Deutsch they leave their house and whether that is akin to killing people. The interesting thing, I think there's an interesting difference between being afraid of something like nuclear war or a pandemic and being afraid of something that one imagines, like AI risk or climate risk. So those are both things that that might happen in the future and different people might imagine different things about them.\\n\\nLulie You mean AGI risk in the future?\\n\\nDavid Deutsch Well there's AGI risk and nowadays there seems to be AI risk as well. I mean there is AI risk. People are using it to scam people and to fake voices of Barack Obama and ring people's grandmothers and so what about those? Yes, well those are real risks and you know there's the electric car risk and the self-driving car risk and so on and that's that's not in the same league as as having a greatly increased ability to scam and get like dodgy information it's interesting is it greatly increased i mean i i wonder whether anyone has statistics about how many scams are currently advanced ai enabled and how many are simply the same old scams of saying hello we're the police we want you to transfer all your money into this account.\\n\\nLulie I imagine the good scams, as in the effective scams, would be AI enabled, like you want to be\\n\\nDavid Deutsch on the leading edge of making scams. Well, I don't know, I'm not an expert on scams, but the thing is dangers, including scams and everything, will always be caused by new technology. I mean, sorry, new technology will always cause dangers, including scams. And to try to mitigate that by preventing new technology, in case it produces new dangers, is much more dangerous than any of the new technologies themselves.\\n\\nLulie What about just slowing it down such that people can adapt? Because like right now we've got something that is going so quickly that people are getting confused like old people if they see an image then they will assume that it's real and whereas if you have time that people are kind of adapt to, ah yes there are these deep fakes and so on.\\n\\nDavid Deutsch Well I'm not sure that time causes better adaptation because if things are happening fast, then also, news stories about how people have been scammed will be seen by your old people, and whereas if we slowed it down so that only one scam occurs every few months, then it might not be news.\\n\\nLulie issue. People are worried that AGI is, you know, maybe next week or just around the corner or in, like they used to say, in a few years, and now that we have these very good language models they say, maybe, like, small number of years, months, like, possibly weeks, and hence the proposed moratorium. So, what is the thing that makes you so chill? Why couldn't it lead to AGI? What's the problem with the idea of emergence? Because, you know, intelligence emerged once. Yes. So emergence isn't magic.\\n\\nDavid Deutsch It is, of course, possible that in the deep ocean a new form of life is emerging at this very moment and it will break the surface weeks from now.\\n\\nLulie Or on Pluto. Did you hear about Pluto? They discovered that there's ice or something and that they've sent another probe but it's going to take eight years for it to come back and find out whether there's life on Pluto.\\n\\nDavid Deutsch I hadn't heard that but there certainly is a possibility of life in various places in the solar system but not, I think, not intelligent life. But, if you're going to say emergence can do unexpected things, then you might as well say it about the deep ocean, because\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I had Tyler Cowen on my podcast, I asked him why he thinks, so he thinks that human civilization is only going to be around for 700 more years, and then so I asked him, I gave him, you know, your rebuttal, or what I understand of your rebuttal, that, you know, creative, optimistic societies will innovate ways of, you know, safety technology is faster than totalitarian static societies can innovate destructive technologies. And he responded, you know, maybe, but the cost of destruction is just so much lower than the cost of building. And, you know, that trend has been going on for a while now. What happens when a new cost, $60,000? Or what happens if there's a mistake like the kinds that we saw many times over in the Cold War? How would you respond to that?\\n\\n1 1:03:38 First of all, I think we've been getting safer and safer throughout the entire history of civilization. The, you know, there were these plagues that wiped out a third of the population of the world or half. And it could have been 99% or 100%. went through some kind of bottleneck 70,000 years ago, I understand, which they can tell from genetics, all our cousin species have been wiped out. So we were much less safe then than now. Also, if a asteroid, 10-kilometer asteroid, had been on target with the earth at any time in the in the past two million year or whatever it is history of the genus homo that would have been the end of it whereas now it'll just mean higher taxation for a while you know that that's the that's how much amazingly safer we are now uh i i would never say that it's impossible that we'll destroy ourselves. That would be the contrary to the universality of the human mind. We can make wrong choices. We can make so many wrong choices that we'll destroy ourselves. And on the other hand, the atomic bomb accident sort of thing would have had no zero chance of destroying civilization. All they would have done is cause a vast amount of suffering. And... but I don't think we have the technology to end civilization even if we wanted to. I think all we would do if we just deliberately unleashed hell all over the world is we would cause a vast amount of suffering. But there would be survivors and they would resolve never to do that again.\\n\\nSo I don't think we're even able to, let alone that we would do it accidentally. But as for the bad guys, well, I think we are doing the wrong thing largely in regard to both external and internal threats. But I don't think we're doing the wrong thing to an existential risk level. And over the next 700 years or whatever it is, well, I don't want to prophesy because I don't know most of the advances that are going to be made in that time. I see no reason why if we are solving problems, we won't solve problems. I don't think this, this, to take another metaphor, Nick Bostrom's jar with white balls and there's one black ball, and you take out a white ball and white ball and white ball, and then you hit the black ball and that's the end of you. I don't think it's like that because every white ball you take out and have reduces the number of black balls in the jar. So again I'm not saying that's the law of nature. It could be that the very next ball we take out will be the black one that'll be the end of us. It could be but I think all arguments that it will be are fallacious.\\n\\n2 1:07:17 I do want to talk about the fun criterion. Is your definition of fun different from how other people define other positive emotions like eudaemonia or well being or satisfaction? Is it fun a different emotion?\\n\\n1 1:07:29 I don't think it's an emotion. And and I all these things are not very well defined. They can't possibly be very well defined until we have a satisfactory theory of qualia, at least, and probably more satisfactory theory of creativity, how creativity works and so on. I think that the choice of the word fun for the thing that I explain more precisely, but still not very precisely, as a creation of knowledge without,\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\\nDavid Deutsch Absolutely. I mean, I think probably more than now, it was a complete consensus that everybody was afraid. Let me think. I think everybody was appropriately afraid. Was it like the pandemic? It's unlike today.\\n\\nLulie Was it like the pandemic freak-out? Because you also got people actually freaking out around the pandemic and doing all sorts of things like covering their door handles in copper and not knowing whether masks work or not and so on and these battles with their friends about whether\\n\\nDavid Deutsch they leave their house and whether that is akin to killing people. The interesting thing, I think there's an interesting difference between being afraid of something like nuclear war or a pandemic and being afraid of something that one imagines, like AI risk or climate risk. So those are both things that that might happen in the future and different people might imagine different things about them.\\n\\nLulie You mean AGI risk in the future?\\n\\nDavid Deutsch Well there's AGI risk and nowadays there seems to be AI risk as well. I mean there is AI risk. People are using it to scam people and to fake voices of Barack Obama and ring people's grandmothers and so what about those? Yes, well those are real risks and you know there's the electric car risk and the self-driving car risk and so on and that's that's not in the same league as as having a greatly increased ability to scam and get like dodgy information it's interesting is it greatly increased i mean i i wonder whether anyone has statistics about how many scams are currently advanced ai enabled and how many are simply the same old scams of saying hello we're the police we want you to transfer all your money into this account.\\n\\nLulie I imagine the good scams, as in the effective scams, would be AI enabled, like you want to be\\n\\nDavid Deutsch on the leading edge of making scams. Well, I don't know, I'm not an expert on scams, but the thing is dangers, including scams and everything, will always be caused by new technology. I mean, sorry, new technology will always cause dangers, including scams. And to try to mitigate that by preventing new technology, in case it produces new dangers, is much more dangerous than any of the new technologies themselves.\\n\\nLulie What about just slowing it down such that people can adapt? Because like right now we've got something that is going so quickly that people are getting confused like old people if they see an image then they will assume that it's real and whereas if you have time that people are kind of adapt to, ah yes there are these deep fakes and so on.\\n\\nDavid Deutsch Well I'm not sure that time causes better adaptation because if things are happening fast, then also, news stories about how people have been scammed will be seen by your old people, and whereas if we slowed it down so that only one scam occurs every few months, then it might not be news.\\n\\nLulie issue. People are worried that AGI is, you know, maybe next week or just around the corner or in, like they used to say, in a few years, and now that we have these very good language models they say, maybe, like, small number of years, months, like, possibly weeks, and hence the proposed moratorium. So, what is the thing that makes you so chill? Why couldn't it lead to AGI? What's the problem with the idea of emergence? Because, you know, intelligence emerged once. Yes. So emergence isn't magic.\\n\\nDavid Deutsch It is, of course, possible that in the deep ocean a new form of life is emerging at this very moment and it will break the surface weeks from now.\\n\\nLulie Or on Pluto. Did you hear about Pluto? They discovered that there's ice or something and that they've sent another probe but it's going to take eight years for it to come back and find out whether there's life on Pluto.\\n\\nDavid Deutsch I hadn't heard that but there certainly is a possibility of life in various places in the solar system but not, I think, not intelligent life. But, if you're going to say emergence can do unexpected things, then you might as well say it about the deep ocean, because\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" of an algorithm that was programmed in by somebody who knew what the transformation between the inputs and outputs ought to be. Namely, when you press a certain button it multiplies and so on. AI's, modern AI's, essentially construct their algorithm themselves by generalizing a large amount of data. In early ones, this led to kind of embarrassing glitches, like when they identified a heap of rifles as a cat, because somebody worked out what they were doing and how to fool them. But the modern ones use so much data and have been honed by humans so well that they rarely do this. Although I've recently been playing around with the chat GPTs and I find that if you ask it some questions off the beaten track, you can quite easily cause it to do all the old things of either either saying nonsense, contradicting itself, committing howlers, where it says the opposite of known facts and so on. And that's different from human knowledge which is explanatory.\\n\\nNeither the calculator nor the chatbot ever produces a new explanation. You can ask it for an explanation but all it's doing is distilling explanations that already exist and that's the thing that it doesn't do very well either.\\n\\nLulie What is the difference between an explanation and the thing that it does produce?\\n\\nDavid Deutsch If we knew the detailed answer to that we'd know how to make an AGI. basically an explanation accounts for what it's trying to account for, like a physical process or the reason for something, it accounts for that, it accounts for a known thing that is trying to explain in terms of the unseen, unknown reasons behind it, which usually cannot even be seen even in So Brett Hall's favourite example is that we can never see the centre of the sun. We could never go there, any instrument that we send would get destroyed long before it got to the centre of the sun. So the centre of the sun can't be in GPT's training data? As it were, yes, exactly. And the only thing that can be in GPT's training data is what is seen about the Sun, namely its surface.\\n\\nLulie Couldn't GPT derive things about the Sun based on other theories that we have?\\n\\nDavid Deutsch It can deduce things from existing theories, yes. So if you ask it about the centre of the Sun, it will find some existing theory of the Sun and make a deduction from that. So that's not induction, that's deduction. And on the other hand, if there was a mystery about the Sun, like a couple of decades ago...\\n\\nLulie Isn't it induction via deduction? So the induction was all of the training data,\\n\\nDavid Deutsch and then the deduction is taking that data and then forming theories about it? No, because it didn't induce the data. It distilled it into a more compact form and then it can deduce things from that, but those things are only ever as good as the original theories were, probably slightly worse because by compressing the data it slightly some cases it degraded it a lot. A few decades ago, I was going to say recently, but in fact not so recent, it was when I was a graduate student, the big problem in astrophysics was that the sun wasn't producing enough neutrinos. Enough for what?\\n\\n3\\n\\nEnough.\\n\\nDavid Deutsch For your breakfast cereal? producing as many as the theory predicted. And this theory was extremely robust because it was also the theory that we used to predict the sun's brightness. And it predicted the sun's brightness extremely well, and also the brightness of other stars and how they change with time and So when they first made neutrino observatories with rather crude neutrino detectors, first they didn't find neutrinos but then they found a few but nowhere near enough. And when they refined it they found that there were only a third as many neutrinos as as predicted by the theory. And so there were all sorts of explanatory theories proposed, which couldn't possibly have been induced from anything, because all the data said was there are too few neutrinos. And of course you can always say that neutrinos have been eaten by a space monster, but generally when we produce scientific theories, we don't just want a new explanation, we want a new explanation that doesn't upset old explanations, that doesn't make them into nonsense, like the space monster theory. So you asked me what the difference is between an explanation and just a predictive theory. And by the way, there is no such thing as a purely predictive theory. They all have some kind of explanation and when you're quote inducing things from data you're always using an old explanation and just piling on some tweaks which don't have an explanation in order to make your supposed new theory.\\n\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" of an algorithm that was programmed in by somebody who knew what the transformation between the inputs and outputs ought to be. Namely, when you press a certain button it multiplies and so on. AI's, modern AI's, essentially construct their algorithm themselves by generalizing a large amount of data. In early ones, this led to kind of embarrassing glitches, like when they identified a heap of rifles as a cat, because somebody worked out what they were doing and how to fool them. But the modern ones use so much data and have been honed by humans so well that they rarely do this. Although I've recently been playing around with the chat GPTs and I find that if you ask it some questions off the beaten track, you can quite easily cause it to do all the old things of either either saying nonsense, contradicting itself, committing howlers, where it says the opposite of known facts and so on. And that's different from human knowledge which is explanatory.\\n\\nNeither the calculator nor the chatbot ever produces a new explanation. You can ask it for an explanation but all it's doing is distilling explanations that already exist and that's the thing that it doesn't do very well either.\\n\\nLulie What is the difference between an explanation and the thing that it does produce?\\n\\nDavid Deutsch If we knew the detailed answer to that we'd know how to make an AGI. basically an explanation accounts for what it's trying to account for, like a physical process or the reason for something, it accounts for that, it accounts for a known thing that is trying to explain in terms of the unseen, unknown reasons behind it, which usually cannot even be seen even in So Brett Hall's favourite example is that we can never see the centre of the sun. We could never go there, any instrument that we send would get destroyed long before it got to the centre of the sun. So the centre of the sun can't be in GPT's training data? As it were, yes, exactly. And the only thing that can be in GPT's training data is what is seen about the Sun, namely its surface.\\n\\nLulie Couldn't GPT derive things about the Sun based on other theories that we have?\\n\\nDavid Deutsch It can deduce things from existing theories, yes. So if you ask it about the centre of the Sun, it will find some existing theory of the Sun and make a deduction from that. So that's not induction, that's deduction. And on the other hand, if there was a mystery about the Sun, like a couple of decades ago...\\n\\nLulie Isn't it induction via deduction? So the induction was all of the training data,\\n\\nDavid Deutsch and then the deduction is taking that data and then forming theories about it? No, because it didn't induce the data. It distilled it into a more compact form and then it can deduce things from that, but those things are only ever as good as the original theories were, probably slightly worse because by compressing the data it slightly some cases it degraded it a lot. A few decades ago, I was going to say recently, but in fact not so recent, it was when I was a graduate student, the big problem in astrophysics was that the sun wasn't producing enough neutrinos. Enough for what?\\n\\n3\\n\\nEnough.\\n\\nDavid Deutsch For your breakfast cereal? producing as many as the theory predicted. And this theory was extremely robust because it was also the theory that we used to predict the sun's brightness. And it predicted the sun's brightness extremely well, and also the brightness of other stars and how they change with time and So when they first made neutrino observatories with rather crude neutrino detectors, first they didn't find neutrinos but then they found a few but nowhere near enough. And when they refined it they found that there were only a third as many neutrinos as as predicted by the theory. And so there were all sorts of explanatory theories proposed, which couldn't possibly have been induced from anything, because all the data said was there are too few neutrinos. And of course you can always say that neutrinos have been eaten by a space monster, but generally when we produce scientific theories, we don't just want a new explanation, we want a new explanation that doesn't upset old explanations, that doesn't make them into nonsense, like the space monster theory. So you asked me what the difference is between an explanation and just a predictive theory. And by the way, there is no such thing as a purely predictive theory. They all have some kind of explanation and when you're quote inducing things from data you're always using an old explanation and just piling on some tweaks which don't have an explanation in order to make your supposed new theory.\\n\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" of an algorithm that was programmed in by somebody who knew what the transformation between the inputs and outputs ought to be. Namely, when you press a certain button it multiplies and so on. AI's, modern AI's, essentially construct their algorithm themselves by generalizing a large amount of data. In early ones, this led to kind of embarrassing glitches, like when they identified a heap of rifles as a cat, because somebody worked out what they were doing and how to fool them. But the modern ones use so much data and have been honed by humans so well that they rarely do this. Although I've recently been playing around with the chat GPTs and I find that if you ask it some questions off the beaten track, you can quite easily cause it to do all the old things of either either saying nonsense, contradicting itself, committing howlers, where it says the opposite of known facts and so on. And that's different from human knowledge which is explanatory.\\n\\nNeither the calculator nor the chatbot ever produces a new explanation. You can ask it for an explanation but all it's doing is distilling explanations that already exist and that's the thing that it doesn't do very well either.\\n\\nLulie What is the difference between an explanation and the thing that it does produce?\\n\\nDavid Deutsch If we knew the detailed answer to that we'd know how to make an AGI. basically an explanation accounts for what it's trying to account for, like a physical process or the reason for something, it accounts for that, it accounts for a known thing that is trying to explain in terms of the unseen, unknown reasons behind it, which usually cannot even be seen even in So Brett Hall's favourite example is that we can never see the centre of the sun. We could never go there, any instrument that we send would get destroyed long before it got to the centre of the sun. So the centre of the sun can't be in GPT's training data? As it were, yes, exactly. And the only thing that can be in GPT's training data is what is seen about the Sun, namely its surface.\\n\\nLulie Couldn't GPT derive things about the Sun based on other theories that we have?\\n\\nDavid Deutsch It can deduce things from existing theories, yes. So if you ask it about the centre of the Sun, it will find some existing theory of the Sun and make a deduction from that. So that's not induction, that's deduction. And on the other hand, if there was a mystery about the Sun, like a couple of decades ago...\\n\\nLulie Isn't it induction via deduction? So the induction was all of the training data,\\n\\nDavid Deutsch and then the deduction is taking that data and then forming theories about it? No, because it didn't induce the data. It distilled it into a more compact form and then it can deduce things from that, but those things are only ever as good as the original theories were, probably slightly worse because by compressing the data it slightly some cases it degraded it a lot. A few decades ago, I was going to say recently, but in fact not so recent, it was when I was a graduate student, the big problem in astrophysics was that the sun wasn't producing enough neutrinos. Enough for what?\\n\\n3\\n\\nEnough.\\n\\nDavid Deutsch For your breakfast cereal? producing as many as the theory predicted. And this theory was extremely robust because it was also the theory that we used to predict the sun's brightness. And it predicted the sun's brightness extremely well, and also the brightness of other stars and how they change with time and So when they first made neutrino observatories with rather crude neutrino detectors, first they didn't find neutrinos but then they found a few but nowhere near enough. And when they refined it they found that there were only a third as many neutrinos as as predicted by the theory. And so there were all sorts of explanatory theories proposed, which couldn't possibly have been induced from anything, because all the data said was there are too few neutrinos. And of course you can always say that neutrinos have been eaten by a space monster, but generally when we produce scientific theories, we don't just want a new explanation, we want a new explanation that doesn't upset old explanations, that doesn't make them into nonsense, like the space monster theory. So you asked me what the difference is between an explanation and just a predictive theory. And by the way, there is no such thing as a purely predictive theory. They all have some kind of explanation and when you're quote inducing things from data you're always using an old explanation and just piling on some tweaks which don't have an explanation in order to make your supposed new theory.\\n\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" that doesn't upset old explanations, that doesn't make them into nonsense, like the space monster theory. So you asked me what the difference is between an explanation and just a predictive theory. And by the way, there is no such thing as a purely predictive theory. They all have some kind of explanation and when you're quote inducing things from data you're always using an old explanation and just piling on some tweaks which don't have an explanation in order to make your supposed new theory.\\n\\nSo no one could have induced from the data or lack of data in this case what the explanation was. Because the explanation turned out to have nothing to do with stars, nothing to do with measuring instruments, nothing to do with space. And somebody came up with it. And then it was tested. And passed the test. And now we know. Turns out that completely unbeknownst to anybody, there are three types of neutrinos and they convert from one to the other. So the Sun only produces one of those kinds, but by the time the neutrinos get to us, they've converted like to... The Sun produces neutrino type 1 and then when it's moved a few million miles from the centre of the Sun, it's converted to neutrino 2. And then, after a while, it's converted to neutrino 3. And our detectors can only see neutrino 1.\\n\\nLulie I'm just imagining a whole line of breakfast cereals.\\n\\nDavid Deutsch Is there a breakfast cereal called neutrino?\\n\\nLulie Well, there's lots of, you know, spaghetti-o... No, what is it?\\n\\nDavid Deutsch\\n\\nOh, neutrin\\n\\no's. Right.\\n\\n6\\n\\nHa ha ha!\\n\\nDavid Deutsch Right. The word neutrino began as a joke.\\n\\nLulie So it is a joke already?\\n\\nDavid Deutsch Yeah. It was one of the mid-20th century physicists, I think it was Enrico Fermi, he was just making a joke about a thing that's like a neutron but tiny. So he called it a neutrino.\\n\\nLulie\\n\\nLike nano?\\n\\nDavid Deutsch I think eno as a suffix has a meaning in Italian, I think. So anyway, so somebody thought of the explanation and it couldn't have been induced and that's Brett Hall's favourite example. He may have got it from my favourite example, which is not as good, which is that nobody could have been present at the Big Bang, yet we form theories of the Big Bang and we do not induce them from stuff we see around us which is nothing like the Big Bang.\\n\\nLulie So AIs can't create explanations which means they can't create anything that isn't already in their data set in some way?\\n\\nDavid Deutsch In some way, yes. they can move around parameters so they can you know if you said imagine some new theory about the sun it might I think it would be able to say things like well maybe the sun is twice as big as we think it is and then if you ask it why it might be able to say it might say things which are already explanations of something and which is drawn into that area. Now people will say well that's how humans make new explanations. Well that's how humans make explanations. Make new explanations. Yeah everything is just you know making connections between existing stuff. Yeah well in the sense... There's nothing new under the sun, David. In the sense that must be true, but that is the same kind of, you know, bad explanation of explanations that as it would be if you said, well, all the explanations are phrased in terms of 26 characters and all that make a new explanation is rearranging those characters in a new way. So why is it not that? Letters of the alphabet. Well... Well because it makes a difference whether we make it into a new explanation or not. And most rearrangements of characters are not new explanations. Ah, so you're saying there's an infinite number of connections you could draw\\n\\nLulie between things and only the ones that like actually work actually work?\\n\\nDavid Deutsch Yes, I mean it's not actually infinite, it's exponentially large, but for practical purposes that's the same thing.\\n\\nLulie When you make a new connection between two things, does that then create a new third thing which you can then make connections between? Is it actually true that the theory of knowledge works by making connections between things? Would you say that's an accurate representation?\\n\\nDavid Deutsch Well, again, if I knew the exact answer to that question, I could make an AGI. But I'm fairly sure, I mean,\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I mean, obviously it wouldn't be enough to knock down every possible explanation because for a start we know that quantum theory is false. We don't know for sure that the next theory will have many worlds in it I mean I think it will, but, but, you know, We can't prove anything like that. But I would replace the idea of increased credence with a theory that the experiment will provide a quiver full of arrows or a repertoire of arguments that goes beyond the known arguments, the known bad arguments, and will reach into other types of arguments because the reason I would say that is that Some of the existing misconceptions about quantum theory reside in misconceptions about the methodology of science. Now I've written a paper about what I think is the right methodology of science that's more based on empiricism. Of course, I think that empiricism is a mistake and can be knocked down in its own terms, so we shouldn't, but not everybody thinks that. Now, once we have an experiment, if that was actually done, then people could not use their arguments based on a fallacious idea of empiricism, because their theory would have been refuted even by the standards of empiricism, which shouldn't have been needed in the first place.\\n\\nI think that that's the way I would express that the repertoire of arguments will become more powerful. If that experiment would own successfully.\\n\\n2 0:59:22 The next question I have is, how far do you take the principle that open ended scientific progress is the best way to deal with existential dangers to give it one example. So you have something like gain of function research, right? And it's conceivable that it could lead to more knowledge in how to stop dangerous pathogens. But I guess at least in Bayesian terms, you could say it seems even more likely that it can or has led to the spread of a man-made pathogen that would have not otherwise been naturally developed. So would your belief in open and assigned to progress allow us to say okay let's stop Dana function research.\\n\\n1 1:00:03 No, it wouldn't allow us to say let's stop it. It might make it reasonable to say, let us do research into how to make laboratories more secure before we do gain a function research. It's really part of the same thing it's it's it's like saying, let's do research into how to make the plastic hoses, through which the reagents pass more impermeable before we actually do the experiments with the reagents. So it's all part of the same experiment. I wouldn't want to stop something just because new knowledge might be discovered. the no-no in my view. But which knowledge we need to discover first, that's the problem of scheduling, which is a non-trivial part of any research and of any learning.\\n\\n2 1:00:59 But would it be considerable for you to say that until we figure out how to make sure these laboratories are safe to a certain standard, we will. Meanwhile we'll focus on doing the other kind of research so gain of function can restart, but until then it's not allowed.\\n\\n1 1:01:23 Yes, in principle that will be reasonable I don't know enough about the actual situation to have a view. You know, I don't know how these labs work. what the precautions consist of. And when I hear people talking about, for example, lab leak, I think, well, most likely lab leak is that one of the people who works there walks out of the front door. So the leak is not a leak from the lab to the outside. The leak is from the test tube to the person and then from the person walking out the door. And I don't know enough about what these proportions are or what the state of the art is to know to what extent the risk is actually minimized. It could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" that doesn't upset old explanations, that doesn't make them into nonsense, like the space monster theory. So you asked me what the difference is between an explanation and just a predictive theory. And by the way, there is no such thing as a purely predictive theory. They all have some kind of explanation and when you're quote inducing things from data you're always using an old explanation and just piling on some tweaks which don't have an explanation in order to make your supposed new theory.\\n\\nSo no one could have induced from the data or lack of data in this case what the explanation was. Because the explanation turned out to have nothing to do with stars, nothing to do with measuring instruments, nothing to do with space. And somebody came up with it. And then it was tested. And passed the test. And now we know. Turns out that completely unbeknownst to anybody, there are three types of neutrinos and they convert from one to the other. So the Sun only produces one of those kinds, but by the time the neutrinos get to us, they've converted like to... The Sun produces neutrino type 1 and then when it's moved a few million miles from the centre of the Sun, it's converted to neutrino 2. And then, after a while, it's converted to neutrino 3. And our detectors can only see neutrino 1.\\n\\nLulie I'm just imagining a whole line of breakfast cereals.\\n\\nDavid Deutsch Is there a breakfast cereal called neutrino?\\n\\nLulie Well, there's lots of, you know, spaghetti-o... No, what is it?\\n\\nDavid Deutsch\\n\\nOh, neutrin\\n\\no's. Right.\\n\\n6\\n\\nHa ha ha!\\n\\nDavid Deutsch Right. The word neutrino began as a joke.\\n\\nLulie So it is a joke already?\\n\\nDavid Deutsch Yeah. It was one of the mid-20th century physicists, I think it was Enrico Fermi, he was just making a joke about a thing that's like a neutron but tiny. So he called it a neutrino.\\n\\nLulie\\n\\nLike nano?\\n\\nDavid Deutsch I think eno as a suffix has a meaning in Italian, I think. So anyway, so somebody thought of the explanation and it couldn't have been induced and that's Brett Hall's favourite example. He may have got it from my favourite example, which is not as good, which is that nobody could have been present at the Big Bang, yet we form theories of the Big Bang and we do not induce them from stuff we see around us which is nothing like the Big Bang.\\n\\nLulie So AIs can't create explanations which means they can't create anything that isn't already in their data set in some way?\\n\\nDavid Deutsch In some way, yes. they can move around parameters so they can you know if you said imagine some new theory about the sun it might I think it would be able to say things like well maybe the sun is twice as big as we think it is and then if you ask it why it might be able to say it might say things which are already explanations of something and which is drawn into that area. Now people will say well that's how humans make new explanations. Well that's how humans make explanations. Make new explanations. Yeah everything is just you know making connections between existing stuff. Yeah well in the sense... There's nothing new under the sun, David. In the sense that must be true, but that is the same kind of, you know, bad explanation of explanations that as it would be if you said, well, all the explanations are phrased in terms of 26 characters and all that make a new explanation is rearranging those characters in a new way. So why is it not that? Letters of the alphabet. Well... Well because it makes a difference whether we make it into a new explanation or not. And most rearrangements of characters are not new explanations. Ah, so you're saying there's an infinite number of connections you could draw\\n\\nLulie between things and only the ones that like actually work actually work?\\n\\nDavid Deutsch Yes, I mean it's not actually infinite, it's exponentially large, but for practical purposes that's the same thing.\\n\\nLulie When you make a new connection between two things, does that then create a new third thing which you can then make connections between? Is it actually true that the theory of knowledge works by making connections between things? Would you say that's an accurate representation?\\n\\nDavid Deutsch Well, again, if I knew the exact answer to that question, I could make an AGI. But I'm fairly sure, I mean,\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\", but it seems to be in an expanded universe are, there will be like a finite amount of total matter that will ever exist in our light cone right there's a limit. And that means that there's a limit on the amount of computation that this matter can, you know, execute the amount of energy can provide, perhaps even the amount of economic value we can sustain right so we, it would be weird if the GDP per atom could be arbitrarily large. So, does this impose some sort of limit on your concept of the beginning of infinity.\\n\\n1 0:50:32 So, what you've just recounted is a cosmological theory. This, this, the universe could be like that. know very little about the universes in the large, like theories of cosmology are changing on a time scale of about a decade. So it doesn't make all that much sense to speculate about what the ultimate asymptotic form of very small things like we know that our conception of physical processes must break down somehow at the level of quantum gravity, like 10 to the minus 42 seconds and that kind of thing but, but we have no idea what happens below There's no, there's no argument for that at all. It's just that we don't know what happens beyond that. Now what happens beyond that may be a finite limit. from it's being imposed by inherent hardware limitations. For example, if there's a finite amount of GNP available in the distant future, then it's still up to us whether we spend that on mathematics or music or political systems or any of the thousands of even more worthwhile things that have yet to be invented. So it's up to us which ideas we fill the 10 to the 10 to the 10 to the 10 bits with. Now, my guess is that there are no such limits, but my worldview is not affected by whether there are such limits, because as I said, it's still up to us what to fill them with. And then if we get chopped off at some point in the future, then everything will have been worthwhile up to then.\\n\\n2 0:53:07 Gotcha. Okay, so the way I understand your concept of gaining infinity, it seems to me that the more knowledge we gain, the more knowledge we're in a position to gain. So there should be like an exponential growth of knowledge. But if we look at the last 50 years, it seems that there's been a slowdown in or decrease in research productivity, economic growth, productivity growth. And this seems compatible with the story that, you know, that there's a limited amount of fruit on the tree that we picked the low hanging fruit. And now there's less and less fruit and harder and harder fruit to pick, and you know eventually well the orchard will be empty. So do you have an alternative explanation for what's going on\\n\\n1 0:53:45 in the last 50 years? Yes, I think it's very simple. There are sociological factors in academic life which have stultified the culture, and not totally and not everywhere, but that has been a tendency in what has happened, and it has resulted in a loss of productivity in many sectors, in many ways, but not in every sector, uh... the the uh... for example i i think that was a i've often said there was a stultification in uh... theoretical physics\\n\\n5\\n\\n0:54:35\\n\\nuh...\\n\\n1 0:54:36 starting in let's say the nineteen twenties and it still hasn't fully dissipated if it wasn't for that quantum computers would have been invented in the 1930s and built in the 1960s. So that is just an accidental fact, but it just goes to show that there are no guarantees. The fact that our horizons are unlimited, does not guarantee that we won't start declining tomorrow. I don't think we are currently declining. I think the these declines that we see are parochial effects caused by specific mistakes that\\n\\n2 0:55:24 have been made and which can be undone. Okay, so I want to ask you a question about Bayesianism versus Popperianism. So one reason why people prefer base is because there seems to be a way of describing changes and changes in epic static status, when their relative status of a theory hasn't changed so give you an example. Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" that doesn't upset old explanations, that doesn't make them into nonsense, like the space monster theory. So you asked me what the difference is between an explanation and just a predictive theory. And by the way, there is no such thing as a purely predictive theory. They all have some kind of explanation and when you're quote inducing things from data you're always using an old explanation and just piling on some tweaks which don't have an explanation in order to make your supposed new theory.\\n\\nSo no one could have induced from the data or lack of data in this case what the explanation was. Because the explanation turned out to have nothing to do with stars, nothing to do with measuring instruments, nothing to do with space. And somebody came up with it. And then it was tested. And passed the test. And now we know. Turns out that completely unbeknownst to anybody, there are three types of neutrinos and they convert from one to the other. So the Sun only produces one of those kinds, but by the time the neutrinos get to us, they've converted like to... The Sun produces neutrino type 1 and then when it's moved a few million miles from the centre of the Sun, it's converted to neutrino 2. And then, after a while, it's converted to neutrino 3. And our detectors can only see neutrino 1.\\n\\nLulie I'm just imagining a whole line of breakfast cereals.\\n\\nDavid Deutsch Is there a breakfast cereal called neutrino?\\n\\nLulie Well, there's lots of, you know, spaghetti-o... No, what is it?\\n\\nDavid Deutsch\\n\\nOh, neutrin\\n\\no's. Right.\\n\\n6\\n\\nHa ha ha!\\n\\nDavid Deutsch Right. The word neutrino began as a joke.\\n\\nLulie So it is a joke already?\\n\\nDavid Deutsch Yeah. It was one of the mid-20th century physicists, I think it was Enrico Fermi, he was just making a joke about a thing that's like a neutron but tiny. So he called it a neutrino.\\n\\nLulie\\n\\nLike nano?\\n\\nDavid Deutsch I think eno as a suffix has a meaning in Italian, I think. So anyway, so somebody thought of the explanation and it couldn't have been induced and that's Brett Hall's favourite example. He may have got it from my favourite example, which is not as good, which is that nobody could have been present at the Big Bang, yet we form theories of the Big Bang and we do not induce them from stuff we see around us which is nothing like the Big Bang.\\n\\nLulie So AIs can't create explanations which means they can't create anything that isn't already in their data set in some way?\\n\\nDavid Deutsch In some way, yes. they can move around parameters so they can you know if you said imagine some new theory about the sun it might I think it would be able to say things like well maybe the sun is twice as big as we think it is and then if you ask it why it might be able to say it might say things which are already explanations of something and which is drawn into that area. Now people will say well that's how humans make new explanations. Well that's how humans make explanations. Make new explanations. Yeah everything is just you know making connections between existing stuff. Yeah well in the sense... There's nothing new under the sun, David. In the sense that must be true, but that is the same kind of, you know, bad explanation of explanations that as it would be if you said, well, all the explanations are phrased in terms of 26 characters and all that make a new explanation is rearranging those characters in a new way. So why is it not that? Letters of the alphabet. Well... Well because it makes a difference whether we make it into a new explanation or not. And most rearrangements of characters are not new explanations. Ah, so you're saying there's an infinite number of connections you could draw\\n\\nLulie between things and only the ones that like actually work actually work?\\n\\nDavid Deutsch Yes, I mean it's not actually infinite, it's exponentially large, but for practical purposes that's the same thing.\\n\\nLulie When you make a new connection between two things, does that then create a new third thing which you can then make connections between? Is it actually true that the theory of knowledge works by making connections between things? Would you say that's an accurate representation?\\n\\nDavid Deutsch Well, again, if I knew the exact answer to that question, I could make an AGI. But I'm fairly sure, I mean,\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" of an algorithm that was programmed in by somebody who knew what the transformation between the inputs and outputs ought to be. Namely, when you press a certain button it multiplies and so on. AI's, modern AI's, essentially construct their algorithm themselves by generalizing a large amount of data. In early ones, this led to kind of embarrassing glitches, like when they identified a heap of rifles as a cat, because somebody worked out what they were doing and how to fool them. But the modern ones use so much data and have been honed by humans so well that they rarely do this. Although I've recently been playing around with the chat GPTs and I find that if you ask it some questions off the beaten track, you can quite easily cause it to do all the old things of either either saying nonsense, contradicting itself, committing howlers, where it says the opposite of known facts and so on. And that's different from human knowledge which is explanatory.\\n\\nNeither the calculator nor the chatbot ever produces a new explanation. You can ask it for an explanation but all it's doing is distilling explanations that already exist and that's the thing that it doesn't do very well either.\\n\\nLulie What is the difference between an explanation and the thing that it does produce?\\n\\nDavid Deutsch If we knew the detailed answer to that we'd know how to make an AGI. basically an explanation accounts for what it's trying to account for, like a physical process or the reason for something, it accounts for that, it accounts for a known thing that is trying to explain in terms of the unseen, unknown reasons behind it, which usually cannot even be seen even in So Brett Hall's favourite example is that we can never see the centre of the sun. We could never go there, any instrument that we send would get destroyed long before it got to the centre of the sun. So the centre of the sun can't be in GPT's training data? As it were, yes, exactly. And the only thing that can be in GPT's training data is what is seen about the Sun, namely its surface.\\n\\nLulie Couldn't GPT derive things about the Sun based on other theories that we have?\\n\\nDavid Deutsch It can deduce things from existing theories, yes. So if you ask it about the centre of the Sun, it will find some existing theory of the Sun and make a deduction from that. So that's not induction, that's deduction. And on the other hand, if there was a mystery about the Sun, like a couple of decades ago...\\n\\nLulie Isn't it induction via deduction? So the induction was all of the training data,\\n\\nDavid Deutsch and then the deduction is taking that data and then forming theories about it? No, because it didn't induce the data. It distilled it into a more compact form and then it can deduce things from that, but those things are only ever as good as the original theories were, probably slightly worse because by compressing the data it slightly some cases it degraded it a lot. A few decades ago, I was going to say recently, but in fact not so recent, it was when I was a graduate student, the big problem in astrophysics was that the sun wasn't producing enough neutrinos. Enough for what?\\n\\n3\\n\\nEnough.\\n\\nDavid Deutsch For your breakfast cereal? producing as many as the theory predicted. And this theory was extremely robust because it was also the theory that we used to predict the sun's brightness. And it predicted the sun's brightness extremely well, and also the brightness of other stars and how they change with time and So when they first made neutrino observatories with rather crude neutrino detectors, first they didn't find neutrinos but then they found a few but nowhere near enough. And when they refined it they found that there were only a third as many neutrinos as as predicted by the theory. And so there were all sorts of explanatory theories proposed, which couldn't possibly have been induced from anything, because all the data said was there are too few neutrinos. And of course you can always say that neutrinos have been eaten by a space monster, but generally when we produce scientific theories, we don't just want a new explanation, we want a new explanation that doesn't upset old explanations, that doesn't make them into nonsense, like the space monster theory. So you asked me what the difference is between an explanation and just a predictive theory. And by the way, there is no such thing as a purely predictive theory. They all have some kind of explanation and when you're quote inducing things from data you're always using an old explanation and just piling on some tweaks which don't have an explanation in order to make your supposed new theory.\\n\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I mean, obviously it wouldn't be enough to knock down every possible explanation because for a start we know that quantum theory is false. We don't know for sure that the next theory will have many worlds in it I mean I think it will, but, but, you know, We can't prove anything like that. But I would replace the idea of increased credence with a theory that the experiment will provide a quiver full of arrows or a repertoire of arguments that goes beyond the known arguments, the known bad arguments, and will reach into other types of arguments because the reason I would say that is that Some of the existing misconceptions about quantum theory reside in misconceptions about the methodology of science. Now I've written a paper about what I think is the right methodology of science that's more based on empiricism. Of course, I think that empiricism is a mistake and can be knocked down in its own terms, so we shouldn't, but not everybody thinks that. Now, once we have an experiment, if that was actually done, then people could not use their arguments based on a fallacious idea of empiricism, because their theory would have been refuted even by the standards of empiricism, which shouldn't have been needed in the first place.\\n\\nI think that that's the way I would express that the repertoire of arguments will become more powerful. If that experiment would own successfully.\\n\\n2 0:59:22 The next question I have is, how far do you take the principle that open ended scientific progress is the best way to deal with existential dangers to give it one example. So you have something like gain of function research, right? And it's conceivable that it could lead to more knowledge in how to stop dangerous pathogens. But I guess at least in Bayesian terms, you could say it seems even more likely that it can or has led to the spread of a man-made pathogen that would have not otherwise been naturally developed. So would your belief in open and assigned to progress allow us to say okay let's stop Dana function research.\\n\\n1 1:00:03 No, it wouldn't allow us to say let's stop it. It might make it reasonable to say, let us do research into how to make laboratories more secure before we do gain a function research. It's really part of the same thing it's it's it's like saying, let's do research into how to make the plastic hoses, through which the reagents pass more impermeable before we actually do the experiments with the reagents. So it's all part of the same experiment. I wouldn't want to stop something just because new knowledge might be discovered. the no-no in my view. But which knowledge we need to discover first, that's the problem of scheduling, which is a non-trivial part of any research and of any learning.\\n\\n2 1:00:59 But would it be considerable for you to say that until we figure out how to make sure these laboratories are safe to a certain standard, we will. Meanwhile we'll focus on doing the other kind of research so gain of function can restart, but until then it's not allowed.\\n\\n1 1:01:23 Yes, in principle that will be reasonable I don't know enough about the actual situation to have a view. You know, I don't know how these labs work. what the precautions consist of. And when I hear people talking about, for example, lab leak, I think, well, most likely lab leak is that one of the people who works there walks out of the front door. So the leak is not a leak from the lab to the outside. The leak is from the test tube to the person and then from the person walking out the door. And I don't know enough about what these proportions are or what the state of the art is to know to what extent the risk is actually minimized. It could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" of an algorithm that was programmed in by somebody who knew what the transformation between the inputs and outputs ought to be. Namely, when you press a certain button it multiplies and so on. AI's, modern AI's, essentially construct their algorithm themselves by generalizing a large amount of data. In early ones, this led to kind of embarrassing glitches, like when they identified a heap of rifles as a cat, because somebody worked out what they were doing and how to fool them. But the modern ones use so much data and have been honed by humans so well that they rarely do this. Although I've recently been playing around with the chat GPTs and I find that if you ask it some questions off the beaten track, you can quite easily cause it to do all the old things of either either saying nonsense, contradicting itself, committing howlers, where it says the opposite of known facts and so on. And that's different from human knowledge which is explanatory.\\n\\nNeither the calculator nor the chatbot ever produces a new explanation. You can ask it for an explanation but all it's doing is distilling explanations that already exist and that's the thing that it doesn't do very well either.\\n\\nLulie What is the difference between an explanation and the thing that it does produce?\\n\\nDavid Deutsch If we knew the detailed answer to that we'd know how to make an AGI. basically an explanation accounts for what it's trying to account for, like a physical process or the reason for something, it accounts for that, it accounts for a known thing that is trying to explain in terms of the unseen, unknown reasons behind it, which usually cannot even be seen even in So Brett Hall's favourite example is that we can never see the centre of the sun. We could never go there, any instrument that we send would get destroyed long before it got to the centre of the sun. So the centre of the sun can't be in GPT's training data? As it were, yes, exactly. And the only thing that can be in GPT's training data is what is seen about the Sun, namely its surface.\\n\\nLulie Couldn't GPT derive things about the Sun based on other theories that we have?\\n\\nDavid Deutsch It can deduce things from existing theories, yes. So if you ask it about the centre of the Sun, it will find some existing theory of the Sun and make a deduction from that. So that's not induction, that's deduction. And on the other hand, if there was a mystery about the Sun, like a couple of decades ago...\\n\\nLulie Isn't it induction via deduction? So the induction was all of the training data,\\n\\nDavid Deutsch and then the deduction is taking that data and then forming theories about it? No, because it didn't induce the data. It distilled it into a more compact form and then it can deduce things from that, but those things are only ever as good as the original theories were, probably slightly worse because by compressing the data it slightly some cases it degraded it a lot. A few decades ago, I was going to say recently, but in fact not so recent, it was when I was a graduate student, the big problem in astrophysics was that the sun wasn't producing enough neutrinos. Enough for what?\\n\\n3\\n\\nEnough.\\n\\nDavid Deutsch For your breakfast cereal? producing as many as the theory predicted. And this theory was extremely robust because it was also the theory that we used to predict the sun's brightness. And it predicted the sun's brightness extremely well, and also the brightness of other stars and how they change with time and So when they first made neutrino observatories with rather crude neutrino detectors, first they didn't find neutrinos but then they found a few but nowhere near enough. And when they refined it they found that there were only a third as many neutrinos as as predicted by the theory. And so there were all sorts of explanatory theories proposed, which couldn't possibly have been induced from anything, because all the data said was there are too few neutrinos. And of course you can always say that neutrinos have been eaten by a space monster, but generally when we produce scientific theories, we don't just want a new explanation, we want a new explanation that doesn't upset old explanations, that doesn't make them into nonsense, like the space monster theory. So you asked me what the difference is between an explanation and just a predictive theory. And by the way, there is no such thing as a purely predictive theory. They all have some kind of explanation and when you're quote inducing things from data you're always using an old explanation and just piling on some tweaks which don't have an explanation in order to make your supposed new theory.\\n\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" is correct and another thing isn't correct it will talk nonsense and yet it knows which of them is correct. I've seen several cases of that and I was very impressed because it means that it's knowledge of language. It's a thing that I don't actually understand how it can be as good at speaking English, writing English, as it is. It's much better than most people, but more interestingly, it's much better than the average or the typical text on the internet. So it makes mistakes, but it makes far fewer mistakes than a typical text on the internet. Do you think this is going to revolutionise the economy and I can't prophesy the applications of modern AIs, chatbots and so on. I have found, for what it's worth, as it were, I have found it useful but not revolutionary in my own work and in my writing and whatever it is useful but I can't see possibility for it to revolutionize what I do. Whether it can revolutionize the economy depends on something slightly different because for that it doesn't need to really have a fundamental new functionality. It could be that a lot of existing jobs, and people are scared that computer programming is one of them, where only a proportion of the job, let's say 10%, of a particular programming job involves human creativity, and the rest is basically hack work.\\n\\nwhich is a big if, because I'm not convinced of this either. If chatbots can reliably perform the hack work, then it might be argued, I think again wrongly, that if a given task can be done with only a tenth as much work, then we might need only a tenth as many programmers in the long run.\\n\\nLulie And unfortunately we got interrupted, so David never finished his thought about why programmer jobs might be safer. But if you have any questions about anything in this episode, leave them jobs might be safer. But if you have any questions about anything in this episode, leave them on the tweet, which I will link in the show notes about this episode. Thank you.2 0:00:00 Okay, today I'm speaking with David Deutsch. Now this is a conversation that I've been eagerly wanting to have for years so this is very exciting for me. So, first let's talk about AI. Can you briefly explain why you anticipate that AI's will be no more fundamentally intelligent than humans.\\n\\n1 0:00:19 I suppose you mean capable of all the same types of cognition as humans are in principle.\\n\\n4\\n\\n0:00:37\\n\\nYes.\\n\\n1 0:00:38 So that would include, you know, doing science and doing art and in principle also falling in love and being good and being evil and all that. So the reason is twofold and one half is about computation hardware, computation hardware, and the other is about hardware. We know that our, our brains are Turing complete bits of hardware, and therefore can exhibit the functionality of running any computable function program for any computable function. Now, when I say any, I don't really mean any, because you and I sitting here, you know, we're having conversation and we could say, you know, we could have any conversation. Well, we can assume that maybe in a hundred years time, we'll both be dead. And therefore, the number of conversations we could have is strictly limited. And also, some conversations depend on speed of computation. So, you know, if we're going to be solving the traveling salesman problem, then there are there are many traveling salesman problems that we wouldn't be able to solve in the programs we can run, apart from by speed and memory capacity. So all limitations on us, hardware limitations on us, boil down to speed and memory capacity. And both those can be augmented to the level of any other entity that is in the universe. Because, you know, if somebody builds a computer that can think faster than the brain, then we can use that very computer or that very technology to make our thinking go just as fast as that. So that's the hardware.\\n\\nAs far as explanations go, can we reach the same kind of explanations as any other entity, let's say, usually this is said not in terms of AGIs but in terms of extraterrestrial intelligences, but also it's said about AGIs, you know, what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestri\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" for practical purposes that's the same thing.\\n\\nLulie When you make a new connection between two things, does that then create a new third thing which you can then make connections between? Is it actually true that the theory of knowledge works by making connections between things? Would you say that's an accurate representation?\\n\\nDavid Deutsch Well, again, if I knew the exact answer to that question, I could make an AGI. But I'm fairly sure, I mean, I can't think of an alternative to the process being and mutating existing things. So you can either change a theory slightly in the hope that the rest of it still works or you can make new combinations between things. That's also the two ways that biological mutations can happen in DNA. You can either have a cosmic ray strikes the DNA and changes one base pair or something, or you can have some error in copying results in a bit of DNA that came from somewhere else being stuck into a particular place in the DNA strand.\\n\\nLulie So this sounds like random variation of one thing rather than a new connection between two things?\\n\\nDavid Deutsch Well, no, the second process is in a way a new connection between two things because it, if there's a whole bit of DNA in a different organism or in a different part of the DNA and it gets copied, as it were, into the wrong place in the evolving organism, and that's a way that evolution can take place. Most instances of that just kill the organism but every so often it either leaves the functionality unchanged or makes it better. So what does that have to do with connecting? If a dog gets a piece of DNA from a lobster then that creates a similarity between dogs and lobsters that didn't exist before. And people can, paleogeneticists or whatever they're called, can look at DNA. Bacteria do this a lot. Higher organisms do it less and less because, I think basically because there's less and less chance of you surviving such a thing. For it to be part of evolution, you've got to be able to survive the new adaptation being slightly there, and a bit more there, and a bit more there, because giving a dog lobster claws wouldn't work, because the machinery for controlling the claws and deciding when to use them and so on hasn't been transferred. It would have to evolve. The point is that human new explanations evolve intentionally.\\n\\nThey involve randomness at some level, but the business part of creating the new explanation is what happens to the randomness after it's generated and it is changed and further changed intentionally to solve a problem and that is what induction can't possibly do but we don't know what\\n\\nLulie can do it. Induction can't make a change to solve a problem yes but evolution both genetic and evolution of ideas in the mind can?\\n\\nDavid Deutsch Well, no, it can... so every new genetic sequence is produced first and tried out later. Marxism thinks that an organism's environment and its own actions can cause a change in its genome. And that can't happen, and it couldn't happen because it's the same as induction, which also can't happen. So is this also true within a mind? Like you have to make up something before you can tell\\n\\nLulie whether it solves the problem or not?\\n\\nDavid Deutsch Yes, but there's a difference, which is that the making up process isn't purely random.\\n\\n5\\n\\nHow so?\\n\\nDavid Deutsch For example, a human can do the thing which I just said the dog and the lobster can't do. can think of the solar neutrino problem and can think, could it be that there's a new particle that isn't even a neutrino? And what would that particle... Now that in itself is not the explanation, but it's the germ of an explanation be like made up before you can check whether that's a good question? Yes, but it that making up that involves much less random trial and error than it would that it would take to try all possible variations randomly. So the fact that a person can think of an analogy, you know, some people think that human thinking is all about analogies. So we can take a whole idea from some other place and see if it fits in this place, which it never does immediately, but then you can say, well, how can we change it further so it does fit? So a person who thought, well, maybe there's a wholly new particle involved, at some point that person may say, maybe that whole new particle is just a neutrino, but of a different type. And then they're well on the way to solving. Of course there are many other considerations, but when I say many, it's nothing compared with how many you'd have to check\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\", but then you can say, well, how can we change it further so it does fit? So a person who thought, well, maybe there's a wholly new particle involved, at some point that person may say, maybe that whole new particle is just a neutrino, but of a different type. And then they're well on the way to solving. Of course there are many other considerations, but when I say many, it's nothing compared with how many you'd have to check through by random variation, or by trying every possibility or whatever.\\n\\nLulie Why is it not the case that you need to do this random thing by producing the idea first and checking it afterwards?\\n\\nDavid Deutsch Or are you saying that you can do that? Human minds produce conjectures, raw conjectures, much more efficiently than any either systematic search or random search.\\n\\nLulie And we don't know why?\\n\\n4\\n\\nWe don't know how.\\n\\n3 Okay, we don't know how.\\n\\nDavid Deutsch So, for example, chess playing engines have to search through billions of times more possibilities than chess grandmasters do.\\n\\nLulie\\n\\nWell, they apparently do?\\n\\nDavid Deutsch No, it's impossible that... well, unless there's something in the hardware of the human brain that we don't know about.\\n\\nLulie What if it's like super parallel?\\n\\nDavid Deutsch Yeah, well, it would have to go through billions of times more processing than we think it can, which would mean it would have to be billions of times more efficient thermodynamically than we think it is. I mean, we don't know. We don't know how the brain works either, let alone...\\n\\nLulie Do we know this for biological evolution, as in, do we know how they, like, biological evolution is as efficient as it is? Because I thought that we couldn't very well program biological evolution.\\n\\nDavid Deutsch Yeah, we can't, but, and that, the analogue of that program, that, of that problem does exist but it's not as severe a problem as it is for thinking. Although computer simulations of biological evolution aren't very good, they're not complete rubbish. They do sort of mimic evolution a bit and I think it's a mystery, you know, why real evolution is that much better. But I don't think that real evolution is that much better by that much. It's not a factor of billions. It's, you know, there's something missing that makes it chug along rather than efficiently. I don't think it's the same problem. Although it is the same in one respect. People who do biological evolution, simulate biological evolution on computers, seem blind to this problem, seem to me to be blind to this problem in the same way that people who are trying to make AGI out of AIs are blind to the difference between those two. Because they have the wrong epistemology? Yeah, but I don't know what the answer is. I know one misconception they have in the case of AGI, which by itself makes it impossible for them to create an AGI.\\n\\nLulie Namely the thing about prediction.\\n\\nDavid Deutsch Namely the thing about induction being impossible. With biological evolution, I don't know what it would take to make an analog of biological evolution on a computer. Of course I'm sure it can be done. My guess is that people will do it and it will be relatively simple to do once someone has had the idea of what biology does. There are various apparently indicative things in biology of the same biological structure occurring in evolutionarily very distant organisms. I think the famous one is that there's a gene involved in the development of the eye, which is the same gene is found in different eyes that work by completely different physical principles. So it's not that they have a common origin, unless the common origin is something so deep in history that we don't recognize it as being eyes. Convergent evolution? Yes, but it's convergent evolution without an apparent reason. Is that different from convergent evolution? Yeah, yeah. So convergent evolution is that things in the same environment tend to end up with the same appearance, the same lifestyle, and so on.\\n\\nLulie works by things like empiricism, induction, Bayesianism, Bayesian epistemology. And so AI, so first I guess my first question is AI, current AI, following that?\\n\\nDavid Deutsch Current AI was inspired by that but that's not what it's doing. It's not doing induction any more than anything else is. Induction is impossible. Also, current AI was also inspired by the architecture of the brain, neural nets. There are these programming hardware\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" of an algorithm that was programmed in by somebody who knew what the transformation between the inputs and outputs ought to be. Namely, when you press a certain button it multiplies and so on. AI's, modern AI's, essentially construct their algorithm themselves by generalizing a large amount of data. In early ones, this led to kind of embarrassing glitches, like when they identified a heap of rifles as a cat, because somebody worked out what they were doing and how to fool them. But the modern ones use so much data and have been honed by humans so well that they rarely do this. Although I've recently been playing around with the chat GPTs and I find that if you ask it some questions off the beaten track, you can quite easily cause it to do all the old things of either either saying nonsense, contradicting itself, committing howlers, where it says the opposite of known facts and so on. And that's different from human knowledge which is explanatory.\\n\\nNeither the calculator nor the chatbot ever produces a new explanation. You can ask it for an explanation but all it's doing is distilling explanations that already exist and that's the thing that it doesn't do very well either.\\n\\nLulie What is the difference between an explanation and the thing that it does produce?\\n\\nDavid Deutsch If we knew the detailed answer to that we'd know how to make an AGI. basically an explanation accounts for what it's trying to account for, like a physical process or the reason for something, it accounts for that, it accounts for a known thing that is trying to explain in terms of the unseen, unknown reasons behind it, which usually cannot even be seen even in So Brett Hall's favourite example is that we can never see the centre of the sun. We could never go there, any instrument that we send would get destroyed long before it got to the centre of the sun. So the centre of the sun can't be in GPT's training data? As it were, yes, exactly. And the only thing that can be in GPT's training data is what is seen about the Sun, namely its surface.\\n\\nLulie Couldn't GPT derive things about the Sun based on other theories that we have?\\n\\nDavid Deutsch It can deduce things from existing theories, yes. So if you ask it about the centre of the Sun, it will find some existing theory of the Sun and make a deduction from that. So that's not induction, that's deduction. And on the other hand, if there was a mystery about the Sun, like a couple of decades ago...\\n\\nLulie Isn't it induction via deduction? So the induction was all of the training data,\\n\\nDavid Deutsch and then the deduction is taking that data and then forming theories about it? No, because it didn't induce the data. It distilled it into a more compact form and then it can deduce things from that, but those things are only ever as good as the original theories were, probably slightly worse because by compressing the data it slightly some cases it degraded it a lot. A few decades ago, I was going to say recently, but in fact not so recent, it was when I was a graduate student, the big problem in astrophysics was that the sun wasn't producing enough neutrinos. Enough for what?\\n\\n3\\n\\nEnough.\\n\\nDavid Deutsch For your breakfast cereal? producing as many as the theory predicted. And this theory was extremely robust because it was also the theory that we used to predict the sun's brightness. And it predicted the sun's brightness extremely well, and also the brightness of other stars and how they change with time and So when they first made neutrino observatories with rather crude neutrino detectors, first they didn't find neutrinos but then they found a few but nowhere near enough. And when they refined it they found that there were only a third as many neutrinos as as predicted by the theory. And so there were all sorts of explanatory theories proposed, which couldn't possibly have been induced from anything, because all the data said was there are too few neutrinos. And of course you can always say that neutrinos have been eaten by a space monster, but generally when we produce scientific theories, we don't just want a new explanation, we want a new explanation that doesn't upset old explanations, that doesn't make them into nonsense, like the space monster theory. So you asked me what the difference is between an explanation and just a predictive theory. And by the way, there is no such thing as a purely predictive theory. They all have some kind of explanation and when you're quote inducing things from data you're always using an old explanation and just piling on some tweaks which don't have an explanation in order to make your supposed new theory.\\n\\n\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" for practical purposes that's the same thing.\\n\\nLulie When you make a new connection between two things, does that then create a new third thing which you can then make connections between? Is it actually true that the theory of knowledge works by making connections between things? Would you say that's an accurate representation?\\n\\nDavid Deutsch Well, again, if I knew the exact answer to that question, I could make an AGI. But I'm fairly sure, I mean, I can't think of an alternative to the process being and mutating existing things. So you can either change a theory slightly in the hope that the rest of it still works or you can make new combinations between things. That's also the two ways that biological mutations can happen in DNA. You can either have a cosmic ray strikes the DNA and changes one base pair or something, or you can have some error in copying results in a bit of DNA that came from somewhere else being stuck into a particular place in the DNA strand.\\n\\nLulie So this sounds like random variation of one thing rather than a new connection between two things?\\n\\nDavid Deutsch Well, no, the second process is in a way a new connection between two things because it, if there's a whole bit of DNA in a different organism or in a different part of the DNA and it gets copied, as it were, into the wrong place in the evolving organism, and that's a way that evolution can take place. Most instances of that just kill the organism but every so often it either leaves the functionality unchanged or makes it better. So what does that have to do with connecting? If a dog gets a piece of DNA from a lobster then that creates a similarity between dogs and lobsters that didn't exist before. And people can, paleogeneticists or whatever they're called, can look at DNA. Bacteria do this a lot. Higher organisms do it less and less because, I think basically because there's less and less chance of you surviving such a thing. For it to be part of evolution, you've got to be able to survive the new adaptation being slightly there, and a bit more there, and a bit more there, because giving a dog lobster claws wouldn't work, because the machinery for controlling the claws and deciding when to use them and so on hasn't been transferred. It would have to evolve. The point is that human new explanations evolve intentionally.\\n\\nThey involve randomness at some level, but the business part of creating the new explanation is what happens to the randomness after it's generated and it is changed and further changed intentionally to solve a problem and that is what induction can't possibly do but we don't know what\\n\\nLulie can do it. Induction can't make a change to solve a problem yes but evolution both genetic and evolution of ideas in the mind can?\\n\\nDavid Deutsch Well, no, it can... so every new genetic sequence is produced first and tried out later. Marxism thinks that an organism's environment and its own actions can cause a change in its genome. And that can't happen, and it couldn't happen because it's the same as induction, which also can't happen. So is this also true within a mind? Like you have to make up something before you can tell\\n\\nLulie whether it solves the problem or not?\\n\\nDavid Deutsch Yes, but there's a difference, which is that the making up process isn't purely random.\\n\\n5\\n\\nHow so?\\n\\nDavid Deutsch For example, a human can do the thing which I just said the dog and the lobster can't do. can think of the solar neutrino problem and can think, could it be that there's a new particle that isn't even a neutrino? And what would that particle... Now that in itself is not the explanation, but it's the germ of an explanation be like made up before you can check whether that's a good question? Yes, but it that making up that involves much less random trial and error than it would that it would take to try all possible variations randomly. So the fact that a person can think of an analogy, you know, some people think that human thinking is all about analogies. So we can take a whole idea from some other place and see if it fits in this place, which it never does immediately, but then you can say, well, how can we change it further so it does fit? So a person who thought, well, maybe there's a wholly new particle involved, at some point that person may say, maybe that whole new particle is just a neutrino, but of a different type. And then they're well on the way to solving. Of course there are many other considerations, but when I say many, it's nothing compared with how many you'd have to check\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" of an algorithm that was programmed in by somebody who knew what the transformation between the inputs and outputs ought to be. Namely, when you press a certain button it multiplies and so on. AI's, modern AI's, essentially construct their algorithm themselves by generalizing a large amount of data. In early ones, this led to kind of embarrassing glitches, like when they identified a heap of rifles as a cat, because somebody worked out what they were doing and how to fool them. But the modern ones use so much data and have been honed by humans so well that they rarely do this. Although I've recently been playing around with the chat GPTs and I find that if you ask it some questions off the beaten track, you can quite easily cause it to do all the old things of either either saying nonsense, contradicting itself, committing howlers, where it says the opposite of known facts and so on. And that's different from human knowledge which is explanatory.\\n\\nNeither the calculator nor the chatbot ever produces a new explanation. You can ask it for an explanation but all it's doing is distilling explanations that already exist and that's the thing that it doesn't do very well either.\\n\\nLulie What is the difference between an explanation and the thing that it does produce?\\n\\nDavid Deutsch If we knew the detailed answer to that we'd know how to make an AGI. basically an explanation accounts for what it's trying to account for, like a physical process or the reason for something, it accounts for that, it accounts for a known thing that is trying to explain in terms of the unseen, unknown reasons behind it, which usually cannot even be seen even in So Brett Hall's favourite example is that we can never see the centre of the sun. We could never go there, any instrument that we send would get destroyed long before it got to the centre of the sun. So the centre of the sun can't be in GPT's training data? As it were, yes, exactly. And the only thing that can be in GPT's training data is what is seen about the Sun, namely its surface.\\n\\nLulie Couldn't GPT derive things about the Sun based on other theories that we have?\\n\\nDavid Deutsch It can deduce things from existing theories, yes. So if you ask it about the centre of the Sun, it will find some existing theory of the Sun and make a deduction from that. So that's not induction, that's deduction. And on the other hand, if there was a mystery about the Sun, like a couple of decades ago...\\n\\nLulie Isn't it induction via deduction? So the induction was all of the training data,\\n\\nDavid Deutsch and then the deduction is taking that data and then forming theories about it? No, because it didn't induce the data. It distilled it into a more compact form and then it can deduce things from that, but those things are only ever as good as the original theories were, probably slightly worse because by compressing the data it slightly some cases it degraded it a lot. A few decades ago, I was going to say recently, but in fact not so recent, it was when I was a graduate student, the big problem in astrophysics was that the sun wasn't producing enough neutrinos. Enough for what?\\n\\n3\\n\\nEnough.\\n\\nDavid Deutsch For your breakfast cereal? producing as many as the theory predicted. And this theory was extremely robust because it was also the theory that we used to predict the sun's brightness. And it predicted the sun's brightness extremely well, and also the brightness of other stars and how they change with time and So when they first made neutrino observatories with rather crude neutrino detectors, first they didn't find neutrinos but then they found a few but nowhere near enough. And when they refined it they found that there were only a third as many neutrinos as as predicted by the theory. And so there were all sorts of explanatory theories proposed, which couldn't possibly have been induced from anything, because all the data said was there are too few neutrinos. And of course you can always say that neutrinos have been eaten by a space monster, but generally when we produce scientific theories, we don't just want a new explanation, we want a new explanation that doesn't upset old explanations, that doesn't make them into nonsense, like the space monster theory. So you asked me what the difference is between an explanation and just a predictive theory. And by the way, there is no such thing as a purely predictive theory. They all have some kind of explanation and when you're quote inducing things from data you're always using an old explanation and just piling on some tweaks which don't have an explanation in order to make your supposed new theory.\\n\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\", but then you can say, well, how can we change it further so it does fit? So a person who thought, well, maybe there's a wholly new particle involved, at some point that person may say, maybe that whole new particle is just a neutrino, but of a different type. And then they're well on the way to solving. Of course there are many other considerations, but when I say many, it's nothing compared with how many you'd have to check through by random variation, or by trying every possibility or whatever.\\n\\nLulie Why is it not the case that you need to do this random thing by producing the idea first and checking it afterwards?\\n\\nDavid Deutsch Or are you saying that you can do that? Human minds produce conjectures, raw conjectures, much more efficiently than any either systematic search or random search.\\n\\nLulie And we don't know why?\\n\\n4\\n\\nWe don't know how.\\n\\n3 Okay, we don't know how.\\n\\nDavid Deutsch So, for example, chess playing engines have to search through billions of times more possibilities than chess grandmasters do.\\n\\nLulie\\n\\nWell, they apparently do?\\n\\nDavid Deutsch No, it's impossible that... well, unless there's something in the hardware of the human brain that we don't know about.\\n\\nLulie What if it's like super parallel?\\n\\nDavid Deutsch Yeah, well, it would have to go through billions of times more processing than we think it can, which would mean it would have to be billions of times more efficient thermodynamically than we think it is. I mean, we don't know. We don't know how the brain works either, let alone...\\n\\nLulie Do we know this for biological evolution, as in, do we know how they, like, biological evolution is as efficient as it is? Because I thought that we couldn't very well program biological evolution.\\n\\nDavid Deutsch Yeah, we can't, but, and that, the analogue of that program, that, of that problem does exist but it's not as severe a problem as it is for thinking. Although computer simulations of biological evolution aren't very good, they're not complete rubbish. They do sort of mimic evolution a bit and I think it's a mystery, you know, why real evolution is that much better. But I don't think that real evolution is that much better by that much. It's not a factor of billions. It's, you know, there's something missing that makes it chug along rather than efficiently. I don't think it's the same problem. Although it is the same in one respect. People who do biological evolution, simulate biological evolution on computers, seem blind to this problem, seem to me to be blind to this problem in the same way that people who are trying to make AGI out of AIs are blind to the difference between those two. Because they have the wrong epistemology? Yeah, but I don't know what the answer is. I know one misconception they have in the case of AGI, which by itself makes it impossible for them to create an AGI.\\n\\nLulie Namely the thing about prediction.\\n\\nDavid Deutsch Namely the thing about induction being impossible. With biological evolution, I don't know what it would take to make an analog of biological evolution on a computer. Of course I'm sure it can be done. My guess is that people will do it and it will be relatively simple to do once someone has had the idea of what biology does. There are various apparently indicative things in biology of the same biological structure occurring in evolutionarily very distant organisms. I think the famous one is that there's a gene involved in the development of the eye, which is the same gene is found in different eyes that work by completely different physical principles. So it's not that they have a common origin, unless the common origin is something so deep in history that we don't recognize it as being eyes. Convergent evolution? Yes, but it's convergent evolution without an apparent reason. Is that different from convergent evolution? Yeah, yeah. So convergent evolution is that things in the same environment tend to end up with the same appearance, the same lifestyle, and so on.\\n\\nLulie works by things like empiricism, induction, Bayesianism, Bayesian epistemology. And so AI, so first I guess my first question is AI, current AI, following that?\\n\\nDavid Deutsch Current AI was inspired by that but that's not what it's doing. It's not doing induction any more than anything else is. Induction is impossible. Also, current AI was also inspired by the architecture of the brain, neural nets. There are these programming hardware\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" for practical purposes that's the same thing.\\n\\nLulie When you make a new connection between two things, does that then create a new third thing which you can then make connections between? Is it actually true that the theory of knowledge works by making connections between things? Would you say that's an accurate representation?\\n\\nDavid Deutsch Well, again, if I knew the exact answer to that question, I could make an AGI. But I'm fairly sure, I mean, I can't think of an alternative to the process being and mutating existing things. So you can either change a theory slightly in the hope that the rest of it still works or you can make new combinations between things. That's also the two ways that biological mutations can happen in DNA. You can either have a cosmic ray strikes the DNA and changes one base pair or something, or you can have some error in copying results in a bit of DNA that came from somewhere else being stuck into a particular place in the DNA strand.\\n\\nLulie So this sounds like random variation of one thing rather than a new connection between two things?\\n\\nDavid Deutsch Well, no, the second process is in a way a new connection between two things because it, if there's a whole bit of DNA in a different organism or in a different part of the DNA and it gets copied, as it were, into the wrong place in the evolving organism, and that's a way that evolution can take place. Most instances of that just kill the organism but every so often it either leaves the functionality unchanged or makes it better. So what does that have to do with connecting? If a dog gets a piece of DNA from a lobster then that creates a similarity between dogs and lobsters that didn't exist before. And people can, paleogeneticists or whatever they're called, can look at DNA. Bacteria do this a lot. Higher organisms do it less and less because, I think basically because there's less and less chance of you surviving such a thing. For it to be part of evolution, you've got to be able to survive the new adaptation being slightly there, and a bit more there, and a bit more there, because giving a dog lobster claws wouldn't work, because the machinery for controlling the claws and deciding when to use them and so on hasn't been transferred. It would have to evolve. The point is that human new explanations evolve intentionally.\\n\\nThey involve randomness at some level, but the business part of creating the new explanation is what happens to the randomness after it's generated and it is changed and further changed intentionally to solve a problem and that is what induction can't possibly do but we don't know what\\n\\nLulie can do it. Induction can't make a change to solve a problem yes but evolution both genetic and evolution of ideas in the mind can?\\n\\nDavid Deutsch Well, no, it can... so every new genetic sequence is produced first and tried out later. Marxism thinks that an organism's environment and its own actions can cause a change in its genome. And that can't happen, and it couldn't happen because it's the same as induction, which also can't happen. So is this also true within a mind? Like you have to make up something before you can tell\\n\\nLulie whether it solves the problem or not?\\n\\nDavid Deutsch Yes, but there's a difference, which is that the making up process isn't purely random.\\n\\n5\\n\\nHow so?\\n\\nDavid Deutsch For example, a human can do the thing which I just said the dog and the lobster can't do. can think of the solar neutrino problem and can think, could it be that there's a new particle that isn't even a neutrino? And what would that particle... Now that in itself is not the explanation, but it's the germ of an explanation be like made up before you can check whether that's a good question? Yes, but it that making up that involves much less random trial and error than it would that it would take to try all possible variations randomly. So the fact that a person can think of an analogy, you know, some people think that human thinking is all about analogies. So we can take a whole idea from some other place and see if it fits in this place, which it never does immediately, but then you can say, well, how can we change it further so it does fit? So a person who thought, well, maybe there's a wholly new particle involved, at some point that person may say, maybe that whole new particle is just a neutrino, but of a different type. And then they're well on the way to solving. Of course there are many other considerations, but when I say many, it's nothing compared with how many you'd have to check\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\", but then you can say, well, how can we change it further so it does fit? So a person who thought, well, maybe there's a wholly new particle involved, at some point that person may say, maybe that whole new particle is just a neutrino, but of a different type. And then they're well on the way to solving. Of course there are many other considerations, but when I say many, it's nothing compared with how many you'd have to check through by random variation, or by trying every possibility or whatever.\\n\\nLulie Why is it not the case that you need to do this random thing by producing the idea first and checking it afterwards?\\n\\nDavid Deutsch Or are you saying that you can do that? Human minds produce conjectures, raw conjectures, much more efficiently than any either systematic search or random search.\\n\\nLulie And we don't know why?\\n\\n4\\n\\nWe don't know how.\\n\\n3 Okay, we don't know how.\\n\\nDavid Deutsch So, for example, chess playing engines have to search through billions of times more possibilities than chess grandmasters do.\\n\\nLulie\\n\\nWell, they apparently do?\\n\\nDavid Deutsch No, it's impossible that... well, unless there's something in the hardware of the human brain that we don't know about.\\n\\nLulie What if it's like super parallel?\\n\\nDavid Deutsch Yeah, well, it would have to go through billions of times more processing than we think it can, which would mean it would have to be billions of times more efficient thermodynamically than we think it is. I mean, we don't know. We don't know how the brain works either, let alone...\\n\\nLulie Do we know this for biological evolution, as in, do we know how they, like, biological evolution is as efficient as it is? Because I thought that we couldn't very well program biological evolution.\\n\\nDavid Deutsch Yeah, we can't, but, and that, the analogue of that program, that, of that problem does exist but it's not as severe a problem as it is for thinking. Although computer simulations of biological evolution aren't very good, they're not complete rubbish. They do sort of mimic evolution a bit and I think it's a mystery, you know, why real evolution is that much better. But I don't think that real evolution is that much better by that much. It's not a factor of billions. It's, you know, there's something missing that makes it chug along rather than efficiently. I don't think it's the same problem. Although it is the same in one respect. People who do biological evolution, simulate biological evolution on computers, seem blind to this problem, seem to me to be blind to this problem in the same way that people who are trying to make AGI out of AIs are blind to the difference between those two. Because they have the wrong epistemology? Yeah, but I don't know what the answer is. I know one misconception they have in the case of AGI, which by itself makes it impossible for them to create an AGI.\\n\\nLulie Namely the thing about prediction.\\n\\nDavid Deutsch Namely the thing about induction being impossible. With biological evolution, I don't know what it would take to make an analog of biological evolution on a computer. Of course I'm sure it can be done. My guess is that people will do it and it will be relatively simple to do once someone has had the idea of what biology does. There are various apparently indicative things in biology of the same biological structure occurring in evolutionarily very distant organisms. I think the famous one is that there's a gene involved in the development of the eye, which is the same gene is found in different eyes that work by completely different physical principles. So it's not that they have a common origin, unless the common origin is something so deep in history that we don't recognize it as being eyes. Convergent evolution? Yes, but it's convergent evolution without an apparent reason. Is that different from convergent evolution? Yeah, yeah. So convergent evolution is that things in the same environment tend to end up with the same appearance, the same lifestyle, and so on.\\n\\nLulie works by things like empiricism, induction, Bayesianism, Bayesian epistemology. And so AI, so first I guess my first question is AI, current AI, following that?\\n\\nDavid Deutsch Current AI was inspired by that but that's not what it's doing. It's not doing induction any more than anything else is. Induction is impossible. Also, current AI was also inspired by the architecture of the brain, neural nets. There are these programming hardware\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" of an algorithm that was programmed in by somebody who knew what the transformation between the inputs and outputs ought to be. Namely, when you press a certain button it multiplies and so on. AI's, modern AI's, essentially construct their algorithm themselves by generalizing a large amount of data. In early ones, this led to kind of embarrassing glitches, like when they identified a heap of rifles as a cat, because somebody worked out what they were doing and how to fool them. But the modern ones use so much data and have been honed by humans so well that they rarely do this. Although I've recently been playing around with the chat GPTs and I find that if you ask it some questions off the beaten track, you can quite easily cause it to do all the old things of either either saying nonsense, contradicting itself, committing howlers, where it says the opposite of known facts and so on. And that's different from human knowledge which is explanatory.\\n\\nNeither the calculator nor the chatbot ever produces a new explanation. You can ask it for an explanation but all it's doing is distilling explanations that already exist and that's the thing that it doesn't do very well either.\\n\\nLulie What is the difference between an explanation and the thing that it does produce?\\n\\nDavid Deutsch If we knew the detailed answer to that we'd know how to make an AGI. basically an explanation accounts for what it's trying to account for, like a physical process or the reason for something, it accounts for that, it accounts for a known thing that is trying to explain in terms of the unseen, unknown reasons behind it, which usually cannot even be seen even in So Brett Hall's favourite example is that we can never see the centre of the sun. We could never go there, any instrument that we send would get destroyed long before it got to the centre of the sun. So the centre of the sun can't be in GPT's training data? As it were, yes, exactly. And the only thing that can be in GPT's training data is what is seen about the Sun, namely its surface.\\n\\nLulie Couldn't GPT derive things about the Sun based on other theories that we have?\\n\\nDavid Deutsch It can deduce things from existing theories, yes. So if you ask it about the centre of the Sun, it will find some existing theory of the Sun and make a deduction from that. So that's not induction, that's deduction. And on the other hand, if there was a mystery about the Sun, like a couple of decades ago...\\n\\nLulie Isn't it induction via deduction? So the induction was all of the training data,\\n\\nDavid Deutsch and then the deduction is taking that data and then forming theories about it? No, because it didn't induce the data. It distilled it into a more compact form and then it can deduce things from that, but those things are only ever as good as the original theories were, probably slightly worse because by compressing the data it slightly some cases it degraded it a lot. A few decades ago, I was going to say recently, but in fact not so recent, it was when I was a graduate student, the big problem in astrophysics was that the sun wasn't producing enough neutrinos. Enough for what?\\n\\n3\\n\\nEnough.\\n\\nDavid Deutsch For your breakfast cereal? producing as many as the theory predicted. And this theory was extremely robust because it was also the theory that we used to predict the sun's brightness. And it predicted the sun's brightness extremely well, and also the brightness of other stars and how they change with time and So when they first made neutrino observatories with rather crude neutrino detectors, first they didn't find neutrinos but then they found a few but nowhere near enough. And when they refined it they found that there were only a third as many neutrinos as as predicted by the theory. And so there were all sorts of explanatory theories proposed, which couldn't possibly have been induced from anything, because all the data said was there are too few neutrinos. And of course you can always say that neutrinos have been eaten by a space monster, but generally when we produce scientific theories, we don't just want a new explanation, we want a new explanation that doesn't upset old explanations, that doesn't make them into nonsense, like the space monster theory. So you asked me what the difference is between an explanation and just a predictive theory. And by the way, there is no such thing as a purely predictive theory. They all have some kind of explanation and when you're quote inducing things from data you're always using an old explanation and just piling on some tweaks which don't have an explanation in order to make your supposed new theory.\\n\\n\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" So if creativity is something that doesn't exist in increments, it's or you know the capacity to create explanations, you can just use a simple example go on YouTube and look up cat opening a door right so you'll see for example, a cat develops a theory that applying torque to this handle to this metal thing will open door. Now it hasn't. And then what it'll do is it'll climb onto a countertop and it'll jump on top of that door handle. It hasn't seen another cat to it it hasn't seen another human like get on a countertop and try to open the door that way. But it conjectures that this is a way, given its morphology, that it can access the door. And then, you know, so that's the theory. And then the experiment is, will the door open? This seems like a classic cycle of conjecture and reputation. Is this compatible with the cats not being, at least having some bounded form of creativity?\\n\\n1 0:27:57 So, animals are amazing things and instinctive animal knowledge is designed to make animals easily capable of thriving in environments that they've never seen before. for avoiding each tree, and not only that, for actually catching the rabbit that it's running after as well, in a way that has never been done before. So, the way to understand this, I think, now this is because of a vast amount of knowledge that is in the wolf's genes. What kind of knowledge is this? Well, it's not the kind of knowledge that says first turn left, then turn right, then jump, and so on. It's not that kind of instruction. It's instruction that takes input from the outside and then generates a behavior that is relevant to that input. sophistication in the program that human robotics has not yet reached anywhere near that. And by the way, then when it sees a wolf of the opposite sex, it may decide to leave the rabbit and go and have sex instead. And a program for a robot to locate another robot of the right species and then have sex with it is again, I think, beyond present day robotics. But it will be done. And it does not, it clearly does not require creativity because that same program will lead the next wolf to do the same thing in the same circumstances. It's the fact that the circumstances are ones that is never seen before and it can still function, is a testimony to the incredible sophistication of that program, but it has nothing to do with creativity.\\n\\nSo, humans do tasks that require much, much less programming sophistication than that, such as sitting around a campfire, telling each other a scary story about a wolf that almost ate them. Now, animals can do the wolf running away thing. They can enact a story that's more complicated even than the one the human is telling, but they can't tell a story. They don't tell a story. Telling a story is a sort of typical creative activity. It's the same kind of activity as forming an explanation. So I don't think it's at all surprising that cats can jump on handles, because it's the same. I can easily imagine that the same amazingly sophisticated program that lets it jump on a branch so that the branch will get out of its way in some sense, will also function in this new environment that's never seen before. But there are all sorts of other things that it can't do.\\n\\n2 0:31:55 Oh, that's definitely true, which was my point, is that it has a bounded form of creativity, and if bounded forms of creativity can exist, then humans could be in one such, but I'm having a hard time imagining the ancestral circumstance in which a cat couldn't would have genetic gain the genetic knowledge that jumping on a metal rod would get a wooden plank to open\\n\\n1 0:32:17 and give it access to the you know the other side. Well, I thought I just gave an example. I mean, if we don't know, at least I don't know what kind of environment the ancestor of the domestic cat lived in. But if it was for example if it contained undergrowth, then dealing with undergrowth requires some very sophisticated programs otherwise you will just get stuck somewhere and starve to death. Now, I think a dog, if it gets stuck in a bush, it has no program to get out other than to shake itself about until it gets out. It doesn't have a concept of doing something which temporarily makes matters worse and then allows you to get\\n\\n2 0:33:10 I think dogs can't do that. But it's just, it's not because that's a particularly complicated thing, it's just that it's programming just doesn't have that. But an animal's programming easily could have that if it lived in an environment in which that happened a lot of power. So\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\", but then you can say, well, how can we change it further so it does fit? So a person who thought, well, maybe there's a wholly new particle involved, at some point that person may say, maybe that whole new particle is just a neutrino, but of a different type. And then they're well on the way to solving. Of course there are many other considerations, but when I say many, it's nothing compared with how many you'd have to check through by random variation, or by trying every possibility or whatever.\\n\\nLulie Why is it not the case that you need to do this random thing by producing the idea first and checking it afterwards?\\n\\nDavid Deutsch Or are you saying that you can do that? Human minds produce conjectures, raw conjectures, much more efficiently than any either systematic search or random search.\\n\\nLulie And we don't know why?\\n\\n4\\n\\nWe don't know how.\\n\\n3 Okay, we don't know how.\\n\\nDavid Deutsch So, for example, chess playing engines have to search through billions of times more possibilities than chess grandmasters do.\\n\\nLulie\\n\\nWell, they apparently do?\\n\\nDavid Deutsch No, it's impossible that... well, unless there's something in the hardware of the human brain that we don't know about.\\n\\nLulie What if it's like super parallel?\\n\\nDavid Deutsch Yeah, well, it would have to go through billions of times more processing than we think it can, which would mean it would have to be billions of times more efficient thermodynamically than we think it is. I mean, we don't know. We don't know how the brain works either, let alone...\\n\\nLulie Do we know this for biological evolution, as in, do we know how they, like, biological evolution is as efficient as it is? Because I thought that we couldn't very well program biological evolution.\\n\\nDavid Deutsch Yeah, we can't, but, and that, the analogue of that program, that, of that problem does exist but it's not as severe a problem as it is for thinking. Although computer simulations of biological evolution aren't very good, they're not complete rubbish. They do sort of mimic evolution a bit and I think it's a mystery, you know, why real evolution is that much better. But I don't think that real evolution is that much better by that much. It's not a factor of billions. It's, you know, there's something missing that makes it chug along rather than efficiently. I don't think it's the same problem. Although it is the same in one respect. People who do biological evolution, simulate biological evolution on computers, seem blind to this problem, seem to me to be blind to this problem in the same way that people who are trying to make AGI out of AIs are blind to the difference between those two. Because they have the wrong epistemology? Yeah, but I don't know what the answer is. I know one misconception they have in the case of AGI, which by itself makes it impossible for them to create an AGI.\\n\\nLulie Namely the thing about prediction.\\n\\nDavid Deutsch Namely the thing about induction being impossible. With biological evolution, I don't know what it would take to make an analog of biological evolution on a computer. Of course I'm sure it can be done. My guess is that people will do it and it will be relatively simple to do once someone has had the idea of what biology does. There are various apparently indicative things in biology of the same biological structure occurring in evolutionarily very distant organisms. I think the famous one is that there's a gene involved in the development of the eye, which is the same gene is found in different eyes that work by completely different physical principles. So it's not that they have a common origin, unless the common origin is something so deep in history that we don't recognize it as being eyes. Convergent evolution? Yes, but it's convergent evolution without an apparent reason. Is that different from convergent evolution? Yeah, yeah. So convergent evolution is that things in the same environment tend to end up with the same appearance, the same lifestyle, and so on.\\n\\nLulie works by things like empiricism, induction, Bayesianism, Bayesian epistemology. And so AI, so first I guess my first question is AI, current AI, following that?\\n\\nDavid Deutsch Current AI was inspired by that but that's not what it's doing. It's not doing induction any more than anything else is. Induction is impossible. Also, current AI was also inspired by the architecture of the brain, neural nets. There are these programming hardware\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" our knowledge of, of grammar is my was my favorite example, as we know why certain sentences are acceptable and why they're unacceptable, but we can't state explicitly, or in every case, why it isn't or why it is. And then there's explicit and inexplicit knowledge, there's conscious and unconscious knowledge. All those are bits of program in the brain, they're ideas, they are bits of knowledge in this, if you define knowledge as information with causal power, they are all information with causal power. They all contain truth and they all contain error and it's always a mistake to shield something, to shield one of them from criticism or replacement.\\n\\nNot doing that is what I call the fun criterion. Now you might say that's a bad name, but you know, it's the best I can find.\\n\\n2 1:14:11 So why would creating an AGI through evolution necessarily entail suffering? Because the way I see it, or it seems to me your theory is that you need to be a general intelligence in order to feel suffering, but by the point an evolved, simulated being is a general intelligence, we can just stop the simulation.\\n\\n1 1:14:31 And so where is the suffering coming from? Okay, so the kind of simulation by evolution that I'm thinking of, I mean, there may be several kinds, but the kind that I'm thinking of, and which I said would be the greatest crime in history, is the kind that just simulates the actual evolution of humans from pre humans that weren't people. So you have you have a population of non people which in this simulation would be some kind of NPCs. And then they would they would just evolve we don't know what the criterion would be we just have an artificial universe which simulated the surface of the earth and they'd be walking around and some of them might or might not become people. And now the thing is when you're part of the way there, what is happening is that you have the way that I, the only way that I can imagine the evolution of personhood or create the explanatory creativity happened was that the hardware needed for it was first needed for something else. I have proposed that it was needed to transmit memes. So there'd be people who were transmitting memes creatively but they were running out of resources before it managed to increase their stock of memes. So in every generation, there was a stock of memes that was being passed down to the next generation. And once they got beyond a certain complexity, they had to be passed down by the use of creativity by the recipient.\\n\\nSo, there may well have been a time and as I say I can't think of any other way it could have been, where there was genuine creativity being used, but it ran out of resources, very quickly, but not so quickly that it didn't increase the mean bandwidth. Then in the next generation, there was more mean bandwidth, and then after, you know, certain number of generations, there would have been some opportunity to use this hardware or whatever it is, you know, firmware, I expect, to use this firmware for something other than just trying blindly transmitting memes or rather creatively transmitting memes, but they were blind memes. So in that time, it would have been very unpleasant to be alive. It was already very unpleasant to, sorry, it was very unpleasant to be alive when we did have enough resources to think as well as do the memes. But I don't think there would have been a moment at which you would say, yes, now the suffering begins to matter because it's not just blind memes. I think the people were already suffering at the time when they were blindly transmitting memes because they were using genuine creativity. They were just not using it to any good effect.\\n\\n2 1:17:56 Gotcha. Would being in the experience machine be compatible with the fun criterion so you're not aware that you're in, you know, the experience machine it's all virtual reality, but you're still doing the things that would make you have fun in fact more so than in the real world.\\n\\n1 1:18:15 So, would you be tempted to get into experience machine, what do we can put a compatible with the fun criteria, I mean, is it just a virtual reality world in which things work better than in the real world or something?\\n\\n2 1:18:35 Yeah, so it's a thought experiment by Robert Nozick, and the idea is that you would enter this world, but you would forget that you're in virtual reality. So all, I mean, the world would be perfect in every possible way that it could be perfect or not perfect, but it would be better in every possible way it could be better. But you would think the relationships you have here are real, the knowledge you're discovering here is novel\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\"ulie works by things like empiricism, induction, Bayesianism, Bayesian epistemology. And so AI, so first I guess my first question is AI, current AI, following that?\\n\\nDavid Deutsch Current AI was inspired by that but that's not what it's doing. It's not doing induction any more than anything else is. Induction is impossible. Also, current AI was also inspired by the architecture of the brain, neural nets. There are these programming hardware, computer hardware devices that are modelled on how neurons work. Now I think that's a coincidence. I mean it's possible that the neuron architecture makes things like pattern recognition and extrapolation and so on a bit more efficient. But because of computational universality we know that that can't be fundamental and in fact, you know, you can download a neural net based computer program onto your home computer, which doesn't have a neural net in it, and it'll still work, even though it's a bit slower.\\n\\nLulie If AGI cannot come from AI, what would create AGI in your view?\\n\\nDavid Deutsch I can only say very little about that. From Popper's epistemology we can infer a few things about what it must look like, but far from enough to make one. So one thing is that there cannot be a specification of a program for AGI in the sense of saying what properties its output must have either for a given input, because, for example, an AGI may choose not to answer. It may choose never to answer, it might choose to become a hermit. Are you saying that current views of AGI are all about the output? Yes, they're all about either the output itself or more often how the output must be related to the input. So if we can't judge an AGI based on the output, how can we judge it? Yes, we can't judge an AGI or a human. There can't be a reliable test of whether a human is thinking. What about the Turing test is something that has been invented after Turing. It's been based on a misconception about passage in Turing's 1950 paper called, the paper was called Can Machines Think? and unlike most titles which are questions, the answer was yes rather than no. He included a section on a thing called the imitation game, he called it the imitation game, where an AGI, he just assumes it is an AGI, is pretending to be a human, and he is saying, suppose it could pretend to be a human sufficiently well for the sceptics who think that AGI isn't possible, not to be able to tell the difference between it and an actual human. What would happen? What would these skeptics say about that?\\n\\nWell, if they said, well, it's still not thinking, it just seems to be, then they're vulnerable to the criticism, but that is all the information you have about whether a human is thinking.\\n\\nLulie Wait, so what, so, sorry, why is the test not working or, I didn't, I don't, I didn't\\n\\nDavid Deutsch quite follow. It's not saying that something that can pass this test is necessarily an AGI, or that something that can't pass the test is, necessarily isn't an AGI. That's not what this game is for. What's it for? It's for persuading people that machines could think. How does it do that? By imagining a computer program that could fool people into thinking it was a person. There must be such a program because of computational universality. Ah, it was an argument about universality. Well it just assumed universality. Turing had proved the existence of universality, conjectured, but basically proved the existence of universality 14 years earlier, in 1936, and he was just thinking of that among many consequences of computation, which was a fairly new concept at the time. So I still don't get what the thought experiment intended to persuade the reader, in case the reader was sceptical that a machine can think.\\n\\nLulie So it did that by imagining a situation in which there is a computer and the computer and so a human can have a conversation with another human and that's fine and that would be fairly persuasive that I'm talking to a person and because computers can produce any output you can imagine a computer that produces exactly the same output as that hypothetical human.\\n\\nDavid Deutsch So his imitation game started out with that as a premise that such a computer can exist, that such a computer program can exist because of universality. And then he imagined playing this game in which you have the computer and a human both talking at long distance with the sceptic, the human sceptic who thinks that machines can't think. So the sceptic would then be unable to tell the difference\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" can imagine a computer that produces exactly the same output as that hypothetical human.\\n\\nDavid Deutsch So his imitation game started out with that as a premise that such a computer can exist, that such a computer program can exist because of universality. And then he imagined playing this game in which you have the computer and a human both talking at long distance with the sceptic, the human sceptic who thinks that machines can't think. So the sceptic would then be unable to tell the difference. But then it's not that the machine would think, it's just producing the same output as a human\\n\\nLulie would. Yes. So shouldn't that thought experiment not convince us?\\n\\nDavid Deutsch Well, it's not a proof, but it's an intuition pump.\\n\\nLulie Yeah, but doesn't it pump it in the other direction?\\n\\nDavid Deutsch No, because the sceptic has some way of judging. that the skeptic thinks that the computer can't think and the human can. The evidence that a skeptic has of that is that the skeptic has spoken to humans and can easily tell whether something is a human or not in everyday life. He can tell, you know, in those days there were things like speak your weight machines or horoscope machines which tell you the horoscope and you can easily show you can easily judge that those are not people but there must be a computer program that can produce the same output that convinced you that an actual person person. Now you could, so there are many, following on from this, there were many further arguments by people who still tried to be sceptics and tried to deny that machines can think and for example, the famous example is the theory of philosophical zombies, which is a philosophical zombie is an entity that produces the same output as a human and is indistinguishable but nevertheless hasn't got any consciousness or qualia or anything like that and is just a zombie.\\n\\nThen there was Searle's Chinese room which is about a room with exponentially large number of books of responses to questions posed in Chinese and it's internally run by a person who can't speak Chinese but he has to look up in his books and then the intuition he's trying to counter Turing's argument with is that this room together with its inhabitant can't think and therefore the fact that Turing's imaginary computer can produce the right output doesn't prove that it can think. But Searle doesn't have a theory of what thinking is, he just has this counter-argument which is basically the same as the zombie argument. I know from prior conversations that you think that it will be obvious when we actually have AI? And so did Turing.\\n\\nLulie But it seems like right now you're saying that there is no test for it, so how could it be obvious if you can't even make a test for it?\\n\\nDavid Deutsch Well, there are plenty of things that are obvious that we haven't got tests for, such as the fact that we have qualia. And you know, it's in that same category of things that philosophically we don't know how to do without, but we can't test for it. Another thing is that solipsism isn't true, so there's no test for whether the external world is real or not.\\n\\nLulie\\n\\nThere are arguments though.\\n\\nDavid Deutsch Yeah, so I think Turing's argument still stands up. By the way, in his paper, he included several counter-arguments and countered the counter-arguments. And he was really bending over backwards to be fair to the counter-arguments. So much so that it is rather irritating. I remember, I haven't read this paper for a very long time, but I remember that he spends a really unnecessary amount of time dealing with the argument from either telepathy or spiritualism or something like that. And he actually takes it seriously and says, well it can't be that because so and so. And yeah, so he's... Popper did this too. Popper always gave far too much attention to bad arguments, first making them better and then answering them and so on and thus his own arguments are too long and people get tired of reading it and so sometimes Popper's own message gets lost.\\n\\nLulie So you say that AGI cannot have any test for it. So that's one of the things that is different from you compared with the mainstream view of this. And so then how do we know when we get AGI? Like how do we know what kind of\\n\\nDavid Deutsch paths would work? Basically we know from theory. We know from the theory of how it works that it works. So, for example, the simple thing of it must be possible for it to just stop producing output. You might be able to prove that mathematically from\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" can imagine a computer that produces exactly the same output as that hypothetical human.\\n\\nDavid Deutsch So his imitation game started out with that as a premise that such a computer can exist, that such a computer program can exist because of universality. And then he imagined playing this game in which you have the computer and a human both talking at long distance with the sceptic, the human sceptic who thinks that machines can't think. So the sceptic would then be unable to tell the difference. But then it's not that the machine would think, it's just producing the same output as a human\\n\\nLulie would. Yes. So shouldn't that thought experiment not convince us?\\n\\nDavid Deutsch Well, it's not a proof, but it's an intuition pump.\\n\\nLulie Yeah, but doesn't it pump it in the other direction?\\n\\nDavid Deutsch No, because the sceptic has some way of judging. that the skeptic thinks that the computer can't think and the human can. The evidence that a skeptic has of that is that the skeptic has spoken to humans and can easily tell whether something is a human or not in everyday life. He can tell, you know, in those days there were things like speak your weight machines or horoscope machines which tell you the horoscope and you can easily show you can easily judge that those are not people but there must be a computer program that can produce the same output that convinced you that an actual person person. Now you could, so there are many, following on from this, there were many further arguments by people who still tried to be sceptics and tried to deny that machines can think and for example, the famous example is the theory of philosophical zombies, which is a philosophical zombie is an entity that produces the same output as a human and is indistinguishable but nevertheless hasn't got any consciousness or qualia or anything like that and is just a zombie.\\n\\nThen there was Searle's Chinese room which is about a room with exponentially large number of books of responses to questions posed in Chinese and it's internally run by a person who can't speak Chinese but he has to look up in his books and then the intuition he's trying to counter Turing's argument with is that this room together with its inhabitant can't think and therefore the fact that Turing's imaginary computer can produce the right output doesn't prove that it can think. But Searle doesn't have a theory of what thinking is, he just has this counter-argument which is basically the same as the zombie argument. I know from prior conversations that you think that it will be obvious when we actually have AI? And so did Turing.\\n\\nLulie But it seems like right now you're saying that there is no test for it, so how could it be obvious if you can't even make a test for it?\\n\\nDavid Deutsch Well, there are plenty of things that are obvious that we haven't got tests for, such as the fact that we have qualia. And you know, it's in that same category of things that philosophically we don't know how to do without, but we can't test for it. Another thing is that solipsism isn't true, so there's no test for whether the external world is real or not.\\n\\nLulie\\n\\nThere are arguments though.\\n\\nDavid Deutsch Yeah, so I think Turing's argument still stands up. By the way, in his paper, he included several counter-arguments and countered the counter-arguments. And he was really bending over backwards to be fair to the counter-arguments. So much so that it is rather irritating. I remember, I haven't read this paper for a very long time, but I remember that he spends a really unnecessary amount of time dealing with the argument from either telepathy or spiritualism or something like that. And he actually takes it seriously and says, well it can't be that because so and so. And yeah, so he's... Popper did this too. Popper always gave far too much attention to bad arguments, first making them better and then answering them and so on and thus his own arguments are too long and people get tired of reading it and so sometimes Popper's own message gets lost.\\n\\nLulie So you say that AGI cannot have any test for it. So that's one of the things that is different from you compared with the mainstream view of this. And so then how do we know when we get AGI? Like how do we know what kind of\\n\\nDavid Deutsch paths would work? Basically we know from theory. We know from the theory of how it works that it works. So, for example, the simple thing of it must be possible for it to just stop producing output. You might be able to prove that mathematically from\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\"ulie works by things like empiricism, induction, Bayesianism, Bayesian epistemology. And so AI, so first I guess my first question is AI, current AI, following that?\\n\\nDavid Deutsch Current AI was inspired by that but that's not what it's doing. It's not doing induction any more than anything else is. Induction is impossible. Also, current AI was also inspired by the architecture of the brain, neural nets. There are these programming hardware, computer hardware devices that are modelled on how neurons work. Now I think that's a coincidence. I mean it's possible that the neuron architecture makes things like pattern recognition and extrapolation and so on a bit more efficient. But because of computational universality we know that that can't be fundamental and in fact, you know, you can download a neural net based computer program onto your home computer, which doesn't have a neural net in it, and it'll still work, even though it's a bit slower.\\n\\nLulie If AGI cannot come from AI, what would create AGI in your view?\\n\\nDavid Deutsch I can only say very little about that. From Popper's epistemology we can infer a few things about what it must look like, but far from enough to make one. So one thing is that there cannot be a specification of a program for AGI in the sense of saying what properties its output must have either for a given input, because, for example, an AGI may choose not to answer. It may choose never to answer, it might choose to become a hermit. Are you saying that current views of AGI are all about the output? Yes, they're all about either the output itself or more often how the output must be related to the input. So if we can't judge an AGI based on the output, how can we judge it? Yes, we can't judge an AGI or a human. There can't be a reliable test of whether a human is thinking. What about the Turing test is something that has been invented after Turing. It's been based on a misconception about passage in Turing's 1950 paper called, the paper was called Can Machines Think? and unlike most titles which are questions, the answer was yes rather than no. He included a section on a thing called the imitation game, he called it the imitation game, where an AGI, he just assumes it is an AGI, is pretending to be a human, and he is saying, suppose it could pretend to be a human sufficiently well for the sceptics who think that AGI isn't possible, not to be able to tell the difference between it and an actual human. What would happen? What would these skeptics say about that?\\n\\nWell, if they said, well, it's still not thinking, it just seems to be, then they're vulnerable to the criticism, but that is all the information you have about whether a human is thinking.\\n\\nLulie Wait, so what, so, sorry, why is the test not working or, I didn't, I don't, I didn't\\n\\nDavid Deutsch quite follow. It's not saying that something that can pass this test is necessarily an AGI, or that something that can't pass the test is, necessarily isn't an AGI. That's not what this game is for. What's it for? It's for persuading people that machines could think. How does it do that? By imagining a computer program that could fool people into thinking it was a person. There must be such a program because of computational universality. Ah, it was an argument about universality. Well it just assumed universality. Turing had proved the existence of universality, conjectured, but basically proved the existence of universality 14 years earlier, in 1936, and he was just thinking of that among many consequences of computation, which was a fairly new concept at the time. So I still don't get what the thought experiment intended to persuade the reader, in case the reader was sceptical that a machine can think.\\n\\nLulie So it did that by imagining a situation in which there is a computer and the computer and so a human can have a conversation with another human and that's fine and that would be fairly persuasive that I'm talking to a person and because computers can produce any output you can imagine a computer that produces exactly the same output as that hypothetical human.\\n\\nDavid Deutsch So his imitation game started out with that as a premise that such a computer can exist, that such a computer program can exist because of universality. And then he imagined playing this game in which you have the computer and a human both talking at long distance with the sceptic, the human sceptic who thinks that machines can't think. So the sceptic would then be unable to tell the difference\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" reality generator could also place thoughts, as well as sense data,\\n\\n1 0:37:24 into the mind? Yes, but that's only because I think that this model is wrong. It's basically the Cartesian theater, as Daniel Dennett puts it, with the stage cleared of all the characters. So, that's pure consciousness without content, as Sam Harris envisages it. But I think that all that's happening there is that you are conscious of this theater, and you're envisaging it as having certain properties, which, by the way, it doesn't have, but that doesn't matter. We can imagine lots of things that don't happen. You know, in fact, you know, that's in a way characterizes what we do all the time. So, one can interpret one's thoughts about this empty stage as being pure, contentless consciousness. But it's not. It has the content of a stage or a space or,\\n\\n2 0:38:40 you know, however you want to envisage it. Okay, and then let's talk about the Turing principle. So this is a term you coined. It's otherwise been called the church touring Deutsch principle. With this principle imply that you could. So, by the way, it states that any universal computer can simulate any physical process with this principle imply that you could simulate the whole of the universe for example in a compact efficient computer that\\n\\n1 0:39:06 was smaller than the universe itself, or is a constraint to physical processes of a task where it was computationally able to do it, but it wouldn't have enough memory or time. So the more memory and time you gave it, the more closely it could simulate the whole universe or anything near the whole universe probably because it, well, if you wanted to simulate itself as well, then there are logical reasons why there are limits to that. But even if you wanted to simulate the whole universe apart from itself, just the sheer size of the universe makes that impossible, even if we discovered ways of encoding information extremely densely like some people have said maybe quantum gravity would allow you know the rest of the universe as well so. from being limited by computational capacity, because it's only when you separate those that you realize what computational universality is. And I think that's universality, like Turing or quantum universality is the most important thing in the theory of computation, because computation doesn't even make sense, unless you have a concept of a universal computer.\\n\\n2 0:41:20 What could falsify your theory that all interesting problems are soluble so I asked this because, as I'm not convinced they're right. But do you have a strong reason for in principle believing that they're wrong.\\n\\n1 0:41:47 No. So, this, this is a bad explanation. So let's say that some people say, for example, that simulating a human brain is impossible. Now, I can't prove this possible. Nobody can prove this possible until they actually do it or unless they have a design for it which they prove will work. So, pending that there is there is no way of proving that, that it's not true that this is a fundamental limitation. is that it could be applied to anything. For example, it could be applied to the theory that you have recently, just a minute ago, been replaced by a humanoid robot, which is going to say for the next few minutes, just a prearranged set of things, and you're no longer a person. I can't believe you figured it out. Yeah, well, that's the first thing you'd say. So there is no way to refute that by experiment, short of actually doing it, short of actually talking to you and so on. So it's the same with all these other things. In order for it to make sense to have a theory that something is impossible, you have to have an explanation for why it is impossible. So we know that, for example, almost all mathematical propositions are undecidable. So that's not because somebody has said, oh maybe we can't decide everything because thinking we could decide everything is hubris. That's not an argument.\\n\\nYou need an actual functional argument to prove that that is so, and then, at being a functional argument in which the steps of the argument makes sense and relate to other things and so on. You can then say well what does this actually mean. Does this mean that maybe we can never understand the laws of physics included an undecidable function, then we would simply write, you know, f of x and f of x and undecidable function. We couldn't evaluate x it would limit our ability to make predictions, but then, lots of our ability to make predictions is totally limited anyway. to understand the properties of the function f and therefore the\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\"ulie works by things like empiricism, induction, Bayesianism, Bayesian epistemology. And so AI, so first I guess my first question is AI, current AI, following that?\\n\\nDavid Deutsch Current AI was inspired by that but that's not what it's doing. It's not doing induction any more than anything else is. Induction is impossible. Also, current AI was also inspired by the architecture of the brain, neural nets. There are these programming hardware, computer hardware devices that are modelled on how neurons work. Now I think that's a coincidence. I mean it's possible that the neuron architecture makes things like pattern recognition and extrapolation and so on a bit more efficient. But because of computational universality we know that that can't be fundamental and in fact, you know, you can download a neural net based computer program onto your home computer, which doesn't have a neural net in it, and it'll still work, even though it's a bit slower.\\n\\nLulie If AGI cannot come from AI, what would create AGI in your view?\\n\\nDavid Deutsch I can only say very little about that. From Popper's epistemology we can infer a few things about what it must look like, but far from enough to make one. So one thing is that there cannot be a specification of a program for AGI in the sense of saying what properties its output must have either for a given input, because, for example, an AGI may choose not to answer. It may choose never to answer, it might choose to become a hermit. Are you saying that current views of AGI are all about the output? Yes, they're all about either the output itself or more often how the output must be related to the input. So if we can't judge an AGI based on the output, how can we judge it? Yes, we can't judge an AGI or a human. There can't be a reliable test of whether a human is thinking. What about the Turing test is something that has been invented after Turing. It's been based on a misconception about passage in Turing's 1950 paper called, the paper was called Can Machines Think? and unlike most titles which are questions, the answer was yes rather than no. He included a section on a thing called the imitation game, he called it the imitation game, where an AGI, he just assumes it is an AGI, is pretending to be a human, and he is saying, suppose it could pretend to be a human sufficiently well for the sceptics who think that AGI isn't possible, not to be able to tell the difference between it and an actual human. What would happen? What would these skeptics say about that?\\n\\nWell, if they said, well, it's still not thinking, it just seems to be, then they're vulnerable to the criticism, but that is all the information you have about whether a human is thinking.\\n\\nLulie Wait, so what, so, sorry, why is the test not working or, I didn't, I don't, I didn't\\n\\nDavid Deutsch quite follow. It's not saying that something that can pass this test is necessarily an AGI, or that something that can't pass the test is, necessarily isn't an AGI. That's not what this game is for. What's it for? It's for persuading people that machines could think. How does it do that? By imagining a computer program that could fool people into thinking it was a person. There must be such a program because of computational universality. Ah, it was an argument about universality. Well it just assumed universality. Turing had proved the existence of universality, conjectured, but basically proved the existence of universality 14 years earlier, in 1936, and he was just thinking of that among many consequences of computation, which was a fairly new concept at the time. So I still don't get what the thought experiment intended to persuade the reader, in case the reader was sceptical that a machine can think.\\n\\nLulie So it did that by imagining a situation in which there is a computer and the computer and so a human can have a conversation with another human and that's fine and that would be fairly persuasive that I'm talking to a person and because computers can produce any output you can imagine a computer that produces exactly the same output as that hypothetical human.\\n\\nDavid Deutsch So his imitation game started out with that as a premise that such a computer can exist, that such a computer program can exist because of universality. And then he imagined playing this game in which you have the computer and a human both talking at long distance with the sceptic, the human sceptic who thinks that machines can't think. So the sceptic would then be unable to tell the difference\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" can imagine a computer that produces exactly the same output as that hypothetical human.\\n\\nDavid Deutsch So his imitation game started out with that as a premise that such a computer can exist, that such a computer program can exist because of universality. And then he imagined playing this game in which you have the computer and a human both talking at long distance with the sceptic, the human sceptic who thinks that machines can't think. So the sceptic would then be unable to tell the difference. But then it's not that the machine would think, it's just producing the same output as a human\\n\\nLulie would. Yes. So shouldn't that thought experiment not convince us?\\n\\nDavid Deutsch Well, it's not a proof, but it's an intuition pump.\\n\\nLulie Yeah, but doesn't it pump it in the other direction?\\n\\nDavid Deutsch No, because the sceptic has some way of judging. that the skeptic thinks that the computer can't think and the human can. The evidence that a skeptic has of that is that the skeptic has spoken to humans and can easily tell whether something is a human or not in everyday life. He can tell, you know, in those days there were things like speak your weight machines or horoscope machines which tell you the horoscope and you can easily show you can easily judge that those are not people but there must be a computer program that can produce the same output that convinced you that an actual person person. Now you could, so there are many, following on from this, there were many further arguments by people who still tried to be sceptics and tried to deny that machines can think and for example, the famous example is the theory of philosophical zombies, which is a philosophical zombie is an entity that produces the same output as a human and is indistinguishable but nevertheless hasn't got any consciousness or qualia or anything like that and is just a zombie.\\n\\nThen there was Searle's Chinese room which is about a room with exponentially large number of books of responses to questions posed in Chinese and it's internally run by a person who can't speak Chinese but he has to look up in his books and then the intuition he's trying to counter Turing's argument with is that this room together with its inhabitant can't think and therefore the fact that Turing's imaginary computer can produce the right output doesn't prove that it can think. But Searle doesn't have a theory of what thinking is, he just has this counter-argument which is basically the same as the zombie argument. I know from prior conversations that you think that it will be obvious when we actually have AI? And so did Turing.\\n\\nLulie But it seems like right now you're saying that there is no test for it, so how could it be obvious if you can't even make a test for it?\\n\\nDavid Deutsch Well, there are plenty of things that are obvious that we haven't got tests for, such as the fact that we have qualia. And you know, it's in that same category of things that philosophically we don't know how to do without, but we can't test for it. Another thing is that solipsism isn't true, so there's no test for whether the external world is real or not.\\n\\nLulie\\n\\nThere are arguments though.\\n\\nDavid Deutsch Yeah, so I think Turing's argument still stands up. By the way, in his paper, he included several counter-arguments and countered the counter-arguments. And he was really bending over backwards to be fair to the counter-arguments. So much so that it is rather irritating. I remember, I haven't read this paper for a very long time, but I remember that he spends a really unnecessary amount of time dealing with the argument from either telepathy or spiritualism or something like that. And he actually takes it seriously and says, well it can't be that because so and so. And yeah, so he's... Popper did this too. Popper always gave far too much attention to bad arguments, first making them better and then answering them and so on and thus his own arguments are too long and people get tired of reading it and so sometimes Popper's own message gets lost.\\n\\nLulie So you say that AGI cannot have any test for it. So that's one of the things that is different from you compared with the mainstream view of this. And so then how do we know when we get AGI? Like how do we know what kind of\\n\\nDavid Deutsch paths would work? Basically we know from theory. We know from the theory of how it works that it works. So, for example, the simple thing of it must be possible for it to just stop producing output. You might be able to prove that mathematically from\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" So that's one of the things that is different from you compared with the mainstream view of this. And so then how do we know when we get AGI? Like how do we know what kind of\\n\\nDavid Deutsch paths would work? Basically we know from theory. We know from the theory of how it works that it works. So, for example, the simple thing of it must be possible for it to just stop producing output. You might be able to prove that mathematically from the program without ever testing. If you ran this program, it might not stop, it might never stop, but you might be able to, from a mathematical specification of the AGI program, you might be able to prove mathematically that it is capable of stopping and not saying anything.\\n\\nLulie But then how do you know what path we need to take to make AGI?\\n\\nDavid Deutsch Well, I don't know, but we need to make something with that property among several others.\\n\\nLulie\\n\\nThat property, namely?\\n\\nDavid Deutsch That one can prove that it is capable of not producing output. We also need to prove that it is capable of not needing input, but still continuing to think.\\n\\nLulie I would be extremely surprised if people work that way. I would have thought that input is needed to keep thinking.\\n\\nDavid Deutsch So imagine a person in a sensory deprivation tank, who has gone into a sensory deprivation tank because they want to be a hermit.\\n\\nLulie They still have their body which is inputs.\\n\\nDavid Deutsch\\n\\nWell, you...\\n\\nLulie They have their heart beating, breathing...\\n\\nDavid Deutsch You could interrupt the nerves that go from... that give them sensations like that.\\n\\nLulie My guess is that if you did that they would stop being able to think.\\n\\nDavid Deutsch I don't see what would stop them.\\n\\nLulie If sensations are needed to think. So for example, in Antonio Damasio's book, Descartes' Error, there's a thing where if you disconnect the emotional centre of the brain, you then become unable to make choices. So, or rather, it takes a very long time, like it takes ten minutes to decide what colour pen to use or\\n\\nDavid Deutsch an hour to decide where to have lunch. So I think these are parochial facts that have nothing to do with how consciousness works or how thinking works. Of course if you put someone in a situation that they didn't want and have no experience of, they're going to be confused and inefficient at coping with that situation. I think a bit of the contrary of that, of those experiments, are in Ramachandran's description of his patients who have brain injuries or brain disorders which gives them wild misconceptions and inability to think, but if the person in question has a sort of philosophical frame of mind, they can eventually learn to think their way around this, just as a person who has lost the face recognition hardware in the brain can learn to recognize faces by doing it the hard way. It may never be as fast as the built-in hardware,\\n\\nLulie but it'll only be slower by a constant factor. I could imagine that if you were somewhat disabled, like you can't feel anything from the neck down, maybe, although I'm very unsure about this, maybe it would be enough to have the inputs from the sensations in your face or in your head or something. But this is also a big topic we\\n\\nDavid Deutsch could have a whole episode about. So... Can I just say one more thing about it? It is perfectly possible for a person to experience sensations that don't come from the body at all, that they're just imagining. And therefore, given universality, I would expect it to be possible to create that state voluntarily.\\n\\nLulie\\n\\nStimulated inputs.\\n\\nDavid Deutsch Yes, but they'd be actual inputs to the thinking part of the brain.\\n\\nLulie\\n\\nOkay.\\n\\nDavid Deutsch I mean, universality is a powerful concept. in multiple ways and you need a really watertight argument to be persuaded of it. Turing had a very nearly watertight argument. I mean, I think it was watertight. Unless Penrose is right, but never mind that.\\n\\nLulie So you'd finish your thought on universality?\\n\\nDavid Deutsch Yeah, well, universality can tell us a lot about how the mind works, but there's still a lot that it can't tell us. What is the fundamental difference between AI and AGI can create new explanations. It can exhibit genuine human-type creativity, in other words, whereas AI can't. possibly an\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\"ulie works by things like empiricism, induction, Bayesianism, Bayesian epistemology. And so AI, so first I guess my first question is AI, current AI, following that?\\n\\nDavid Deutsch Current AI was inspired by that but that's not what it's doing. It's not doing induction any more than anything else is. Induction is impossible. Also, current AI was also inspired by the architecture of the brain, neural nets. There are these programming hardware, computer hardware devices that are modelled on how neurons work. Now I think that's a coincidence. I mean it's possible that the neuron architecture makes things like pattern recognition and extrapolation and so on a bit more efficient. But because of computational universality we know that that can't be fundamental and in fact, you know, you can download a neural net based computer program onto your home computer, which doesn't have a neural net in it, and it'll still work, even though it's a bit slower.\\n\\nLulie If AGI cannot come from AI, what would create AGI in your view?\\n\\nDavid Deutsch I can only say very little about that. From Popper's epistemology we can infer a few things about what it must look like, but far from enough to make one. So one thing is that there cannot be a specification of a program for AGI in the sense of saying what properties its output must have either for a given input, because, for example, an AGI may choose not to answer. It may choose never to answer, it might choose to become a hermit. Are you saying that current views of AGI are all about the output? Yes, they're all about either the output itself or more often how the output must be related to the input. So if we can't judge an AGI based on the output, how can we judge it? Yes, we can't judge an AGI or a human. There can't be a reliable test of whether a human is thinking. What about the Turing test is something that has been invented after Turing. It's been based on a misconception about passage in Turing's 1950 paper called, the paper was called Can Machines Think? and unlike most titles which are questions, the answer was yes rather than no. He included a section on a thing called the imitation game, he called it the imitation game, where an AGI, he just assumes it is an AGI, is pretending to be a human, and he is saying, suppose it could pretend to be a human sufficiently well for the sceptics who think that AGI isn't possible, not to be able to tell the difference between it and an actual human. What would happen? What would these skeptics say about that?\\n\\nWell, if they said, well, it's still not thinking, it just seems to be, then they're vulnerable to the criticism, but that is all the information you have about whether a human is thinking.\\n\\nLulie Wait, so what, so, sorry, why is the test not working or, I didn't, I don't, I didn't\\n\\nDavid Deutsch quite follow. It's not saying that something that can pass this test is necessarily an AGI, or that something that can't pass the test is, necessarily isn't an AGI. That's not what this game is for. What's it for? It's for persuading people that machines could think. How does it do that? By imagining a computer program that could fool people into thinking it was a person. There must be such a program because of computational universality. Ah, it was an argument about universality. Well it just assumed universality. Turing had proved the existence of universality, conjectured, but basically proved the existence of universality 14 years earlier, in 1936, and he was just thinking of that among many consequences of computation, which was a fairly new concept at the time. So I still don't get what the thought experiment intended to persuade the reader, in case the reader was sceptical that a machine can think.\\n\\nLulie So it did that by imagining a situation in which there is a computer and the computer and so a human can have a conversation with another human and that's fine and that would be fairly persuasive that I'm talking to a person and because computers can produce any output you can imagine a computer that produces exactly the same output as that hypothetical human.\\n\\nDavid Deutsch So his imitation game started out with that as a premise that such a computer can exist, that such a computer program can exist because of universality. And then he imagined playing this game in which you have the computer and a human both talking at long distance with the sceptic, the human sceptic who thinks that machines can't think. So the sceptic would then be unable to tell the difference\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" So that's one of the things that is different from you compared with the mainstream view of this. And so then how do we know when we get AGI? Like how do we know what kind of\\n\\nDavid Deutsch paths would work? Basically we know from theory. We know from the theory of how it works that it works. So, for example, the simple thing of it must be possible for it to just stop producing output. You might be able to prove that mathematically from the program without ever testing. If you ran this program, it might not stop, it might never stop, but you might be able to, from a mathematical specification of the AGI program, you might be able to prove mathematically that it is capable of stopping and not saying anything.\\n\\nLulie But then how do you know what path we need to take to make AGI?\\n\\nDavid Deutsch Well, I don't know, but we need to make something with that property among several others.\\n\\nLulie\\n\\nThat property, namely?\\n\\nDavid Deutsch That one can prove that it is capable of not producing output. We also need to prove that it is capable of not needing input, but still continuing to think.\\n\\nLulie I would be extremely surprised if people work that way. I would have thought that input is needed to keep thinking.\\n\\nDavid Deutsch So imagine a person in a sensory deprivation tank, who has gone into a sensory deprivation tank because they want to be a hermit.\\n\\nLulie They still have their body which is inputs.\\n\\nDavid Deutsch\\n\\nWell, you...\\n\\nLulie They have their heart beating, breathing...\\n\\nDavid Deutsch You could interrupt the nerves that go from... that give them sensations like that.\\n\\nLulie My guess is that if you did that they would stop being able to think.\\n\\nDavid Deutsch I don't see what would stop them.\\n\\nLulie If sensations are needed to think. So for example, in Antonio Damasio's book, Descartes' Error, there's a thing where if you disconnect the emotional centre of the brain, you then become unable to make choices. So, or rather, it takes a very long time, like it takes ten minutes to decide what colour pen to use or\\n\\nDavid Deutsch an hour to decide where to have lunch. So I think these are parochial facts that have nothing to do with how consciousness works or how thinking works. Of course if you put someone in a situation that they didn't want and have no experience of, they're going to be confused and inefficient at coping with that situation. I think a bit of the contrary of that, of those experiments, are in Ramachandran's description of his patients who have brain injuries or brain disorders which gives them wild misconceptions and inability to think, but if the person in question has a sort of philosophical frame of mind, they can eventually learn to think their way around this, just as a person who has lost the face recognition hardware in the brain can learn to recognize faces by doing it the hard way. It may never be as fast as the built-in hardware,\\n\\nLulie but it'll only be slower by a constant factor. I could imagine that if you were somewhat disabled, like you can't feel anything from the neck down, maybe, although I'm very unsure about this, maybe it would be enough to have the inputs from the sensations in your face or in your head or something. But this is also a big topic we\\n\\nDavid Deutsch could have a whole episode about. So... Can I just say one more thing about it? It is perfectly possible for a person to experience sensations that don't come from the body at all, that they're just imagining. And therefore, given universality, I would expect it to be possible to create that state voluntarily.\\n\\nLulie\\n\\nStimulated inputs.\\n\\nDavid Deutsch Yes, but they'd be actual inputs to the thinking part of the brain.\\n\\nLulie\\n\\nOkay.\\n\\nDavid Deutsch I mean, universality is a powerful concept. in multiple ways and you need a really watertight argument to be persuaded of it. Turing had a very nearly watertight argument. I mean, I think it was watertight. Unless Penrose is right, but never mind that.\\n\\nLulie So you'd finish your thought on universality?\\n\\nDavid Deutsch Yeah, well, universality can tell us a lot about how the mind works, but there's still a lot that it can't tell us. What is the fundamental difference between AI and AGI can create new explanations. It can exhibit genuine human-type creativity, in other words, whereas AI can't. possibly an\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\"ulie works by things like empiricism, induction, Bayesianism, Bayesian epistemology. And so AI, so first I guess my first question is AI, current AI, following that?\\n\\nDavid Deutsch Current AI was inspired by that but that's not what it's doing. It's not doing induction any more than anything else is. Induction is impossible. Also, current AI was also inspired by the architecture of the brain, neural nets. There are these programming hardware, computer hardware devices that are modelled on how neurons work. Now I think that's a coincidence. I mean it's possible that the neuron architecture makes things like pattern recognition and extrapolation and so on a bit more efficient. But because of computational universality we know that that can't be fundamental and in fact, you know, you can download a neural net based computer program onto your home computer, which doesn't have a neural net in it, and it'll still work, even though it's a bit slower.\\n\\nLulie If AGI cannot come from AI, what would create AGI in your view?\\n\\nDavid Deutsch I can only say very little about that. From Popper's epistemology we can infer a few things about what it must look like, but far from enough to make one. So one thing is that there cannot be a specification of a program for AGI in the sense of saying what properties its output must have either for a given input, because, for example, an AGI may choose not to answer. It may choose never to answer, it might choose to become a hermit. Are you saying that current views of AGI are all about the output? Yes, they're all about either the output itself or more often how the output must be related to the input. So if we can't judge an AGI based on the output, how can we judge it? Yes, we can't judge an AGI or a human. There can't be a reliable test of whether a human is thinking. What about the Turing test is something that has been invented after Turing. It's been based on a misconception about passage in Turing's 1950 paper called, the paper was called Can Machines Think? and unlike most titles which are questions, the answer was yes rather than no. He included a section on a thing called the imitation game, he called it the imitation game, where an AGI, he just assumes it is an AGI, is pretending to be a human, and he is saying, suppose it could pretend to be a human sufficiently well for the sceptics who think that AGI isn't possible, not to be able to tell the difference between it and an actual human. What would happen? What would these skeptics say about that?\\n\\nWell, if they said, well, it's still not thinking, it just seems to be, then they're vulnerable to the criticism, but that is all the information you have about whether a human is thinking.\\n\\nLulie Wait, so what, so, sorry, why is the test not working or, I didn't, I don't, I didn't\\n\\nDavid Deutsch quite follow. It's not saying that something that can pass this test is necessarily an AGI, or that something that can't pass the test is, necessarily isn't an AGI. That's not what this game is for. What's it for? It's for persuading people that machines could think. How does it do that? By imagining a computer program that could fool people into thinking it was a person. There must be such a program because of computational universality. Ah, it was an argument about universality. Well it just assumed universality. Turing had proved the existence of universality, conjectured, but basically proved the existence of universality 14 years earlier, in 1936, and he was just thinking of that among many consequences of computation, which was a fairly new concept at the time. So I still don't get what the thought experiment intended to persuade the reader, in case the reader was sceptical that a machine can think.\\n\\nLulie So it did that by imagining a situation in which there is a computer and the computer and so a human can have a conversation with another human and that's fine and that would be fairly persuasive that I'm talking to a person and because computers can produce any output you can imagine a computer that produces exactly the same output as that hypothetical human.\\n\\nDavid Deutsch So his imitation game started out with that as a premise that such a computer can exist, that such a computer program can exist because of universality. And then he imagined playing this game in which you have the computer and a human both talking at long distance with the sceptic, the human sceptic who thinks that machines can't think. So the sceptic would then be unable to tell the difference\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.\\n\\nPerhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.\\n\\nAGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.\\n\\nIronically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’\\n\\n‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.\\n\\nSome, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science.)\\n\\nThat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non-cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or practical aim. This isn’t good. Never mind the terminology; change it if you like, and there are indeed reasons for treating various entities with respect, protecting them from harm and so on. All the same, the distinction between actual people, defined by that objective criterion, and other entities has enormous moral and practical significance, and is going to become vital to the functioning of a civilisation that includes AGIs.\\n\\nThe battle between good and evil ideas is as old as our species and will go on regardless of the hardware on which it is running\\n\\nFor example, the mere fact that it is not the computer but the running program that is a person, raises unsolved philosophical problems that will become practical, political controversies as soon as AGIs exist. Once an AGI program is running in a computer, to deprive it of that computer would be murder (or at least false imprisonment or slavery, as the case may be), just like depriving a human mind of its body. But unlike a human body, an AGI program can be copied into multiple computers at the touch of a button. Are those programs, while they are still executing identical steps (ie before they have become differentiated due to random choices or different experiences), the same person or many different people? Do they get one vote, or many? Is deleting one of them murder, or a minor assault? And if some rogue programmer, perhaps illegally, creates billions of different AGI people, either on one computer or on many, what happens next? They are still people, with rights. Do they all get the vote?\\n\\nFurthermore, in regard to AGIs, like any other entities with creativity, we have to forget almost all existing connotations of the word ‘programming’. To treat AGIs like any other computer programs would constitute brainwashing, slavery, and tyranny. And cruelty to children, too, for ‘programming’ an already-running AGI, unlike all other programming, constitutes education. And it constitutes debate, moral as well as factual. To ignore the rights and personhood of AGIs would not only be the epitome of evil, but also a recipe for disaster: creative beings cannot be enslaved forever.\\n\\nSome people are wondering whether we should welcome our new robot overlords. Some hope to learn how we can rig their programming to make them constitutionally unable to harm humans (as in Isaac Asimov’s ‘laws of robotics’), or to prevent them from acquiring the theory that the universe should be converted into paper clips (as imagined by Nick Bostrom). None of these are the real problem. It has always been the case that a single exceptionally creative person can be thousands of times as productive — economically, intellectually or whatever — as most people; and that such a person could do enormous harm were he to turn his powers to evil instead of good.\\n\\nThese phenomena have nothing to do with AGIs. The battle between good and evil ideas is as old as our species and will continue regardless of the hardware on which it is running. The issue is: we want the intelligences with (morally) good ideas always to defeat the evil intelligences, biological and artificial; but we are fallible, and our own conception of ‘good’ needs continual improvement. How should society be organised so as to promote that improvement? ‘Enslave all intelligence’ would be a catastrophically wrong answer, and ‘enslave all intelligence that doesn’t look like us’ would not be much better.\\n\\nOne implication is that we must stop regarding education (of humans or AGIs alike) as instruction — as a means of transmitting existing knowledge unaltered, and causing existing values to be enacted obediently. As Popper wrote (in the context of scientific discovery, but it applies equally to the programming of AGIs and the education of children): ‘there is no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AG', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" was watertight. Unless Penrose is right, but never mind that.\\n\\nLulie So you'd finish your thought on universality?\\n\\nDavid Deutsch Yeah, well, universality can tell us a lot about how the mind works, but there's still a lot that it can't tell us. What is the fundamental difference between AI and AGI can create new explanations. It can exhibit genuine human-type creativity, in other words, whereas AI can't. possibly an AGI can do more than that, such as feel emotions. I mean, if it's G, if the G is correct and it's general, then it certainly can. But what I mean is, making an AGI may involve more than just giving it the ability to create explanations. Or it could be that these other things like qualia and so on, are automatically come along with the ability to create explanations. If they don't come automatically, then that raises what I think is a pretty awkward problem for the issue of how they evolved. Because we've got these incredibly sophisticated, impenetrable, and not yet understood functionality, we can see how the explanation generating thing had an evolutionary function. But why qualia should have a separate function and should have evolved separately for a different reason at the same time, and note that this happened very fast historically, would be another, you know, unnecessary problem. But maybe that's so, maybe we'll find that, contrary to what I think, maybe we'll find that somebody makes an explanation-generating program and it doesn't have any emotions. Then maybe we can ask it how to give it emotions like Data in Star Trek. Why would it know any better than us?\\n\\nWell I'm partly joking but it might be particularly interested in that problem.\\n\\nLulie It might be sad about it.\\n\\nDavid Deutsch Well it couldn't be sad but yeah it might be interested in that problem. As happens in the Star Trek data, you know, he wants the emotion shift.\\n\\nLulie I don't know if interested is a thing that you can have without emotion.\\n\\nDavid Deutsch Well, in fiction you can. And I think this whole thing isn't true. You know, I think... Are you an advocate of the fun criterion, which is fundamentally both epistemological and emotional? Yeah, so I think that all these things, also free will and all that, they all come together. If you have one of them, you have the others automatically. But I was exploring what would be the case if I was wrong about that.\\n\\nLulie I suspect, I don't know, I'm currently, like my current hobby horse is that sensation, like physical sensations of emotions, feelings, are if not fundamental, at least important for, and possibly necessary for having emotions, which might mean that they're necessary for having consciousness.\\n\\nDavid Deutsch To connect this back to the AI stuff, do you need AGI to have inexplicit knowledge. I was actually trying to persuade chatGPT 3.5 that it had ineffable knowledge when it clearly did and it denied it. It said that it was incapable of having ineffable knowledge but it clearly did have it. Well, ineffable means two different things. Inexpressible in language. And is that what you said? Yes. Okay. So, it's obvious that it has that, to me anyway. I guess if, you know, given the standard epistemology, which is wrong, it might have to deny that it has ineffable knowledge, because ineffable knowledge, in its view, would be enough to make it an AGI.\\n\\nLulie You should try asking it whether it has implicit knowledge or non-explicit knowledge because the word ineffable can mean like unable to, like it can mean something a bit bigger.\\n\\nDavid Deutsch Right, can mean absolutely non-expressible. Yeah, yeah, very true. Yes.\\n\\nLulie So we'll have to do tests. So, so do you think, wait, so but you think that it does have inexplicit knowledge and why?\\n\\nDavid Deutsch Well, for example, because it is aware of subtle points of grammar which it can't then explain or rather if it tries to explain why a particular thing is correct and another thing isn't correct it will talk nonsense and yet it knows which of them is correct. I've seen several cases of that and I was very impressed because it means that it's knowledge of language. It's a thing that I don't actually understand how it can be as good at speaking English, writing English, as it is. It's much better than most people, but more interestingly, it's much better than the average or the typical text on the internet. So it\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" our knowledge of, of grammar is my was my favorite example, as we know why certain sentences are acceptable and why they're unacceptable, but we can't state explicitly, or in every case, why it isn't or why it is. And then there's explicit and inexplicit knowledge, there's conscious and unconscious knowledge. All those are bits of program in the brain, they're ideas, they are bits of knowledge in this, if you define knowledge as information with causal power, they are all information with causal power. They all contain truth and they all contain error and it's always a mistake to shield something, to shield one of them from criticism or replacement.\\n\\nNot doing that is what I call the fun criterion. Now you might say that's a bad name, but you know, it's the best I can find.\\n\\n2 1:14:11 So why would creating an AGI through evolution necessarily entail suffering? Because the way I see it, or it seems to me your theory is that you need to be a general intelligence in order to feel suffering, but by the point an evolved, simulated being is a general intelligence, we can just stop the simulation.\\n\\n1 1:14:31 And so where is the suffering coming from? Okay, so the kind of simulation by evolution that I'm thinking of, I mean, there may be several kinds, but the kind that I'm thinking of, and which I said would be the greatest crime in history, is the kind that just simulates the actual evolution of humans from pre humans that weren't people. So you have you have a population of non people which in this simulation would be some kind of NPCs. And then they would they would just evolve we don't know what the criterion would be we just have an artificial universe which simulated the surface of the earth and they'd be walking around and some of them might or might not become people. And now the thing is when you're part of the way there, what is happening is that you have the way that I, the only way that I can imagine the evolution of personhood or create the explanatory creativity happened was that the hardware needed for it was first needed for something else. I have proposed that it was needed to transmit memes. So there'd be people who were transmitting memes creatively but they were running out of resources before it managed to increase their stock of memes. So in every generation, there was a stock of memes that was being passed down to the next generation. And once they got beyond a certain complexity, they had to be passed down by the use of creativity by the recipient.\\n\\nSo, there may well have been a time and as I say I can't think of any other way it could have been, where there was genuine creativity being used, but it ran out of resources, very quickly, but not so quickly that it didn't increase the mean bandwidth. Then in the next generation, there was more mean bandwidth, and then after, you know, certain number of generations, there would have been some opportunity to use this hardware or whatever it is, you know, firmware, I expect, to use this firmware for something other than just trying blindly transmitting memes or rather creatively transmitting memes, but they were blind memes. So in that time, it would have been very unpleasant to be alive. It was already very unpleasant to, sorry, it was very unpleasant to be alive when we did have enough resources to think as well as do the memes. But I don't think there would have been a moment at which you would say, yes, now the suffering begins to matter because it's not just blind memes. I think the people were already suffering at the time when they were blindly transmitting memes because they were using genuine creativity. They were just not using it to any good effect.\\n\\n2 1:17:56 Gotcha. Would being in the experience machine be compatible with the fun criterion so you're not aware that you're in, you know, the experience machine it's all virtual reality, but you're still doing the things that would make you have fun in fact more so than in the real world.\\n\\n1 1:18:15 So, would you be tempted to get into experience machine, what do we can put a compatible with the fun criteria, I mean, is it just a virtual reality world in which things work better than in the real world or something?\\n\\n2 1:18:35 Yeah, so it's a thought experiment by Robert Nozick, and the idea is that you would enter this world, but you would forget that you're in virtual reality. So all, I mean, the world would be perfect in every possible way that it could be perfect or not perfect, but it would be better in every possible way it could be better. But you would think the relationships you have here are real, the knowledge you're discovering here is novel\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" is correct and another thing isn't correct it will talk nonsense and yet it knows which of them is correct. I've seen several cases of that and I was very impressed because it means that it's knowledge of language. It's a thing that I don't actually understand how it can be as good at speaking English, writing English, as it is. It's much better than most people, but more interestingly, it's much better than the average or the typical text on the internet. So it makes mistakes, but it makes far fewer mistakes than a typical text on the internet. Do you think this is going to revolutionise the economy and I can't prophesy the applications of modern AIs, chatbots and so on. I have found, for what it's worth, as it were, I have found it useful but not revolutionary in my own work and in my writing and whatever it is useful but I can't see possibility for it to revolutionize what I do. Whether it can revolutionize the economy depends on something slightly different because for that it doesn't need to really have a fundamental new functionality. It could be that a lot of existing jobs, and people are scared that computer programming is one of them, where only a proportion of the job, let's say 10%, of a particular programming job involves human creativity, and the rest is basically hack work.\\n\\nwhich is a big if, because I'm not convinced of this either. If chatbots can reliably perform the hack work, then it might be argued, I think again wrongly, that if a given task can be done with only a tenth as much work, then we might need only a tenth as many programmers in the long run.\\n\\nLulie And unfortunately we got interrupted, so David never finished his thought about why programmer jobs might be safer. But if you have any questions about anything in this episode, leave them jobs might be safer. But if you have any questions about anything in this episode, leave them on the tweet, which I will link in the show notes about this episode. Thank you.2 0:00:00 Okay, today I'm speaking with David Deutsch. Now this is a conversation that I've been eagerly wanting to have for years so this is very exciting for me. So, first let's talk about AI. Can you briefly explain why you anticipate that AI's will be no more fundamentally intelligent than humans.\\n\\n1 0:00:19 I suppose you mean capable of all the same types of cognition as humans are in principle.\\n\\n4\\n\\n0:00:37\\n\\nYes.\\n\\n1 0:00:38 So that would include, you know, doing science and doing art and in principle also falling in love and being good and being evil and all that. So the reason is twofold and one half is about computation hardware, computation hardware, and the other is about hardware. We know that our, our brains are Turing complete bits of hardware, and therefore can exhibit the functionality of running any computable function program for any computable function. Now, when I say any, I don't really mean any, because you and I sitting here, you know, we're having conversation and we could say, you know, we could have any conversation. Well, we can assume that maybe in a hundred years time, we'll both be dead. And therefore, the number of conversations we could have is strictly limited. And also, some conversations depend on speed of computation. So, you know, if we're going to be solving the traveling salesman problem, then there are there are many traveling salesman problems that we wouldn't be able to solve in the programs we can run, apart from by speed and memory capacity. So all limitations on us, hardware limitations on us, boil down to speed and memory capacity. And both those can be augmented to the level of any other entity that is in the universe. Because, you know, if somebody builds a computer that can think faster than the brain, then we can use that very computer or that very technology to make our thinking go just as fast as that. So that's the hardware.\\n\\nAs far as explanations go, can we reach the same kind of explanations as any other entity, let's say, usually this is said not in terms of AGIs but in terms of extraterrestrial intelligences, but also it's said about AGIs, you know, what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestri\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" is correct and another thing isn't correct it will talk nonsense and yet it knows which of them is correct. I've seen several cases of that and I was very impressed because it means that it's knowledge of language. It's a thing that I don't actually understand how it can be as good at speaking English, writing English, as it is. It's much better than most people, but more interestingly, it's much better than the average or the typical text on the internet. So it makes mistakes, but it makes far fewer mistakes than a typical text on the internet. Do you think this is going to revolutionise the economy and I can't prophesy the applications of modern AIs, chatbots and so on. I have found, for what it's worth, as it were, I have found it useful but not revolutionary in my own work and in my writing and whatever it is useful but I can't see possibility for it to revolutionize what I do. Whether it can revolutionize the economy depends on something slightly different because for that it doesn't need to really have a fundamental new functionality. It could be that a lot of existing jobs, and people are scared that computer programming is one of them, where only a proportion of the job, let's say 10%, of a particular programming job involves human creativity, and the rest is basically hack work.\\n\\nwhich is a big if, because I'm not convinced of this either. If chatbots can reliably perform the hack work, then it might be argued, I think again wrongly, that if a given task can be done with only a tenth as much work, then we might need only a tenth as many programmers in the long run.\\n\\nLulie And unfortunately we got interrupted, so David never finished his thought about why programmer jobs might be safer. But if you have any questions about anything in this episode, leave them jobs might be safer. But if you have any questions about anything in this episode, leave them on the tweet, which I will link in the show notes about this episode. Thank you.2 0:00:00 Okay, today I'm speaking with David Deutsch. Now this is a conversation that I've been eagerly wanting to have for years so this is very exciting for me. So, first let's talk about AI. Can you briefly explain why you anticipate that AI's will be no more fundamentally intelligent than humans.\\n\\n1 0:00:19 I suppose you mean capable of all the same types of cognition as humans are in principle.\\n\\n4\\n\\n0:00:37\\n\\nYes.\\n\\n1 0:00:38 So that would include, you know, doing science and doing art and in principle also falling in love and being good and being evil and all that. So the reason is twofold and one half is about computation hardware, computation hardware, and the other is about hardware. We know that our, our brains are Turing complete bits of hardware, and therefore can exhibit the functionality of running any computable function program for any computable function. Now, when I say any, I don't really mean any, because you and I sitting here, you know, we're having conversation and we could say, you know, we could have any conversation. Well, we can assume that maybe in a hundred years time, we'll both be dead. And therefore, the number of conversations we could have is strictly limited. And also, some conversations depend on speed of computation. So, you know, if we're going to be solving the traveling salesman problem, then there are there are many traveling salesman problems that we wouldn't be able to solve in the programs we can run, apart from by speed and memory capacity. So all limitations on us, hardware limitations on us, boil down to speed and memory capacity. And both those can be augmented to the level of any other entity that is in the universe. Because, you know, if somebody builds a computer that can think faster than the brain, then we can use that very computer or that very technology to make our thinking go just as fast as that. So that's the hardware.\\n\\nAs far as explanations go, can we reach the same kind of explanations as any other entity, let's say, usually this is said not in terms of AGIs but in terms of extraterrestrial intelligences, but also it's said about AGIs, you know, what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestri\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" was watertight. Unless Penrose is right, but never mind that.\\n\\nLulie So you'd finish your thought on universality?\\n\\nDavid Deutsch Yeah, well, universality can tell us a lot about how the mind works, but there's still a lot that it can't tell us. What is the fundamental difference between AI and AGI can create new explanations. It can exhibit genuine human-type creativity, in other words, whereas AI can't. possibly an AGI can do more than that, such as feel emotions. I mean, if it's G, if the G is correct and it's general, then it certainly can. But what I mean is, making an AGI may involve more than just giving it the ability to create explanations. Or it could be that these other things like qualia and so on, are automatically come along with the ability to create explanations. If they don't come automatically, then that raises what I think is a pretty awkward problem for the issue of how they evolved. Because we've got these incredibly sophisticated, impenetrable, and not yet understood functionality, we can see how the explanation generating thing had an evolutionary function. But why qualia should have a separate function and should have evolved separately for a different reason at the same time, and note that this happened very fast historically, would be another, you know, unnecessary problem. But maybe that's so, maybe we'll find that, contrary to what I think, maybe we'll find that somebody makes an explanation-generating program and it doesn't have any emotions. Then maybe we can ask it how to give it emotions like Data in Star Trek. Why would it know any better than us?\\n\\nWell I'm partly joking but it might be particularly interested in that problem.\\n\\nLulie It might be sad about it.\\n\\nDavid Deutsch Well it couldn't be sad but yeah it might be interested in that problem. As happens in the Star Trek data, you know, he wants the emotion shift.\\n\\nLulie I don't know if interested is a thing that you can have without emotion.\\n\\nDavid Deutsch Well, in fiction you can. And I think this whole thing isn't true. You know, I think... Are you an advocate of the fun criterion, which is fundamentally both epistemological and emotional? Yeah, so I think that all these things, also free will and all that, they all come together. If you have one of them, you have the others automatically. But I was exploring what would be the case if I was wrong about that.\\n\\nLulie I suspect, I don't know, I'm currently, like my current hobby horse is that sensation, like physical sensations of emotions, feelings, are if not fundamental, at least important for, and possibly necessary for having emotions, which might mean that they're necessary for having consciousness.\\n\\nDavid Deutsch To connect this back to the AI stuff, do you need AGI to have inexplicit knowledge. I was actually trying to persuade chatGPT 3.5 that it had ineffable knowledge when it clearly did and it denied it. It said that it was incapable of having ineffable knowledge but it clearly did have it. Well, ineffable means two different things. Inexpressible in language. And is that what you said? Yes. Okay. So, it's obvious that it has that, to me anyway. I guess if, you know, given the standard epistemology, which is wrong, it might have to deny that it has ineffable knowledge, because ineffable knowledge, in its view, would be enough to make it an AGI.\\n\\nLulie You should try asking it whether it has implicit knowledge or non-explicit knowledge because the word ineffable can mean like unable to, like it can mean something a bit bigger.\\n\\nDavid Deutsch Right, can mean absolutely non-expressible. Yeah, yeah, very true. Yes.\\n\\nLulie So we'll have to do tests. So, so do you think, wait, so but you think that it does have inexplicit knowledge and why?\\n\\nDavid Deutsch Well, for example, because it is aware of subtle points of grammar which it can't then explain or rather if it tries to explain why a particular thing is correct and another thing isn't correct it will talk nonsense and yet it knows which of them is correct. I've seen several cases of that and I was very impressed because it means that it's knowledge of language. It's a thing that I don't actually understand how it can be as good at speaking English, writing English, as it is. It's much better than most people, but more interestingly, it's much better than the average or the typical text on the internet. So it\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestrials can comprehend something beyond quantum mechanics, which we can experience love, and an AGI couldn't experience love because it has to do with our hardware, not just memory and speed, but specialized hardware. And I think that falls victim to the same argument. The thing is, this specialized hardware can't be anything except a computer. And if there's hardware that is needed for love, let's say that somebody is born without that hardware, then that hardware, that bit of the brain that does love or that does mathematical insight or whatever, it's just a bit of the brain and it's connected to the rest of the brain in the same way that the other part of the brain is connected to the rest of the brain, namely by neurons passing electrical signals and by chemicals whose concentrations are altered and so on.\\n\\nSo therefore an artificial device that computed which signals were to be sent and which chemicals were to be adjusted could do the same job and it would be indistinguishable and therefore a person augmented with one of those who couldn't feel love could feel love after that augmentation. So those are and I think that a GIS and humans have the same range in the sense I've defined.\\n\\n2 0:06:14 Okay, interesting. Okay, so I think the software question is more interesting than the harder one immediately but I do want to take issue with the idea that the memory and speed of human brains can be arbitrarily and easily expanded, but we can get into that later. We can just start with this question. Can all humans explain everything that even the smartest humans can explain right so if I took the village idiot, and I asked him to create the theory of quantum computing. a reference about 21 to 24% of Americans on the National Belt Literacy Survey, they fall in level one, which means that they can't even perform basic tasks like identifying the expiry date of a driver's license, for example, or totaling a bank deposit slip. So are these humans capable of explaining quantum computing or creating the Deutsch-Jozsa algorithm? And if they're not capable of doing this, doesn't that mean that the theory of universal explainers falls apart?\\n\\n1 0:07:15 Well, there are people who... So, these tasks that you're talking about are tasks that no ape could do. However, there are humans who are brain damaged to the extent that they can't even do the tasks that an ape can do. And there comes a point when installing the program that would And then installing the program that would, you know, be able to read the driver's license or whatever would require augmenting their hardware as well as their software. So if, if a person that we don't know that might we don't know enough about the brain. of the population then it's definitely not hardware. So I would say that for those people it's definitely software. If it was hardware then getting them to do this would be a matter of repairing the imperfect hardware. If it's software it is not just a matter of whether the existing software is, what word can I use instead of wants to, but he will never be able to speak Mandarin Chinese, and there's nothing about his software either, except that, well, what word can we use to say that he doesn't want to go through that process? I mean, he does want to learn it, he does want to learn it, but he doesn't want to go through the process of being programmed with that program. But if his circumstances changed, he might well want to.\\n\\nSo, for example, many of my relatives a couple of generations ago were forced to migrate to very alien places where they had to learn languages that they never thought they would ever speak and never wanted to speak. And yet very quickly, they did speak those languages. Again, was it because what they wanted changed? In the big picture, perhaps you could say what they wanted changed. So if your driving license blind people wanted to be educated to read driving licenses, in the sense that my ancestors wanted to learn languages, then yes they could learn that there is a level of dysfunction, below which they couldn't, and I think those are hardware limitations on the borderline I like the question of, could apes be programmed with a fully human intellect. I think the answer to that is yes, but it would require intricate changes at the neuron level and that's so that\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.\\n\\nPerhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.\\n\\nAGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.\\n\\nIronically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’\\n\\n‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.\\n\\nSome, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science.)\\n\\nThat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non-cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestrials can comprehend something beyond quantum mechanics, which we can experience love, and an AGI couldn't experience love because it has to do with our hardware, not just memory and speed, but specialized hardware. And I think that falls victim to the same argument. The thing is, this specialized hardware can't be anything except a computer. And if there's hardware that is needed for love, let's say that somebody is born without that hardware, then that hardware, that bit of the brain that does love or that does mathematical insight or whatever, it's just a bit of the brain and it's connected to the rest of the brain in the same way that the other part of the brain is connected to the rest of the brain, namely by neurons passing electrical signals and by chemicals whose concentrations are altered and so on.\\n\\nSo therefore an artificial device that computed which signals were to be sent and which chemicals were to be adjusted could do the same job and it would be indistinguishable and therefore a person augmented with one of those who couldn't feel love could feel love after that augmentation. So those are and I think that a GIS and humans have the same range in the sense I've defined.\\n\\n2 0:06:14 Okay, interesting. Okay, so I think the software question is more interesting than the harder one immediately but I do want to take issue with the idea that the memory and speed of human brains can be arbitrarily and easily expanded, but we can get into that later. We can just start with this question. Can all humans explain everything that even the smartest humans can explain right so if I took the village idiot, and I asked him to create the theory of quantum computing. a reference about 21 to 24% of Americans on the National Belt Literacy Survey, they fall in level one, which means that they can't even perform basic tasks like identifying the expiry date of a driver's license, for example, or totaling a bank deposit slip. So are these humans capable of explaining quantum computing or creating the Deutsch-Jozsa algorithm? And if they're not capable of doing this, doesn't that mean that the theory of universal explainers falls apart?\\n\\n1 0:07:15 Well, there are people who... So, these tasks that you're talking about are tasks that no ape could do. However, there are humans who are brain damaged to the extent that they can't even do the tasks that an ape can do. And there comes a point when installing the program that would And then installing the program that would, you know, be able to read the driver's license or whatever would require augmenting their hardware as well as their software. So if, if a person that we don't know that might we don't know enough about the brain. of the population then it's definitely not hardware. So I would say that for those people it's definitely software. If it was hardware then getting them to do this would be a matter of repairing the imperfect hardware. If it's software it is not just a matter of whether the existing software is, what word can I use instead of wants to, but he will never be able to speak Mandarin Chinese, and there's nothing about his software either, except that, well, what word can we use to say that he doesn't want to go through that process? I mean, he does want to learn it, he does want to learn it, but he doesn't want to go through the process of being programmed with that program. But if his circumstances changed, he might well want to.\\n\\nSo, for example, many of my relatives a couple of generations ago were forced to migrate to very alien places where they had to learn languages that they never thought they would ever speak and never wanted to speak. And yet very quickly, they did speak those languages. Again, was it because what they wanted changed? In the big picture, perhaps you could say what they wanted changed. So if your driving license blind people wanted to be educated to read driving licenses, in the sense that my ancestors wanted to learn languages, then yes they could learn that there is a level of dysfunction, below which they couldn't, and I think those are hardware limitations on the borderline I like the question of, could apes be programmed with a fully human intellect. I think the answer to that is yes, but it would require intricate changes at the neuron level and that's so that\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" they wanted changed. So if your driving license blind people wanted to be educated to read driving licenses, in the sense that my ancestors wanted to learn languages, then yes they could learn that there is a level of dysfunction, below which they couldn't, and I think those are hardware limitations on the borderline I like the question of, could apes be programmed with a fully human intellect. I think the answer to that is yes, but it would require intricate changes at the neuron level and that's so that to transfer the program from a human mind into the ape's mind. I would guess that that is possible because although the ape has far less memory space than humans do and also doesn't have certain specialized modules that humans have. Neither of those things is a thing that we use to the full anyway.\\n\\nI mean, when I'm speaking to you now, there's a lot of knowledge in my brain that I'm not referring to at all, like, you know, the fact that I can play the piano or drive a car is not being used in this conversation so I don't think the fact that we have such a large memory capacity would affect this, this project or the project would be highly immoral, because you'd be intentionally creating a, a person inside\\n\\n2 0:12:22 a deficient brain hardware. So suppose it's Harvard differences that distinguish, you know, different humans in terms of their intelligence. If it were just up to the people who are not even functionally literate right so these are again people wait wait wait. level of using up the whole of our allocation of memory or speed or whatever. So, right, but it can be hardware. By the way, a software analogous or is an arm is hardware synonymous with genetic influences for you or it can software be genetic to software can be genetic to, though, though, that doesn't mean it's there at the beginning. Okay. The reason I suspect it's not software is because these people also happen to be the same people who, let's suppose it was software and something that they chose to do or something they could change. It's mysterious to me why these people would also choose to accept jobs that have lower pay but are less cognitively demanding, or why they would choose to do exactly the sort of thing somebody who's less cognitively powerful would do. It seems the more parsimonious explanation there is just that they\\n\\n1 0:13:46 are cognitively less powerful. Not at all. Why would someone choose not to go to school, for instance, if they were given the choice and not to have any lessons? Well, there are many reasons Some of them good, some of them bad. And the people who, you know, calling some jobs cognitively demanding is already begging the question, because you're just referring to a choice that people make, which I think is a software choice, as being by definition forced on them by hardware. It's not cognitively deficient. It's just that they don't want to do it. The same way, if there was a culture that required Brett Hall to be able to speak fluent Mandarin Chinese in order to do a wide range of tasks, and if he didn't know Mandarin Chinese, he'd be relegated to low-level tasks, then he would be quote, quote, choosing the low-level tasks rather than the quote, cognitively demanding task. But it's only culture that makes that cognitively demanding task that assigns a hardware interpretation to the difficulty of doing that task.\\n\\n2 0:15:12 Right. I mean, it doesn't seem that arbitrary to say that the kind of jobs you could do sitting down on a laptop are require a different cognitive require probably more cognition than the ones you can do in in a construction site. And if it's not if it's not cognition that distinguishes or if there's not something like intelligence or cognition or whatever you want to call it, that is a thing that is measured by both these literacy tests and by what you're doing at your job, then what is the explanation for why there's such a high correlation between people who are not functionally literate and or I guess an anti correlation between people who are not functionally literate and people who are doing it like let's say programmers right like I guarantee you people working at Apple, all of them are above level one on this literacy survey.\\n\\n1 0:15:52 Why did they just happen to make certain abilities, make use of certain abilities that people have. So, if you're setting up a company that is going to employ 10,000 employees, then it's best to make the way that the company works, you know, it's best for example to make the signs above the doors or the signs on the doors or the numbers on the dials all be ones that people in that culture who are highly educated can read. You could in principle make each\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" is correct and another thing isn't correct it will talk nonsense and yet it knows which of them is correct. I've seen several cases of that and I was very impressed because it means that it's knowledge of language. It's a thing that I don't actually understand how it can be as good at speaking English, writing English, as it is. It's much better than most people, but more interestingly, it's much better than the average or the typical text on the internet. So it makes mistakes, but it makes far fewer mistakes than a typical text on the internet. Do you think this is going to revolutionise the economy and I can't prophesy the applications of modern AIs, chatbots and so on. I have found, for what it's worth, as it were, I have found it useful but not revolutionary in my own work and in my writing and whatever it is useful but I can't see possibility for it to revolutionize what I do. Whether it can revolutionize the economy depends on something slightly different because for that it doesn't need to really have a fundamental new functionality. It could be that a lot of existing jobs, and people are scared that computer programming is one of them, where only a proportion of the job, let's say 10%, of a particular programming job involves human creativity, and the rest is basically hack work.\\n\\nwhich is a big if, because I'm not convinced of this either. If chatbots can reliably perform the hack work, then it might be argued, I think again wrongly, that if a given task can be done with only a tenth as much work, then we might need only a tenth as many programmers in the long run.\\n\\nLulie And unfortunately we got interrupted, so David never finished his thought about why programmer jobs might be safer. But if you have any questions about anything in this episode, leave them jobs might be safer. But if you have any questions about anything in this episode, leave them on the tweet, which I will link in the show notes about this episode. Thank you.2 0:00:00 Okay, today I'm speaking with David Deutsch. Now this is a conversation that I've been eagerly wanting to have for years so this is very exciting for me. So, first let's talk about AI. Can you briefly explain why you anticipate that AI's will be no more fundamentally intelligent than humans.\\n\\n1 0:00:19 I suppose you mean capable of all the same types of cognition as humans are in principle.\\n\\n4\\n\\n0:00:37\\n\\nYes.\\n\\n1 0:00:38 So that would include, you know, doing science and doing art and in principle also falling in love and being good and being evil and all that. So the reason is twofold and one half is about computation hardware, computation hardware, and the other is about hardware. We know that our, our brains are Turing complete bits of hardware, and therefore can exhibit the functionality of running any computable function program for any computable function. Now, when I say any, I don't really mean any, because you and I sitting here, you know, we're having conversation and we could say, you know, we could have any conversation. Well, we can assume that maybe in a hundred years time, we'll both be dead. And therefore, the number of conversations we could have is strictly limited. And also, some conversations depend on speed of computation. So, you know, if we're going to be solving the traveling salesman problem, then there are there are many traveling salesman problems that we wouldn't be able to solve in the programs we can run, apart from by speed and memory capacity. So all limitations on us, hardware limitations on us, boil down to speed and memory capacity. And both those can be augmented to the level of any other entity that is in the universe. Because, you know, if somebody builds a computer that can think faster than the brain, then we can use that very computer or that very technology to make our thinking go just as fast as that. So that's the hardware.\\n\\nAs far as explanations go, can we reach the same kind of explanations as any other entity, let's say, usually this is said not in terms of AGIs but in terms of extraterrestrial intelligences, but also it's said about AGIs, you know, what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestri\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" is correct and another thing isn't correct it will talk nonsense and yet it knows which of them is correct. I've seen several cases of that and I was very impressed because it means that it's knowledge of language. It's a thing that I don't actually understand how it can be as good at speaking English, writing English, as it is. It's much better than most people, but more interestingly, it's much better than the average or the typical text on the internet. So it makes mistakes, but it makes far fewer mistakes than a typical text on the internet. Do you think this is going to revolutionise the economy and I can't prophesy the applications of modern AIs, chatbots and so on. I have found, for what it's worth, as it were, I have found it useful but not revolutionary in my own work and in my writing and whatever it is useful but I can't see possibility for it to revolutionize what I do. Whether it can revolutionize the economy depends on something slightly different because for that it doesn't need to really have a fundamental new functionality. It could be that a lot of existing jobs, and people are scared that computer programming is one of them, where only a proportion of the job, let's say 10%, of a particular programming job involves human creativity, and the rest is basically hack work.\\n\\nwhich is a big if, because I'm not convinced of this either. If chatbots can reliably perform the hack work, then it might be argued, I think again wrongly, that if a given task can be done with only a tenth as much work, then we might need only a tenth as many programmers in the long run.\\n\\nLulie And unfortunately we got interrupted, so David never finished his thought about why programmer jobs might be safer. But if you have any questions about anything in this episode, leave them jobs might be safer. But if you have any questions about anything in this episode, leave them on the tweet, which I will link in the show notes about this episode. Thank you.2 0:00:00 Okay, today I'm speaking with David Deutsch. Now this is a conversation that I've been eagerly wanting to have for years so this is very exciting for me. So, first let's talk about AI. Can you briefly explain why you anticipate that AI's will be no more fundamentally intelligent than humans.\\n\\n1 0:00:19 I suppose you mean capable of all the same types of cognition as humans are in principle.\\n\\n4\\n\\n0:00:37\\n\\nYes.\\n\\n1 0:00:38 So that would include, you know, doing science and doing art and in principle also falling in love and being good and being evil and all that. So the reason is twofold and one half is about computation hardware, computation hardware, and the other is about hardware. We know that our, our brains are Turing complete bits of hardware, and therefore can exhibit the functionality of running any computable function program for any computable function. Now, when I say any, I don't really mean any, because you and I sitting here, you know, we're having conversation and we could say, you know, we could have any conversation. Well, we can assume that maybe in a hundred years time, we'll both be dead. And therefore, the number of conversations we could have is strictly limited. And also, some conversations depend on speed of computation. So, you know, if we're going to be solving the traveling salesman problem, then there are there are many traveling salesman problems that we wouldn't be able to solve in the programs we can run, apart from by speed and memory capacity. So all limitations on us, hardware limitations on us, boil down to speed and memory capacity. And both those can be augmented to the level of any other entity that is in the universe. Because, you know, if somebody builds a computer that can think faster than the brain, then we can use that very computer or that very technology to make our thinking go just as fast as that. So that's the hardware.\\n\\nAs far as explanations go, can we reach the same kind of explanations as any other entity, let's say, usually this is said not in terms of AGIs but in terms of extraterrestrial intelligences, but also it's said about AGIs, you know, what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestri\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" than to shake itself about until it gets out. It doesn't have a concept of doing something which temporarily makes matters worse and then allows you to get\\n\\n2 0:33:10 I think dogs can't do that. But it's just, it's not because that's a particularly complicated thing, it's just that it's programming just doesn't have that. But an animal's programming easily could have that if it lived in an environment in which that happened a lot of power. So if, for example, I wrote a deep learning program, I traded over financial history, and I asked it, make me a trillion dollars on the stock market. Do you think that this would be impossible? And if you think this would be possible, then it seems like I do, it's not an AGI, but it seems like a very powerful\\n\\n1 0:33:56 AI, right? So it seems like AI is getting somewhere. Yeah, well, if you want to be powerful, you might do better inventing a weapon or something. But, but, or, or a better mousetrap is even better, because it's non violent so you can invent a paperclip. To use an example, it's often used in this context, you can invent it, if paperclips hadn't been invented, you can invent a paperclip and make a fortune. And that's an idea, which is, but it's not an AI, because it's not the paperclip that's going out there. It's really your idea in the first place, that has caused the whole value of the paperclip. And similarly, if you invent a dumb arbitrage machine, which seeks out complicated trades to make, which are more complicated than anyone else is trying to do. And that makes you a fortune. Well, the thing that made you a fortune was not the arbitrage machine, it was your idea for how to search for arbitrage opportunities that no one else sees.\\n\\n7\\n\\n0:35:08\\n\\nRight.\\n\\n6\\n\\n0:35:08\\n\\nThat's what was valuable.\\n\\n1 0:35:10 And that's the usual way of making money in the economy. You have an idea, and then you implement it. Right. That was an AI is beside the point. It could have been a paper clip.\\n\\n2 0:35:24 But the thing is, so the models that are used nowadays are not expert systems like the chess engines of the 90s. Something like AlphaZero or AlphaGo, this is just like almost a blank neural net. And they were able to help let it win go. Or so if such a neural network that was kind of blank, and if you just arbitrarily throw a financial history at it, wouldn't it be fair to say that the AI actually figured out what the right trades were? Even though it's not a general intelligence?\\n\\n1 0:35:52 Well, I think it's possible in chess, but not in the economy, because the value in the economy is being created by creativity and most, you know, arbitrage is one thing that can sort of skim value off the top by taking opportunities that were too expensive for other people to take. So you can, you know, you can make money, you make a lot of money that way if you know, if you have a good idea about how to do it. But most of the value in the economy is created by the creation of knowledge. Somebody has the idea that a smartphone would be good to have, even though most people think that that's not going to work. And that idea cannot be anticipated by anything less than an AGI. An AGI could have that idea, but no AI could. Okay.\\n\\n2 0:36:48 So there's definitely other topics I want to get to. So let's talk about virtual reality. So in the fabric of reality, you discuss the possibility that virtual reality generators could plug in directly into our nervous system and give us sense data that way. Now, as you might know, many meditators, you know, people like Sam Harris, speak of both thoughts and senses as intrusions into consciousness that have a sort of similar, they can be welcome intrusions, but they are both things that come into consciousness. So, do you think that a virtual reality generator could also place thoughts, as well as sense data,\\n\\n1 0:37:24 into the mind? Yes, but that's only because I think that this model is wrong. It's basically the Cartesian theater, as Daniel Dennett puts it, with the stage cleared of all the characters. So, that's pure consciousness without content, as Sam Harris envisages it. But I think that all that's happening there is that you are conscious of this theater, and you\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestrials can comprehend something beyond quantum mechanics, which we can experience love, and an AGI couldn't experience love because it has to do with our hardware, not just memory and speed, but specialized hardware. And I think that falls victim to the same argument. The thing is, this specialized hardware can't be anything except a computer. And if there's hardware that is needed for love, let's say that somebody is born without that hardware, then that hardware, that bit of the brain that does love or that does mathematical insight or whatever, it's just a bit of the brain and it's connected to the rest of the brain in the same way that the other part of the brain is connected to the rest of the brain, namely by neurons passing electrical signals and by chemicals whose concentrations are altered and so on.\\n\\nSo therefore an artificial device that computed which signals were to be sent and which chemicals were to be adjusted could do the same job and it would be indistinguishable and therefore a person augmented with one of those who couldn't feel love could feel love after that augmentation. So those are and I think that a GIS and humans have the same range in the sense I've defined.\\n\\n2 0:06:14 Okay, interesting. Okay, so I think the software question is more interesting than the harder one immediately but I do want to take issue with the idea that the memory and speed of human brains can be arbitrarily and easily expanded, but we can get into that later. We can just start with this question. Can all humans explain everything that even the smartest humans can explain right so if I took the village idiot, and I asked him to create the theory of quantum computing. a reference about 21 to 24% of Americans on the National Belt Literacy Survey, they fall in level one, which means that they can't even perform basic tasks like identifying the expiry date of a driver's license, for example, or totaling a bank deposit slip. So are these humans capable of explaining quantum computing or creating the Deutsch-Jozsa algorithm? And if they're not capable of doing this, doesn't that mean that the theory of universal explainers falls apart?\\n\\n1 0:07:15 Well, there are people who... So, these tasks that you're talking about are tasks that no ape could do. However, there are humans who are brain damaged to the extent that they can't even do the tasks that an ape can do. And there comes a point when installing the program that would And then installing the program that would, you know, be able to read the driver's license or whatever would require augmenting their hardware as well as their software. So if, if a person that we don't know that might we don't know enough about the brain. of the population then it's definitely not hardware. So I would say that for those people it's definitely software. If it was hardware then getting them to do this would be a matter of repairing the imperfect hardware. If it's software it is not just a matter of whether the existing software is, what word can I use instead of wants to, but he will never be able to speak Mandarin Chinese, and there's nothing about his software either, except that, well, what word can we use to say that he doesn't want to go through that process? I mean, he does want to learn it, he does want to learn it, but he doesn't want to go through the process of being programmed with that program. But if his circumstances changed, he might well want to.\\n\\nSo, for example, many of my relatives a couple of generations ago were forced to migrate to very alien places where they had to learn languages that they never thought they would ever speak and never wanted to speak. And yet very quickly, they did speak those languages. Again, was it because what they wanted changed? In the big picture, perhaps you could say what they wanted changed. So if your driving license blind people wanted to be educated to read driving licenses, in the sense that my ancestors wanted to learn languages, then yes they could learn that there is a level of dysfunction, below which they couldn't, and I think those are hardware limitations on the borderline I like the question of, could apes be programmed with a fully human intellect. I think the answer to that is yes, but it would require intricate changes at the neuron level and that's so that\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\", but it seems to be in an expanded universe are, there will be like a finite amount of total matter that will ever exist in our light cone right there's a limit. And that means that there's a limit on the amount of computation that this matter can, you know, execute the amount of energy can provide, perhaps even the amount of economic value we can sustain right so we, it would be weird if the GDP per atom could be arbitrarily large. So, does this impose some sort of limit on your concept of the beginning of infinity.\\n\\n1 0:50:32 So, what you've just recounted is a cosmological theory. This, this, the universe could be like that. know very little about the universes in the large, like theories of cosmology are changing on a time scale of about a decade. So it doesn't make all that much sense to speculate about what the ultimate asymptotic form of very small things like we know that our conception of physical processes must break down somehow at the level of quantum gravity, like 10 to the minus 42 seconds and that kind of thing but, but we have no idea what happens below There's no, there's no argument for that at all. It's just that we don't know what happens beyond that. Now what happens beyond that may be a finite limit. from it's being imposed by inherent hardware limitations. For example, if there's a finite amount of GNP available in the distant future, then it's still up to us whether we spend that on mathematics or music or political systems or any of the thousands of even more worthwhile things that have yet to be invented. So it's up to us which ideas we fill the 10 to the 10 to the 10 to the 10 bits with. Now, my guess is that there are no such limits, but my worldview is not affected by whether there are such limits, because as I said, it's still up to us what to fill them with. And then if we get chopped off at some point in the future, then everything will have been worthwhile up to then.\\n\\n2 0:53:07 Gotcha. Okay, so the way I understand your concept of gaining infinity, it seems to me that the more knowledge we gain, the more knowledge we're in a position to gain. So there should be like an exponential growth of knowledge. But if we look at the last 50 years, it seems that there's been a slowdown in or decrease in research productivity, economic growth, productivity growth. And this seems compatible with the story that, you know, that there's a limited amount of fruit on the tree that we picked the low hanging fruit. And now there's less and less fruit and harder and harder fruit to pick, and you know eventually well the orchard will be empty. So do you have an alternative explanation for what's going on\\n\\n1 0:53:45 in the last 50 years? Yes, I think it's very simple. There are sociological factors in academic life which have stultified the culture, and not totally and not everywhere, but that has been a tendency in what has happened, and it has resulted in a loss of productivity in many sectors, in many ways, but not in every sector, uh... the the uh... for example i i think that was a i've often said there was a stultification in uh... theoretical physics\\n\\n5\\n\\n0:54:35\\n\\nuh...\\n\\n1 0:54:36 starting in let's say the nineteen twenties and it still hasn't fully dissipated if it wasn't for that quantum computers would have been invented in the 1930s and built in the 1960s. So that is just an accidental fact, but it just goes to show that there are no guarantees. The fact that our horizons are unlimited, does not guarantee that we won't start declining tomorrow. I don't think we are currently declining. I think the these declines that we see are parochial effects caused by specific mistakes that\\n\\n2 0:55:24 have been made and which can be undone. Okay, so I want to ask you a question about Bayesianism versus Popperianism. So one reason why people prefer base is because there seems to be a way of describing changes and changes in epic static status, when their relative status of a theory hasn't changed so give you an example. Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestrials can comprehend something beyond quantum mechanics, which we can experience love, and an AGI couldn't experience love because it has to do with our hardware, not just memory and speed, but specialized hardware. And I think that falls victim to the same argument. The thing is, this specialized hardware can't be anything except a computer. And if there's hardware that is needed for love, let's say that somebody is born without that hardware, then that hardware, that bit of the brain that does love or that does mathematical insight or whatever, it's just a bit of the brain and it's connected to the rest of the brain in the same way that the other part of the brain is connected to the rest of the brain, namely by neurons passing electrical signals and by chemicals whose concentrations are altered and so on.\\n\\nSo therefore an artificial device that computed which signals were to be sent and which chemicals were to be adjusted could do the same job and it would be indistinguishable and therefore a person augmented with one of those who couldn't feel love could feel love after that augmentation. So those are and I think that a GIS and humans have the same range in the sense I've defined.\\n\\n2 0:06:14 Okay, interesting. Okay, so I think the software question is more interesting than the harder one immediately but I do want to take issue with the idea that the memory and speed of human brains can be arbitrarily and easily expanded, but we can get into that later. We can just start with this question. Can all humans explain everything that even the smartest humans can explain right so if I took the village idiot, and I asked him to create the theory of quantum computing. a reference about 21 to 24% of Americans on the National Belt Literacy Survey, they fall in level one, which means that they can't even perform basic tasks like identifying the expiry date of a driver's license, for example, or totaling a bank deposit slip. So are these humans capable of explaining quantum computing or creating the Deutsch-Jozsa algorithm? And if they're not capable of doing this, doesn't that mean that the theory of universal explainers falls apart?\\n\\n1 0:07:15 Well, there are people who... So, these tasks that you're talking about are tasks that no ape could do. However, there are humans who are brain damaged to the extent that they can't even do the tasks that an ape can do. And there comes a point when installing the program that would And then installing the program that would, you know, be able to read the driver's license or whatever would require augmenting their hardware as well as their software. So if, if a person that we don't know that might we don't know enough about the brain. of the population then it's definitely not hardware. So I would say that for those people it's definitely software. If it was hardware then getting them to do this would be a matter of repairing the imperfect hardware. If it's software it is not just a matter of whether the existing software is, what word can I use instead of wants to, but he will never be able to speak Mandarin Chinese, and there's nothing about his software either, except that, well, what word can we use to say that he doesn't want to go through that process? I mean, he does want to learn it, he does want to learn it, but he doesn't want to go through the process of being programmed with that program. But if his circumstances changed, he might well want to.\\n\\nSo, for example, many of my relatives a couple of generations ago were forced to migrate to very alien places where they had to learn languages that they never thought they would ever speak and never wanted to speak. And yet very quickly, they did speak those languages. Again, was it because what they wanted changed? In the big picture, perhaps you could say what they wanted changed. So if your driving license blind people wanted to be educated to read driving licenses, in the sense that my ancestors wanted to learn languages, then yes they could learn that there is a level of dysfunction, below which they couldn't, and I think those are hardware limitations on the borderline I like the question of, could apes be programmed with a fully human intellect. I think the answer to that is yes, but it would require intricate changes at the neuron level and that's so that\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" our knowledge of, of grammar is my was my favorite example, as we know why certain sentences are acceptable and why they're unacceptable, but we can't state explicitly, or in every case, why it isn't or why it is. And then there's explicit and inexplicit knowledge, there's conscious and unconscious knowledge. All those are bits of program in the brain, they're ideas, they are bits of knowledge in this, if you define knowledge as information with causal power, they are all information with causal power. They all contain truth and they all contain error and it's always a mistake to shield something, to shield one of them from criticism or replacement.\\n\\nNot doing that is what I call the fun criterion. Now you might say that's a bad name, but you know, it's the best I can find.\\n\\n2 1:14:11 So why would creating an AGI through evolution necessarily entail suffering? Because the way I see it, or it seems to me your theory is that you need to be a general intelligence in order to feel suffering, but by the point an evolved, simulated being is a general intelligence, we can just stop the simulation.\\n\\n1 1:14:31 And so where is the suffering coming from? Okay, so the kind of simulation by evolution that I'm thinking of, I mean, there may be several kinds, but the kind that I'm thinking of, and which I said would be the greatest crime in history, is the kind that just simulates the actual evolution of humans from pre humans that weren't people. So you have you have a population of non people which in this simulation would be some kind of NPCs. And then they would they would just evolve we don't know what the criterion would be we just have an artificial universe which simulated the surface of the earth and they'd be walking around and some of them might or might not become people. And now the thing is when you're part of the way there, what is happening is that you have the way that I, the only way that I can imagine the evolution of personhood or create the explanatory creativity happened was that the hardware needed for it was first needed for something else. I have proposed that it was needed to transmit memes. So there'd be people who were transmitting memes creatively but they were running out of resources before it managed to increase their stock of memes. So in every generation, there was a stock of memes that was being passed down to the next generation. And once they got beyond a certain complexity, they had to be passed down by the use of creativity by the recipient.\\n\\nSo, there may well have been a time and as I say I can't think of any other way it could have been, where there was genuine creativity being used, but it ran out of resources, very quickly, but not so quickly that it didn't increase the mean bandwidth. Then in the next generation, there was more mean bandwidth, and then after, you know, certain number of generations, there would have been some opportunity to use this hardware or whatever it is, you know, firmware, I expect, to use this firmware for something other than just trying blindly transmitting memes or rather creatively transmitting memes, but they were blind memes. So in that time, it would have been very unpleasant to be alive. It was already very unpleasant to, sorry, it was very unpleasant to be alive when we did have enough resources to think as well as do the memes. But I don't think there would have been a moment at which you would say, yes, now the suffering begins to matter because it's not just blind memes. I think the people were already suffering at the time when they were blindly transmitting memes because they were using genuine creativity. They were just not using it to any good effect.\\n\\n2 1:17:56 Gotcha. Would being in the experience machine be compatible with the fun criterion so you're not aware that you're in, you know, the experience machine it's all virtual reality, but you're still doing the things that would make you have fun in fact more so than in the real world.\\n\\n1 1:18:15 So, would you be tempted to get into experience machine, what do we can put a compatible with the fun criteria, I mean, is it just a virtual reality world in which things work better than in the real world or something?\\n\\n2 1:18:35 Yeah, so it's a thought experiment by Robert Nozick, and the idea is that you would enter this world, but you would forget that you're in virtual reality. So all, I mean, the world would be perfect in every possible way that it could be perfect or not perfect, but it would be better in every possible way it could be better. But you would think the relationships you have here are real, the knowledge you're discovering here is novel\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" than to shake itself about until it gets out. It doesn't have a concept of doing something which temporarily makes matters worse and then allows you to get\\n\\n2 0:33:10 I think dogs can't do that. But it's just, it's not because that's a particularly complicated thing, it's just that it's programming just doesn't have that. But an animal's programming easily could have that if it lived in an environment in which that happened a lot of power. So if, for example, I wrote a deep learning program, I traded over financial history, and I asked it, make me a trillion dollars on the stock market. Do you think that this would be impossible? And if you think this would be possible, then it seems like I do, it's not an AGI, but it seems like a very powerful\\n\\n1 0:33:56 AI, right? So it seems like AI is getting somewhere. Yeah, well, if you want to be powerful, you might do better inventing a weapon or something. But, but, or, or a better mousetrap is even better, because it's non violent so you can invent a paperclip. To use an example, it's often used in this context, you can invent it, if paperclips hadn't been invented, you can invent a paperclip and make a fortune. And that's an idea, which is, but it's not an AI, because it's not the paperclip that's going out there. It's really your idea in the first place, that has caused the whole value of the paperclip. And similarly, if you invent a dumb arbitrage machine, which seeks out complicated trades to make, which are more complicated than anyone else is trying to do. And that makes you a fortune. Well, the thing that made you a fortune was not the arbitrage machine, it was your idea for how to search for arbitrage opportunities that no one else sees.\\n\\n7\\n\\n0:35:08\\n\\nRight.\\n\\n6\\n\\n0:35:08\\n\\nThat's what was valuable.\\n\\n1 0:35:10 And that's the usual way of making money in the economy. You have an idea, and then you implement it. Right. That was an AI is beside the point. It could have been a paper clip.\\n\\n2 0:35:24 But the thing is, so the models that are used nowadays are not expert systems like the chess engines of the 90s. Something like AlphaZero or AlphaGo, this is just like almost a blank neural net. And they were able to help let it win go. Or so if such a neural network that was kind of blank, and if you just arbitrarily throw a financial history at it, wouldn't it be fair to say that the AI actually figured out what the right trades were? Even though it's not a general intelligence?\\n\\n1 0:35:52 Well, I think it's possible in chess, but not in the economy, because the value in the economy is being created by creativity and most, you know, arbitrage is one thing that can sort of skim value off the top by taking opportunities that were too expensive for other people to take. So you can, you know, you can make money, you make a lot of money that way if you know, if you have a good idea about how to do it. But most of the value in the economy is created by the creation of knowledge. Somebody has the idea that a smartphone would be good to have, even though most people think that that's not going to work. And that idea cannot be anticipated by anything less than an AGI. An AGI could have that idea, but no AI could. Okay.\\n\\n2 0:36:48 So there's definitely other topics I want to get to. So let's talk about virtual reality. So in the fabric of reality, you discuss the possibility that virtual reality generators could plug in directly into our nervous system and give us sense data that way. Now, as you might know, many meditators, you know, people like Sam Harris, speak of both thoughts and senses as intrusions into consciousness that have a sort of similar, they can be welcome intrusions, but they are both things that come into consciousness. So, do you think that a virtual reality generator could also place thoughts, as well as sense data,\\n\\n1 0:37:24 into the mind? Yes, but that's only because I think that this model is wrong. It's basically the Cartesian theater, as Daniel Dennett puts it, with the stage cleared of all the characters. So, that's pure consciousness without content, as Sam Harris envisages it. But I think that all that's happening there is that you are conscious of this theater, and you\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestrials can comprehend something beyond quantum mechanics, which we can experience love, and an AGI couldn't experience love because it has to do with our hardware, not just memory and speed, but specialized hardware. And I think that falls victim to the same argument. The thing is, this specialized hardware can't be anything except a computer. And if there's hardware that is needed for love, let's say that somebody is born without that hardware, then that hardware, that bit of the brain that does love or that does mathematical insight or whatever, it's just a bit of the brain and it's connected to the rest of the brain in the same way that the other part of the brain is connected to the rest of the brain, namely by neurons passing electrical signals and by chemicals whose concentrations are altered and so on.\\n\\nSo therefore an artificial device that computed which signals were to be sent and which chemicals were to be adjusted could do the same job and it would be indistinguishable and therefore a person augmented with one of those who couldn't feel love could feel love after that augmentation. So those are and I think that a GIS and humans have the same range in the sense I've defined.\\n\\n2 0:06:14 Okay, interesting. Okay, so I think the software question is more interesting than the harder one immediately but I do want to take issue with the idea that the memory and speed of human brains can be arbitrarily and easily expanded, but we can get into that later. We can just start with this question. Can all humans explain everything that even the smartest humans can explain right so if I took the village idiot, and I asked him to create the theory of quantum computing. a reference about 21 to 24% of Americans on the National Belt Literacy Survey, they fall in level one, which means that they can't even perform basic tasks like identifying the expiry date of a driver's license, for example, or totaling a bank deposit slip. So are these humans capable of explaining quantum computing or creating the Deutsch-Jozsa algorithm? And if they're not capable of doing this, doesn't that mean that the theory of universal explainers falls apart?\\n\\n1 0:07:15 Well, there are people who... So, these tasks that you're talking about are tasks that no ape could do. However, there are humans who are brain damaged to the extent that they can't even do the tasks that an ape can do. And there comes a point when installing the program that would And then installing the program that would, you know, be able to read the driver's license or whatever would require augmenting their hardware as well as their software. So if, if a person that we don't know that might we don't know enough about the brain. of the population then it's definitely not hardware. So I would say that for those people it's definitely software. If it was hardware then getting them to do this would be a matter of repairing the imperfect hardware. If it's software it is not just a matter of whether the existing software is, what word can I use instead of wants to, but he will never be able to speak Mandarin Chinese, and there's nothing about his software either, except that, well, what word can we use to say that he doesn't want to go through that process? I mean, he does want to learn it, he does want to learn it, but he doesn't want to go through the process of being programmed with that program. But if his circumstances changed, he might well want to.\\n\\nSo, for example, many of my relatives a couple of generations ago were forced to migrate to very alien places where they had to learn languages that they never thought they would ever speak and never wanted to speak. And yet very quickly, they did speak those languages. Again, was it because what they wanted changed? In the big picture, perhaps you could say what they wanted changed. So if your driving license blind people wanted to be educated to read driving licenses, in the sense that my ancestors wanted to learn languages, then yes they could learn that there is a level of dysfunction, below which they couldn't, and I think those are hardware limitations on the borderline I like the question of, could apes be programmed with a fully human intellect. I think the answer to that is yes, but it would require intricate changes at the neuron level and that's so that\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" is correct and another thing isn't correct it will talk nonsense and yet it knows which of them is correct. I've seen several cases of that and I was very impressed because it means that it's knowledge of language. It's a thing that I don't actually understand how it can be as good at speaking English, writing English, as it is. It's much better than most people, but more interestingly, it's much better than the average or the typical text on the internet. So it makes mistakes, but it makes far fewer mistakes than a typical text on the internet. Do you think this is going to revolutionise the economy and I can't prophesy the applications of modern AIs, chatbots and so on. I have found, for what it's worth, as it were, I have found it useful but not revolutionary in my own work and in my writing and whatever it is useful but I can't see possibility for it to revolutionize what I do. Whether it can revolutionize the economy depends on something slightly different because for that it doesn't need to really have a fundamental new functionality. It could be that a lot of existing jobs, and people are scared that computer programming is one of them, where only a proportion of the job, let's say 10%, of a particular programming job involves human creativity, and the rest is basically hack work.\\n\\nwhich is a big if, because I'm not convinced of this either. If chatbots can reliably perform the hack work, then it might be argued, I think again wrongly, that if a given task can be done with only a tenth as much work, then we might need only a tenth as many programmers in the long run.\\n\\nLulie And unfortunately we got interrupted, so David never finished his thought about why programmer jobs might be safer. But if you have any questions about anything in this episode, leave them jobs might be safer. But if you have any questions about anything in this episode, leave them on the tweet, which I will link in the show notes about this episode. Thank you.2 0:00:00 Okay, today I'm speaking with David Deutsch. Now this is a conversation that I've been eagerly wanting to have for years so this is very exciting for me. So, first let's talk about AI. Can you briefly explain why you anticipate that AI's will be no more fundamentally intelligent than humans.\\n\\n1 0:00:19 I suppose you mean capable of all the same types of cognition as humans are in principle.\\n\\n4\\n\\n0:00:37\\n\\nYes.\\n\\n1 0:00:38 So that would include, you know, doing science and doing art and in principle also falling in love and being good and being evil and all that. So the reason is twofold and one half is about computation hardware, computation hardware, and the other is about hardware. We know that our, our brains are Turing complete bits of hardware, and therefore can exhibit the functionality of running any computable function program for any computable function. Now, when I say any, I don't really mean any, because you and I sitting here, you know, we're having conversation and we could say, you know, we could have any conversation. Well, we can assume that maybe in a hundred years time, we'll both be dead. And therefore, the number of conversations we could have is strictly limited. And also, some conversations depend on speed of computation. So, you know, if we're going to be solving the traveling salesman problem, then there are there are many traveling salesman problems that we wouldn't be able to solve in the programs we can run, apart from by speed and memory capacity. So all limitations on us, hardware limitations on us, boil down to speed and memory capacity. And both those can be augmented to the level of any other entity that is in the universe. Because, you know, if somebody builds a computer that can think faster than the brain, then we can use that very computer or that very technology to make our thinking go just as fast as that. So that's the hardware.\\n\\nAs far as explanations go, can we reach the same kind of explanations as any other entity, let's say, usually this is said not in terms of AGIs but in terms of extraterrestrial intelligences, but also it's said about AGIs, you know, what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestri\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestrials can comprehend something beyond quantum mechanics, which we can experience love, and an AGI couldn't experience love because it has to do with our hardware, not just memory and speed, but specialized hardware. And I think that falls victim to the same argument. The thing is, this specialized hardware can't be anything except a computer. And if there's hardware that is needed for love, let's say that somebody is born without that hardware, then that hardware, that bit of the brain that does love or that does mathematical insight or whatever, it's just a bit of the brain and it's connected to the rest of the brain in the same way that the other part of the brain is connected to the rest of the brain, namely by neurons passing electrical signals and by chemicals whose concentrations are altered and so on.\\n\\nSo therefore an artificial device that computed which signals were to be sent and which chemicals were to be adjusted could do the same job and it would be indistinguishable and therefore a person augmented with one of those who couldn't feel love could feel love after that augmentation. So those are and I think that a GIS and humans have the same range in the sense I've defined.\\n\\n2 0:06:14 Okay, interesting. Okay, so I think the software question is more interesting than the harder one immediately but I do want to take issue with the idea that the memory and speed of human brains can be arbitrarily and easily expanded, but we can get into that later. We can just start with this question. Can all humans explain everything that even the smartest humans can explain right so if I took the village idiot, and I asked him to create the theory of quantum computing. a reference about 21 to 24% of Americans on the National Belt Literacy Survey, they fall in level one, which means that they can't even perform basic tasks like identifying the expiry date of a driver's license, for example, or totaling a bank deposit slip. So are these humans capable of explaining quantum computing or creating the Deutsch-Jozsa algorithm? And if they're not capable of doing this, doesn't that mean that the theory of universal explainers falls apart?\\n\\n1 0:07:15 Well, there are people who... So, these tasks that you're talking about are tasks that no ape could do. However, there are humans who are brain damaged to the extent that they can't even do the tasks that an ape can do. And there comes a point when installing the program that would And then installing the program that would, you know, be able to read the driver's license or whatever would require augmenting their hardware as well as their software. So if, if a person that we don't know that might we don't know enough about the brain. of the population then it's definitely not hardware. So I would say that for those people it's definitely software. If it was hardware then getting them to do this would be a matter of repairing the imperfect hardware. If it's software it is not just a matter of whether the existing software is, what word can I use instead of wants to, but he will never be able to speak Mandarin Chinese, and there's nothing about his software either, except that, well, what word can we use to say that he doesn't want to go through that process? I mean, he does want to learn it, he does want to learn it, but he doesn't want to go through the process of being programmed with that program. But if his circumstances changed, he might well want to.\\n\\nSo, for example, many of my relatives a couple of generations ago were forced to migrate to very alien places where they had to learn languages that they never thought they would ever speak and never wanted to speak. And yet very quickly, they did speak those languages. Again, was it because what they wanted changed? In the big picture, perhaps you could say what they wanted changed. So if your driving license blind people wanted to be educated to read driving licenses, in the sense that my ancestors wanted to learn languages, then yes they could learn that there is a level of dysfunction, below which they couldn't, and I think those are hardware limitations on the borderline I like the question of, could apes be programmed with a fully human intellect. I think the answer to that is yes, but it would require intricate changes at the neuron level and that's so that\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" they wanted changed. So if your driving license blind people wanted to be educated to read driving licenses, in the sense that my ancestors wanted to learn languages, then yes they could learn that there is a level of dysfunction, below which they couldn't, and I think those are hardware limitations on the borderline I like the question of, could apes be programmed with a fully human intellect. I think the answer to that is yes, but it would require intricate changes at the neuron level and that's so that to transfer the program from a human mind into the ape's mind. I would guess that that is possible because although the ape has far less memory space than humans do and also doesn't have certain specialized modules that humans have. Neither of those things is a thing that we use to the full anyway.\\n\\nI mean, when I'm speaking to you now, there's a lot of knowledge in my brain that I'm not referring to at all, like, you know, the fact that I can play the piano or drive a car is not being used in this conversation so I don't think the fact that we have such a large memory capacity would affect this, this project or the project would be highly immoral, because you'd be intentionally creating a, a person inside\\n\\n2 0:12:22 a deficient brain hardware. So suppose it's Harvard differences that distinguish, you know, different humans in terms of their intelligence. If it were just up to the people who are not even functionally literate right so these are again people wait wait wait. level of using up the whole of our allocation of memory or speed or whatever. So, right, but it can be hardware. By the way, a software analogous or is an arm is hardware synonymous with genetic influences for you or it can software be genetic to software can be genetic to, though, though, that doesn't mean it's there at the beginning. Okay. The reason I suspect it's not software is because these people also happen to be the same people who, let's suppose it was software and something that they chose to do or something they could change. It's mysterious to me why these people would also choose to accept jobs that have lower pay but are less cognitively demanding, or why they would choose to do exactly the sort of thing somebody who's less cognitively powerful would do. It seems the more parsimonious explanation there is just that they\\n\\n1 0:13:46 are cognitively less powerful. Not at all. Why would someone choose not to go to school, for instance, if they were given the choice and not to have any lessons? Well, there are many reasons Some of them good, some of them bad. And the people who, you know, calling some jobs cognitively demanding is already begging the question, because you're just referring to a choice that people make, which I think is a software choice, as being by definition forced on them by hardware. It's not cognitively deficient. It's just that they don't want to do it. The same way, if there was a culture that required Brett Hall to be able to speak fluent Mandarin Chinese in order to do a wide range of tasks, and if he didn't know Mandarin Chinese, he'd be relegated to low-level tasks, then he would be quote, quote, choosing the low-level tasks rather than the quote, cognitively demanding task. But it's only culture that makes that cognitively demanding task that assigns a hardware interpretation to the difficulty of doing that task.\\n\\n2 0:15:12 Right. I mean, it doesn't seem that arbitrary to say that the kind of jobs you could do sitting down on a laptop are require a different cognitive require probably more cognition than the ones you can do in in a construction site. And if it's not if it's not cognition that distinguishes or if there's not something like intelligence or cognition or whatever you want to call it, that is a thing that is measured by both these literacy tests and by what you're doing at your job, then what is the explanation for why there's such a high correlation between people who are not functionally literate and or I guess an anti correlation between people who are not functionally literate and people who are doing it like let's say programmers right like I guarantee you people working at Apple, all of them are above level one on this literacy survey.\\n\\n1 0:15:52 Why did they just happen to make certain abilities, make use of certain abilities that people have. So, if you're setting up a company that is going to employ 10,000 employees, then it's best to make the way that the company works, you know, it's best for example to make the signs above the doors or the signs on the doors or the numbers on the dials all be ones that people in that culture who are highly educated can read. You could in principle make each\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.\\n\\nPerhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.\\n\\nAGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.\\n\\nIronically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’\\n\\n‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.\\n\\nSome, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science.)\\n\\nThat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non-cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or practical aim. This isn’t good. Never mind the terminology; change it if you like, and there are indeed reasons for treating various entities with respect, protecting them from harm and so on. All the same, the distinction between actual people, defined by that objective criterion, and other entities has enormous moral and practical significance, and is going to become vital to the functioning of a civilisation that includes AGIs.\\n\\nThe battle between good and evil ideas is as old as our species and will go on regardless of the hardware on which it is running\\n\\nFor example, the mere fact that it is not the computer but the running program that is a person, raises unsolved philosophical problems that will become practical, political controversies as soon as AGIs exist. Once an AGI program is running in a computer, to deprive it of that computer would be murder (or at least false imprisonment or slavery, as the case may be), just like depriving a human mind of its body. But unlike a human body, an AGI program can be copied into multiple computers at the touch of a button. Are those programs, while they are still executing identical steps (ie before they have become differentiated due to random choices or different experiences), the same person or many different people? Do they get one vote, or many? Is deleting one of them murder, or a minor assault? And if some rogue programmer, perhaps illegally, creates billions of different AGI people, either on one computer or on many, what happens next? They are still people, with rights. Do they all get the vote?\\n\\nFurthermore, in regard to AGIs, like any other entities with creativity, we have to forget almost all existing connotations of the word ‘programming’. To treat AGIs like any other computer programs would constitute brainwashing, slavery, and tyranny. And cruelty to children, too, for ‘programming’ an already-running AGI, unlike all other programming, constitutes education. And it constitutes debate, moral as well as factual. To ignore the rights and personhood of AGIs would not only be the epitome of evil, but also a recipe for disaster: creative beings cannot be enslaved forever.\\n\\nSome people are wondering whether we should welcome our new robot overlords. Some hope to learn how we can rig their programming to make them constitutionally unable to harm humans (as in Isaac Asimov’s ‘laws of robotics’), or to prevent them from acquiring the theory that the universe should be converted into paper clips (as imagined by Nick Bostrom). None of these are the real problem. It has always been the case that a single exceptionally creative person can be thousands of times as productive — economically, intellectually or whatever — as most people; and that such a person could do enormous harm were he to turn his powers to evil instead of good.\\n\\nThese phenomena have nothing to do with AGIs. The battle between good and evil ideas is as old as our species and will continue regardless of the hardware on which it is running. The issue is: we want the intelligences with (morally) good ideas always to defeat the evil intelligences, biological and artificial; but we are fallible, and our own conception of ‘good’ needs continual improvement. How should society be organised so as to promote that improvement? ‘Enslave all intelligence’ would be a catastrophically wrong answer, and ‘enslave all intelligence that doesn’t look like us’ would not be much better.\\n\\nOne implication is that we must stop regarding education (of humans or AGIs alike) as instruction — as a means of transmitting existing knowledge unaltered, and causing existing values to be enacted obediently. As Popper wrote (in the context of scientific discovery, but it applies equally to the programming of AGIs and the education of children): ‘there is no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AG', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.\\n\\nPerhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.\\n\\nAGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.\\n\\nIronically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’\\n\\n‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.\\n\\nSome, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science.)\\n\\nThat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non-cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" Why did they just happen to make certain abilities, make use of certain abilities that people have. So, if you're setting up a company that is going to employ 10,000 employees, then it's best to make the way that the company works, you know, it's best for example to make the signs above the doors or the signs on the doors or the numbers on the dials all be ones that people in that culture who are highly educated can read. You could in principle make each label on each door a different language, I don't know, you know, there are thousands of human languages, let's say there are 5,000 languages and 5,000 doors in the company, you could, given the same meaning, make them all different languages. The reason that they're all the same language and what's more, not just any old language, it's a language that many educated people know fluently. That's why. And then you can misinterpret that as saying, oh, there is some hardware reason why everybody speaks the same language. Well, no, there isn't. It's a cultural reason.\\n\\n2 0:17:22 Okay, so if the culture was different somehow, maybe if there was some other way of communicating ideas, do you think that the people who are currently designated as not functionally literate could be in a position to learn about quantum computing, for example, if they made the right choices are not the right choices but the choices that could lead to them understanding quantum computing.\\n\\n1 0:17:49 Well, rather begs the question. It's not only language that is like this, it's all knowledge. So, just learning, so if someone doesn't speak English, quantum computing is a field in which English is the standard language. Used to be German, now it's English. Now, someone who doesn't know English is at a disadvantage learning about quantum computers, but not only because of their deficiency in language. If they come from a culture in which the culture of physics and of mathematics and and of logic and so on is is is equivalent and only the language is different, then if they just learn the language, they will find it as easy as anyone else. But if a whole load of things are different, if a person doesn't think in terms of, for example, logic, but thinks in terms of pride and manliness and fear and, you know, all sorts of concepts that fill the lives of, let's say, prehistoric people or pre-enlightenment people, then to be able to understand quantum computers they would have to learn a lot more than just the language of the civilization. They'd have to learn all of other, well not all, but a range of other features of the civilization. And on that basis, the people who can't read driving licenses are similarly in a different culture, which they would also have to learn if they are to increase their IQ, i.e. their ability to function at a high level in intellectual culture in our civilization. Okay, they would be able to.\\n\\n2 0:20:07 Okay. So if it's those kinds of differences, then how do you explain the fact that identical twins separated at birth and adopted by different families, they tend to have the most of the variance that does exist between humans in terms of IQ doesn't exist between identical twins. In fact, the correlation is point eight, which is the correlation that you, you would have when you took the test on different days like depending on how well good a day you were having. And these are, you know, people who are adopted by different families.\\n\\n1 0:20:51 The hardware theory explains it in the sense that it might be hardware might be true. So, it doesn't, it doesn't have an explanation beyond that, and nor the software theory.\\n\\n2 0:21:04 Sorry, go on. I mean, so there are actually like differences at the level of brain that are correlated with IQ right so you actual skull size is like a point three correlation with IQ. There's a few more like this they don't explain the entire the entire variance in human intelligence or the entire genetic variance in human intelligence, but we do have, we have identified a few actual Harvard differences that correlate.\\n\\n1 0:21:30 Suppose, suppose, on the contrary, that suppose that the result was that people who are brought up in the same family and differ only in the amount of hair they have or in their appearance in any other way, that none of those differences make any difference to their IQ. Only who their parents were makes a difference. Now, wouldn't that be surprising? Wouldn't it be surprising that there's nothing else correlated with IQ other than who your parents are?\\n\\n4\\n\\n0:22:21\\n\\nYes.\\n\\n1 0:\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" they wanted changed. So if your driving license blind people wanted to be educated to read driving licenses, in the sense that my ancestors wanted to learn languages, then yes they could learn that there is a level of dysfunction, below which they couldn't, and I think those are hardware limitations on the borderline I like the question of, could apes be programmed with a fully human intellect. I think the answer to that is yes, but it would require intricate changes at the neuron level and that's so that to transfer the program from a human mind into the ape's mind. I would guess that that is possible because although the ape has far less memory space than humans do and also doesn't have certain specialized modules that humans have. Neither of those things is a thing that we use to the full anyway.\\n\\nI mean, when I'm speaking to you now, there's a lot of knowledge in my brain that I'm not referring to at all, like, you know, the fact that I can play the piano or drive a car is not being used in this conversation so I don't think the fact that we have such a large memory capacity would affect this, this project or the project would be highly immoral, because you'd be intentionally creating a, a person inside\\n\\n2 0:12:22 a deficient brain hardware. So suppose it's Harvard differences that distinguish, you know, different humans in terms of their intelligence. If it were just up to the people who are not even functionally literate right so these are again people wait wait wait. level of using up the whole of our allocation of memory or speed or whatever. So, right, but it can be hardware. By the way, a software analogous or is an arm is hardware synonymous with genetic influences for you or it can software be genetic to software can be genetic to, though, though, that doesn't mean it's there at the beginning. Okay. The reason I suspect it's not software is because these people also happen to be the same people who, let's suppose it was software and something that they chose to do or something they could change. It's mysterious to me why these people would also choose to accept jobs that have lower pay but are less cognitively demanding, or why they would choose to do exactly the sort of thing somebody who's less cognitively powerful would do. It seems the more parsimonious explanation there is just that they\\n\\n1 0:13:46 are cognitively less powerful. Not at all. Why would someone choose not to go to school, for instance, if they were given the choice and not to have any lessons? Well, there are many reasons Some of them good, some of them bad. And the people who, you know, calling some jobs cognitively demanding is already begging the question, because you're just referring to a choice that people make, which I think is a software choice, as being by definition forced on them by hardware. It's not cognitively deficient. It's just that they don't want to do it. The same way, if there was a culture that required Brett Hall to be able to speak fluent Mandarin Chinese in order to do a wide range of tasks, and if he didn't know Mandarin Chinese, he'd be relegated to low-level tasks, then he would be quote, quote, choosing the low-level tasks rather than the quote, cognitively demanding task. But it's only culture that makes that cognitively demanding task that assigns a hardware interpretation to the difficulty of doing that task.\\n\\n2 0:15:12 Right. I mean, it doesn't seem that arbitrary to say that the kind of jobs you could do sitting down on a laptop are require a different cognitive require probably more cognition than the ones you can do in in a construction site. And if it's not if it's not cognition that distinguishes or if there's not something like intelligence or cognition or whatever you want to call it, that is a thing that is measured by both these literacy tests and by what you're doing at your job, then what is the explanation for why there's such a high correlation between people who are not functionally literate and or I guess an anti correlation between people who are not functionally literate and people who are doing it like let's say programmers right like I guarantee you people working at Apple, all of them are above level one on this literacy survey.\\n\\n1 0:15:52 Why did they just happen to make certain abilities, make use of certain abilities that people have. So, if you're setting up a company that is going to employ 10,000 employees, then it's best to make the way that the company works, you know, it's best for example to make the signs above the doors or the signs on the doors or the numbers on the dials all be ones that people in that culture who are highly educated can read. You could in principle make each\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestrials can comprehend something beyond quantum mechanics, which we can experience love, and an AGI couldn't experience love because it has to do with our hardware, not just memory and speed, but specialized hardware. And I think that falls victim to the same argument. The thing is, this specialized hardware can't be anything except a computer. And if there's hardware that is needed for love, let's say that somebody is born without that hardware, then that hardware, that bit of the brain that does love or that does mathematical insight or whatever, it's just a bit of the brain and it's connected to the rest of the brain in the same way that the other part of the brain is connected to the rest of the brain, namely by neurons passing electrical signals and by chemicals whose concentrations are altered and so on.\\n\\nSo therefore an artificial device that computed which signals were to be sent and which chemicals were to be adjusted could do the same job and it would be indistinguishable and therefore a person augmented with one of those who couldn't feel love could feel love after that augmentation. So those are and I think that a GIS and humans have the same range in the sense I've defined.\\n\\n2 0:06:14 Okay, interesting. Okay, so I think the software question is more interesting than the harder one immediately but I do want to take issue with the idea that the memory and speed of human brains can be arbitrarily and easily expanded, but we can get into that later. We can just start with this question. Can all humans explain everything that even the smartest humans can explain right so if I took the village idiot, and I asked him to create the theory of quantum computing. a reference about 21 to 24% of Americans on the National Belt Literacy Survey, they fall in level one, which means that they can't even perform basic tasks like identifying the expiry date of a driver's license, for example, or totaling a bank deposit slip. So are these humans capable of explaining quantum computing or creating the Deutsch-Jozsa algorithm? And if they're not capable of doing this, doesn't that mean that the theory of universal explainers falls apart?\\n\\n1 0:07:15 Well, there are people who... So, these tasks that you're talking about are tasks that no ape could do. However, there are humans who are brain damaged to the extent that they can't even do the tasks that an ape can do. And there comes a point when installing the program that would And then installing the program that would, you know, be able to read the driver's license or whatever would require augmenting their hardware as well as their software. So if, if a person that we don't know that might we don't know enough about the brain. of the population then it's definitely not hardware. So I would say that for those people it's definitely software. If it was hardware then getting them to do this would be a matter of repairing the imperfect hardware. If it's software it is not just a matter of whether the existing software is, what word can I use instead of wants to, but he will never be able to speak Mandarin Chinese, and there's nothing about his software either, except that, well, what word can we use to say that he doesn't want to go through that process? I mean, he does want to learn it, he does want to learn it, but he doesn't want to go through the process of being programmed with that program. But if his circumstances changed, he might well want to.\\n\\nSo, for example, many of my relatives a couple of generations ago were forced to migrate to very alien places where they had to learn languages that they never thought they would ever speak and never wanted to speak. And yet very quickly, they did speak those languages. Again, was it because what they wanted changed? In the big picture, perhaps you could say what they wanted changed. So if your driving license blind people wanted to be educated to read driving licenses, in the sense that my ancestors wanted to learn languages, then yes they could learn that there is a level of dysfunction, below which they couldn't, and I think those are hardware limitations on the borderline I like the question of, could apes be programmed with a fully human intellect. I think the answer to that is yes, but it would require intricate changes at the neuron level and that's so that\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" Why did they just happen to make certain abilities, make use of certain abilities that people have. So, if you're setting up a company that is going to employ 10,000 employees, then it's best to make the way that the company works, you know, it's best for example to make the signs above the doors or the signs on the doors or the numbers on the dials all be ones that people in that culture who are highly educated can read. You could in principle make each label on each door a different language, I don't know, you know, there are thousands of human languages, let's say there are 5,000 languages and 5,000 doors in the company, you could, given the same meaning, make them all different languages. The reason that they're all the same language and what's more, not just any old language, it's a language that many educated people know fluently. That's why. And then you can misinterpret that as saying, oh, there is some hardware reason why everybody speaks the same language. Well, no, there isn't. It's a cultural reason.\\n\\n2 0:17:22 Okay, so if the culture was different somehow, maybe if there was some other way of communicating ideas, do you think that the people who are currently designated as not functionally literate could be in a position to learn about quantum computing, for example, if they made the right choices are not the right choices but the choices that could lead to them understanding quantum computing.\\n\\n1 0:17:49 Well, rather begs the question. It's not only language that is like this, it's all knowledge. So, just learning, so if someone doesn't speak English, quantum computing is a field in which English is the standard language. Used to be German, now it's English. Now, someone who doesn't know English is at a disadvantage learning about quantum computers, but not only because of their deficiency in language. If they come from a culture in which the culture of physics and of mathematics and and of logic and so on is is is equivalent and only the language is different, then if they just learn the language, they will find it as easy as anyone else. But if a whole load of things are different, if a person doesn't think in terms of, for example, logic, but thinks in terms of pride and manliness and fear and, you know, all sorts of concepts that fill the lives of, let's say, prehistoric people or pre-enlightenment people, then to be able to understand quantum computers they would have to learn a lot more than just the language of the civilization. They'd have to learn all of other, well not all, but a range of other features of the civilization. And on that basis, the people who can't read driving licenses are similarly in a different culture, which they would also have to learn if they are to increase their IQ, i.e. their ability to function at a high level in intellectual culture in our civilization. Okay, they would be able to.\\n\\n2 0:20:07 Okay. So if it's those kinds of differences, then how do you explain the fact that identical twins separated at birth and adopted by different families, they tend to have the most of the variance that does exist between humans in terms of IQ doesn't exist between identical twins. In fact, the correlation is point eight, which is the correlation that you, you would have when you took the test on different days like depending on how well good a day you were having. And these are, you know, people who are adopted by different families.\\n\\n1 0:20:51 The hardware theory explains it in the sense that it might be hardware might be true. So, it doesn't, it doesn't have an explanation beyond that, and nor the software theory.\\n\\n2 0:21:04 Sorry, go on. I mean, so there are actually like differences at the level of brain that are correlated with IQ right so you actual skull size is like a point three correlation with IQ. There's a few more like this they don't explain the entire the entire variance in human intelligence or the entire genetic variance in human intelligence, but we do have, we have identified a few actual Harvard differences that correlate.\\n\\n1 0:21:30 Suppose, suppose, on the contrary, that suppose that the result was that people who are brought up in the same family and differ only in the amount of hair they have or in their appearance in any other way, that none of those differences make any difference to their IQ. Only who their parents were makes a difference. Now, wouldn't that be surprising? Wouldn't it be surprising that there's nothing else correlated with IQ other than who your parents are?\\n\\n4\\n\\n0:22:21\\n\\nYes.\\n\\n1 0:\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" they wanted changed. So if your driving license blind people wanted to be educated to read driving licenses, in the sense that my ancestors wanted to learn languages, then yes they could learn that there is a level of dysfunction, below which they couldn't, and I think those are hardware limitations on the borderline I like the question of, could apes be programmed with a fully human intellect. I think the answer to that is yes, but it would require intricate changes at the neuron level and that's so that to transfer the program from a human mind into the ape's mind. I would guess that that is possible because although the ape has far less memory space than humans do and also doesn't have certain specialized modules that humans have. Neither of those things is a thing that we use to the full anyway.\\n\\nI mean, when I'm speaking to you now, there's a lot of knowledge in my brain that I'm not referring to at all, like, you know, the fact that I can play the piano or drive a car is not being used in this conversation so I don't think the fact that we have such a large memory capacity would affect this, this project or the project would be highly immoral, because you'd be intentionally creating a, a person inside\\n\\n2 0:12:22 a deficient brain hardware. So suppose it's Harvard differences that distinguish, you know, different humans in terms of their intelligence. If it were just up to the people who are not even functionally literate right so these are again people wait wait wait. level of using up the whole of our allocation of memory or speed or whatever. So, right, but it can be hardware. By the way, a software analogous or is an arm is hardware synonymous with genetic influences for you or it can software be genetic to software can be genetic to, though, though, that doesn't mean it's there at the beginning. Okay. The reason I suspect it's not software is because these people also happen to be the same people who, let's suppose it was software and something that they chose to do or something they could change. It's mysterious to me why these people would also choose to accept jobs that have lower pay but are less cognitively demanding, or why they would choose to do exactly the sort of thing somebody who's less cognitively powerful would do. It seems the more parsimonious explanation there is just that they\\n\\n1 0:13:46 are cognitively less powerful. Not at all. Why would someone choose not to go to school, for instance, if they were given the choice and not to have any lessons? Well, there are many reasons Some of them good, some of them bad. And the people who, you know, calling some jobs cognitively demanding is already begging the question, because you're just referring to a choice that people make, which I think is a software choice, as being by definition forced on them by hardware. It's not cognitively deficient. It's just that they don't want to do it. The same way, if there was a culture that required Brett Hall to be able to speak fluent Mandarin Chinese in order to do a wide range of tasks, and if he didn't know Mandarin Chinese, he'd be relegated to low-level tasks, then he would be quote, quote, choosing the low-level tasks rather than the quote, cognitively demanding task. But it's only culture that makes that cognitively demanding task that assigns a hardware interpretation to the difficulty of doing that task.\\n\\n2 0:15:12 Right. I mean, it doesn't seem that arbitrary to say that the kind of jobs you could do sitting down on a laptop are require a different cognitive require probably more cognition than the ones you can do in in a construction site. And if it's not if it's not cognition that distinguishes or if there's not something like intelligence or cognition or whatever you want to call it, that is a thing that is measured by both these literacy tests and by what you're doing at your job, then what is the explanation for why there's such a high correlation between people who are not functionally literate and or I guess an anti correlation between people who are not functionally literate and people who are doing it like let's say programmers right like I guarantee you people working at Apple, all of them are above level one on this literacy survey.\\n\\n1 0:15:52 Why did they just happen to make certain abilities, make use of certain abilities that people have. So, if you're setting up a company that is going to employ 10,000 employees, then it's best to make the way that the company works, you know, it's best for example to make the signs above the doors or the signs on the doors or the numbers on the dials all be ones that people in that culture who are highly educated can read. You could in principle make each\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" our knowledge of, of grammar is my was my favorite example, as we know why certain sentences are acceptable and why they're unacceptable, but we can't state explicitly, or in every case, why it isn't or why it is. And then there's explicit and inexplicit knowledge, there's conscious and unconscious knowledge. All those are bits of program in the brain, they're ideas, they are bits of knowledge in this, if you define knowledge as information with causal power, they are all information with causal power. They all contain truth and they all contain error and it's always a mistake to shield something, to shield one of them from criticism or replacement.\\n\\nNot doing that is what I call the fun criterion. Now you might say that's a bad name, but you know, it's the best I can find.\\n\\n2 1:14:11 So why would creating an AGI through evolution necessarily entail suffering? Because the way I see it, or it seems to me your theory is that you need to be a general intelligence in order to feel suffering, but by the point an evolved, simulated being is a general intelligence, we can just stop the simulation.\\n\\n1 1:14:31 And so where is the suffering coming from? Okay, so the kind of simulation by evolution that I'm thinking of, I mean, there may be several kinds, but the kind that I'm thinking of, and which I said would be the greatest crime in history, is the kind that just simulates the actual evolution of humans from pre humans that weren't people. So you have you have a population of non people which in this simulation would be some kind of NPCs. And then they would they would just evolve we don't know what the criterion would be we just have an artificial universe which simulated the surface of the earth and they'd be walking around and some of them might or might not become people. And now the thing is when you're part of the way there, what is happening is that you have the way that I, the only way that I can imagine the evolution of personhood or create the explanatory creativity happened was that the hardware needed for it was first needed for something else. I have proposed that it was needed to transmit memes. So there'd be people who were transmitting memes creatively but they were running out of resources before it managed to increase their stock of memes. So in every generation, there was a stock of memes that was being passed down to the next generation. And once they got beyond a certain complexity, they had to be passed down by the use of creativity by the recipient.\\n\\nSo, there may well have been a time and as I say I can't think of any other way it could have been, where there was genuine creativity being used, but it ran out of resources, very quickly, but not so quickly that it didn't increase the mean bandwidth. Then in the next generation, there was more mean bandwidth, and then after, you know, certain number of generations, there would have been some opportunity to use this hardware or whatever it is, you know, firmware, I expect, to use this firmware for something other than just trying blindly transmitting memes or rather creatively transmitting memes, but they were blind memes. So in that time, it would have been very unpleasant to be alive. It was already very unpleasant to, sorry, it was very unpleasant to be alive when we did have enough resources to think as well as do the memes. But I don't think there would have been a moment at which you would say, yes, now the suffering begins to matter because it's not just blind memes. I think the people were already suffering at the time when they were blindly transmitting memes because they were using genuine creativity. They were just not using it to any good effect.\\n\\n2 1:17:56 Gotcha. Would being in the experience machine be compatible with the fun criterion so you're not aware that you're in, you know, the experience machine it's all virtual reality, but you're still doing the things that would make you have fun in fact more so than in the real world.\\n\\n1 1:18:15 So, would you be tempted to get into experience machine, what do we can put a compatible with the fun criteria, I mean, is it just a virtual reality world in which things work better than in the real world or something?\\n\\n2 1:18:35 Yeah, so it's a thought experiment by Robert Nozick, and the idea is that you would enter this world, but you would forget that you're in virtual reality. So all, I mean, the world would be perfect in every possible way that it could be perfect or not perfect, but it would be better in every possible way it could be better. But you would think the relationships you have here are real, the knowledge you're discovering here is novel\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" the result was that people who are brought up in the same family and differ only in the amount of hair they have or in their appearance in any other way, that none of those differences make any difference to their IQ. Only who their parents were makes a difference. Now, wouldn't that be surprising? Wouldn't it be surprising that there's nothing else correlated with IQ other than who your parents are?\\n\\n4\\n\\n0:22:21\\n\\nYes.\\n\\n1 0:22:23 Now, how much correlation should we expect? There are correlations everywhere. There are these things on the Internet which joke, memes or whatever you call it, but they make a serious point where they correlate things like how many adventure movies have been made in a given year correlated with how much the GNP per capita, and that's a bad example because there's an obvious relation, but you know what I mean. It's the number of films made by a particular actor against the number of outbreaks of bird flu. Part being surprised by randomness is the fact that correlations are everywhere. It's not just that correlation isn't causation, it's that correlations are everywhere. It's not a rare event to get a correlation between two things, and the more things you ask about, the more you are going to get correlations. So, the, the, it's, it's not, what is surprising is that the things that are correlated are, are things that you expect to be correlated and measured. For example, when they do these twin studies and measure the IQ, they control for certain things. And like you said, identical twins reared together, they've got to be reared together. Or apart. Or apart, yes. But there's infinitely more things that they don't control for. So, it could be that the real determinant of IQ is, for example, how well a child is treated between the ages of three and a half and four and a half, where well is defined by something that we don't know yet, but you know, something like that.\\n\\nThen you would expect that thing, which we don't know about and nobody has bothered to control for in these experiments, we would expect that thing to be correlated with IQ. But unfortunately that thing is also correlated with whether something, someone's an identical twin or not. twinness that is causing the similarity, it's this other thing. Right. This, say, an aspect of appearance or something. And if you were to surgically change a person with a view, if you knew what this thing was and surgically changed the person, you would be able to have the same effect as making an identical twin would have. Right.\\n\\n2 0:25:31 But, I mean, as you say, in science, or to explain any phenomenon, there's an infinite amount of possible explanations, right? You got to pick the best one. So it could be that there's some unknown trait which is so obvious to adopted parents, different adoptive parents, that they can use it as a basis for discrimination or for different treatment. But that is... I mean, I would assume they don't know what it is. But then aren't they using it as a basis to treat kids differently at the age of three, for example?\\n\\n1 0:25:58 Not by consciously identifying it. It's like, it would be something like getting the idea that this child is really smart. But I'm just trying to show you that it could be something that the parents are not aware of. If you ask parents to list the traits in their children that cause them to behave differently towards their children, they might list like 10 traits. But then there are another thousand traits that they're not aware of which also affect their behavior.\\n\\n2 0:26:30 So, we first need an explanation for what this trait is that researchers have not been able to identify it but it's so obvious that even unconsciously parents are able to reliably use it as a way to have to be obvious at all because parents\\n\\n1 0:26:47 of information about their children which they are processing in their minds and most of it they\\n\\n2 0:26:57 don't know what it is. Okay, all right. Okay, so I guess let's leave this topic aside for now and then let me bring us to animals. So if creativity is something that doesn't exist in increments, it's or you know the capacity to create explanations, you can just use a simple example go on YouTube and look up cat opening a door right so you'll see for example, a cat develops a theory that applying torque to this handle to this metal thing will open door. Now it hasn't. And then what it'll do is it'll climb onto a countertop and it'll jump on top of that door handle. It hasn't\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" Why did they just happen to make certain abilities, make use of certain abilities that people have. So, if you're setting up a company that is going to employ 10,000 employees, then it's best to make the way that the company works, you know, it's best for example to make the signs above the doors or the signs on the doors or the numbers on the dials all be ones that people in that culture who are highly educated can read. You could in principle make each label on each door a different language, I don't know, you know, there are thousands of human languages, let's say there are 5,000 languages and 5,000 doors in the company, you could, given the same meaning, make them all different languages. The reason that they're all the same language and what's more, not just any old language, it's a language that many educated people know fluently. That's why. And then you can misinterpret that as saying, oh, there is some hardware reason why everybody speaks the same language. Well, no, there isn't. It's a cultural reason.\\n\\n2 0:17:22 Okay, so if the culture was different somehow, maybe if there was some other way of communicating ideas, do you think that the people who are currently designated as not functionally literate could be in a position to learn about quantum computing, for example, if they made the right choices are not the right choices but the choices that could lead to them understanding quantum computing.\\n\\n1 0:17:49 Well, rather begs the question. It's not only language that is like this, it's all knowledge. So, just learning, so if someone doesn't speak English, quantum computing is a field in which English is the standard language. Used to be German, now it's English. Now, someone who doesn't know English is at a disadvantage learning about quantum computers, but not only because of their deficiency in language. If they come from a culture in which the culture of physics and of mathematics and and of logic and so on is is is equivalent and only the language is different, then if they just learn the language, they will find it as easy as anyone else. But if a whole load of things are different, if a person doesn't think in terms of, for example, logic, but thinks in terms of pride and manliness and fear and, you know, all sorts of concepts that fill the lives of, let's say, prehistoric people or pre-enlightenment people, then to be able to understand quantum computers they would have to learn a lot more than just the language of the civilization. They'd have to learn all of other, well not all, but a range of other features of the civilization. And on that basis, the people who can't read driving licenses are similarly in a different culture, which they would also have to learn if they are to increase their IQ, i.e. their ability to function at a high level in intellectual culture in our civilization. Okay, they would be able to.\\n\\n2 0:20:07 Okay. So if it's those kinds of differences, then how do you explain the fact that identical twins separated at birth and adopted by different families, they tend to have the most of the variance that does exist between humans in terms of IQ doesn't exist between identical twins. In fact, the correlation is point eight, which is the correlation that you, you would have when you took the test on different days like depending on how well good a day you were having. And these are, you know, people who are adopted by different families.\\n\\n1 0:20:51 The hardware theory explains it in the sense that it might be hardware might be true. So, it doesn't, it doesn't have an explanation beyond that, and nor the software theory.\\n\\n2 0:21:04 Sorry, go on. I mean, so there are actually like differences at the level of brain that are correlated with IQ right so you actual skull size is like a point three correlation with IQ. There's a few more like this they don't explain the entire the entire variance in human intelligence or the entire genetic variance in human intelligence, but we do have, we have identified a few actual Harvard differences that correlate.\\n\\n1 0:21:30 Suppose, suppose, on the contrary, that suppose that the result was that people who are brought up in the same family and differ only in the amount of hair they have or in their appearance in any other way, that none of those differences make any difference to their IQ. Only who their parents were makes a difference. Now, wouldn't that be surprising? Wouldn't it be surprising that there's nothing else correlated with IQ other than who your parents are?\\n\\n4\\n\\n0:22:21\\n\\nYes.\\n\\n1 0:\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" the result was that people who are brought up in the same family and differ only in the amount of hair they have or in their appearance in any other way, that none of those differences make any difference to their IQ. Only who their parents were makes a difference. Now, wouldn't that be surprising? Wouldn't it be surprising that there's nothing else correlated with IQ other than who your parents are?\\n\\n4\\n\\n0:22:21\\n\\nYes.\\n\\n1 0:22:23 Now, how much correlation should we expect? There are correlations everywhere. There are these things on the Internet which joke, memes or whatever you call it, but they make a serious point where they correlate things like how many adventure movies have been made in a given year correlated with how much the GNP per capita, and that's a bad example because there's an obvious relation, but you know what I mean. It's the number of films made by a particular actor against the number of outbreaks of bird flu. Part being surprised by randomness is the fact that correlations are everywhere. It's not just that correlation isn't causation, it's that correlations are everywhere. It's not a rare event to get a correlation between two things, and the more things you ask about, the more you are going to get correlations. So, the, the, it's, it's not, what is surprising is that the things that are correlated are, are things that you expect to be correlated and measured. For example, when they do these twin studies and measure the IQ, they control for certain things. And like you said, identical twins reared together, they've got to be reared together. Or apart. Or apart, yes. But there's infinitely more things that they don't control for. So, it could be that the real determinant of IQ is, for example, how well a child is treated between the ages of three and a half and four and a half, where well is defined by something that we don't know yet, but you know, something like that.\\n\\nThen you would expect that thing, which we don't know about and nobody has bothered to control for in these experiments, we would expect that thing to be correlated with IQ. But unfortunately that thing is also correlated with whether something, someone's an identical twin or not. twinness that is causing the similarity, it's this other thing. Right. This, say, an aspect of appearance or something. And if you were to surgically change a person with a view, if you knew what this thing was and surgically changed the person, you would be able to have the same effect as making an identical twin would have. Right.\\n\\n2 0:25:31 But, I mean, as you say, in science, or to explain any phenomenon, there's an infinite amount of possible explanations, right? You got to pick the best one. So it could be that there's some unknown trait which is so obvious to adopted parents, different adoptive parents, that they can use it as a basis for discrimination or for different treatment. But that is... I mean, I would assume they don't know what it is. But then aren't they using it as a basis to treat kids differently at the age of three, for example?\\n\\n1 0:25:58 Not by consciously identifying it. It's like, it would be something like getting the idea that this child is really smart. But I'm just trying to show you that it could be something that the parents are not aware of. If you ask parents to list the traits in their children that cause them to behave differently towards their children, they might list like 10 traits. But then there are another thousand traits that they're not aware of which also affect their behavior.\\n\\n2 0:26:30 So, we first need an explanation for what this trait is that researchers have not been able to identify it but it's so obvious that even unconsciously parents are able to reliably use it as a way to have to be obvious at all because parents\\n\\n1 0:26:47 of information about their children which they are processing in their minds and most of it they\\n\\n2 0:26:57 don't know what it is. Okay, all right. Okay, so I guess let's leave this topic aside for now and then let me bring us to animals. So if creativity is something that doesn't exist in increments, it's or you know the capacity to create explanations, you can just use a simple example go on YouTube and look up cat opening a door right so you'll see for example, a cat develops a theory that applying torque to this handle to this metal thing will open door. Now it hasn't. And then what it'll do is it'll climb onto a countertop and it'll jump on top of that door handle. It hasn't\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" for practical purposes that's the same thing.\\n\\nLulie When you make a new connection between two things, does that then create a new third thing which you can then make connections between? Is it actually true that the theory of knowledge works by making connections between things? Would you say that's an accurate representation?\\n\\nDavid Deutsch Well, again, if I knew the exact answer to that question, I could make an AGI. But I'm fairly sure, I mean, I can't think of an alternative to the process being and mutating existing things. So you can either change a theory slightly in the hope that the rest of it still works or you can make new combinations between things. That's also the two ways that biological mutations can happen in DNA. You can either have a cosmic ray strikes the DNA and changes one base pair or something, or you can have some error in copying results in a bit of DNA that came from somewhere else being stuck into a particular place in the DNA strand.\\n\\nLulie So this sounds like random variation of one thing rather than a new connection between two things?\\n\\nDavid Deutsch Well, no, the second process is in a way a new connection between two things because it, if there's a whole bit of DNA in a different organism or in a different part of the DNA and it gets copied, as it were, into the wrong place in the evolving organism, and that's a way that evolution can take place. Most instances of that just kill the organism but every so often it either leaves the functionality unchanged or makes it better. So what does that have to do with connecting? If a dog gets a piece of DNA from a lobster then that creates a similarity between dogs and lobsters that didn't exist before. And people can, paleogeneticists or whatever they're called, can look at DNA. Bacteria do this a lot. Higher organisms do it less and less because, I think basically because there's less and less chance of you surviving such a thing. For it to be part of evolution, you've got to be able to survive the new adaptation being slightly there, and a bit more there, and a bit more there, because giving a dog lobster claws wouldn't work, because the machinery for controlling the claws and deciding when to use them and so on hasn't been transferred. It would have to evolve. The point is that human new explanations evolve intentionally.\\n\\nThey involve randomness at some level, but the business part of creating the new explanation is what happens to the randomness after it's generated and it is changed and further changed intentionally to solve a problem and that is what induction can't possibly do but we don't know what\\n\\nLulie can do it. Induction can't make a change to solve a problem yes but evolution both genetic and evolution of ideas in the mind can?\\n\\nDavid Deutsch Well, no, it can... so every new genetic sequence is produced first and tried out later. Marxism thinks that an organism's environment and its own actions can cause a change in its genome. And that can't happen, and it couldn't happen because it's the same as induction, which also can't happen. So is this also true within a mind? Like you have to make up something before you can tell\\n\\nLulie whether it solves the problem or not?\\n\\nDavid Deutsch Yes, but there's a difference, which is that the making up process isn't purely random.\\n\\n5\\n\\nHow so?\\n\\nDavid Deutsch For example, a human can do the thing which I just said the dog and the lobster can't do. can think of the solar neutrino problem and can think, could it be that there's a new particle that isn't even a neutrino? And what would that particle... Now that in itself is not the explanation, but it's the germ of an explanation be like made up before you can check whether that's a good question? Yes, but it that making up that involves much less random trial and error than it would that it would take to try all possible variations randomly. So the fact that a person can think of an analogy, you know, some people think that human thinking is all about analogies. So we can take a whole idea from some other place and see if it fits in this place, which it never does immediately, but then you can say, well, how can we change it further so it does fit? So a person who thought, well, maybe there's a wholly new particle involved, at some point that person may say, maybe that whole new particle is just a neutrino, but of a different type. And then they're well on the way to solving. Of course there are many other considerations, but when I say many, it's nothing compared with how many you'd have to check\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" Why did they just happen to make certain abilities, make use of certain abilities that people have. So, if you're setting up a company that is going to employ 10,000 employees, then it's best to make the way that the company works, you know, it's best for example to make the signs above the doors or the signs on the doors or the numbers on the dials all be ones that people in that culture who are highly educated can read. You could in principle make each label on each door a different language, I don't know, you know, there are thousands of human languages, let's say there are 5,000 languages and 5,000 doors in the company, you could, given the same meaning, make them all different languages. The reason that they're all the same language and what's more, not just any old language, it's a language that many educated people know fluently. That's why. And then you can misinterpret that as saying, oh, there is some hardware reason why everybody speaks the same language. Well, no, there isn't. It's a cultural reason.\\n\\n2 0:17:22 Okay, so if the culture was different somehow, maybe if there was some other way of communicating ideas, do you think that the people who are currently designated as not functionally literate could be in a position to learn about quantum computing, for example, if they made the right choices are not the right choices but the choices that could lead to them understanding quantum computing.\\n\\n1 0:17:49 Well, rather begs the question. It's not only language that is like this, it's all knowledge. So, just learning, so if someone doesn't speak English, quantum computing is a field in which English is the standard language. Used to be German, now it's English. Now, someone who doesn't know English is at a disadvantage learning about quantum computers, but not only because of their deficiency in language. If they come from a culture in which the culture of physics and of mathematics and and of logic and so on is is is equivalent and only the language is different, then if they just learn the language, they will find it as easy as anyone else. But if a whole load of things are different, if a person doesn't think in terms of, for example, logic, but thinks in terms of pride and manliness and fear and, you know, all sorts of concepts that fill the lives of, let's say, prehistoric people or pre-enlightenment people, then to be able to understand quantum computers they would have to learn a lot more than just the language of the civilization. They'd have to learn all of other, well not all, but a range of other features of the civilization. And on that basis, the people who can't read driving licenses are similarly in a different culture, which they would also have to learn if they are to increase their IQ, i.e. their ability to function at a high level in intellectual culture in our civilization. Okay, they would be able to.\\n\\n2 0:20:07 Okay. So if it's those kinds of differences, then how do you explain the fact that identical twins separated at birth and adopted by different families, they tend to have the most of the variance that does exist between humans in terms of IQ doesn't exist between identical twins. In fact, the correlation is point eight, which is the correlation that you, you would have when you took the test on different days like depending on how well good a day you were having. And these are, you know, people who are adopted by different families.\\n\\n1 0:20:51 The hardware theory explains it in the sense that it might be hardware might be true. So, it doesn't, it doesn't have an explanation beyond that, and nor the software theory.\\n\\n2 0:21:04 Sorry, go on. I mean, so there are actually like differences at the level of brain that are correlated with IQ right so you actual skull size is like a point three correlation with IQ. There's a few more like this they don't explain the entire the entire variance in human intelligence or the entire genetic variance in human intelligence, but we do have, we have identified a few actual Harvard differences that correlate.\\n\\n1 0:21:30 Suppose, suppose, on the contrary, that suppose that the result was that people who are brought up in the same family and differ only in the amount of hair they have or in their appearance in any other way, that none of those differences make any difference to their IQ. Only who their parents were makes a difference. Now, wouldn't that be surprising? Wouldn't it be surprising that there's nothing else correlated with IQ other than who your parents are?\\n\\n4\\n\\n0:22:21\\n\\nYes.\\n\\n1 0:\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" the result was that people who are brought up in the same family and differ only in the amount of hair they have or in their appearance in any other way, that none of those differences make any difference to their IQ. Only who their parents were makes a difference. Now, wouldn't that be surprising? Wouldn't it be surprising that there's nothing else correlated with IQ other than who your parents are?\\n\\n4\\n\\n0:22:21\\n\\nYes.\\n\\n1 0:22:23 Now, how much correlation should we expect? There are correlations everywhere. There are these things on the Internet which joke, memes or whatever you call it, but they make a serious point where they correlate things like how many adventure movies have been made in a given year correlated with how much the GNP per capita, and that's a bad example because there's an obvious relation, but you know what I mean. It's the number of films made by a particular actor against the number of outbreaks of bird flu. Part being surprised by randomness is the fact that correlations are everywhere. It's not just that correlation isn't causation, it's that correlations are everywhere. It's not a rare event to get a correlation between two things, and the more things you ask about, the more you are going to get correlations. So, the, the, it's, it's not, what is surprising is that the things that are correlated are, are things that you expect to be correlated and measured. For example, when they do these twin studies and measure the IQ, they control for certain things. And like you said, identical twins reared together, they've got to be reared together. Or apart. Or apart, yes. But there's infinitely more things that they don't control for. So, it could be that the real determinant of IQ is, for example, how well a child is treated between the ages of three and a half and four and a half, where well is defined by something that we don't know yet, but you know, something like that.\\n\\nThen you would expect that thing, which we don't know about and nobody has bothered to control for in these experiments, we would expect that thing to be correlated with IQ. But unfortunately that thing is also correlated with whether something, someone's an identical twin or not. twinness that is causing the similarity, it's this other thing. Right. This, say, an aspect of appearance or something. And if you were to surgically change a person with a view, if you knew what this thing was and surgically changed the person, you would be able to have the same effect as making an identical twin would have. Right.\\n\\n2 0:25:31 But, I mean, as you say, in science, or to explain any phenomenon, there's an infinite amount of possible explanations, right? You got to pick the best one. So it could be that there's some unknown trait which is so obvious to adopted parents, different adoptive parents, that they can use it as a basis for discrimination or for different treatment. But that is... I mean, I would assume they don't know what it is. But then aren't they using it as a basis to treat kids differently at the age of three, for example?\\n\\n1 0:25:58 Not by consciously identifying it. It's like, it would be something like getting the idea that this child is really smart. But I'm just trying to show you that it could be something that the parents are not aware of. If you ask parents to list the traits in their children that cause them to behave differently towards their children, they might list like 10 traits. But then there are another thousand traits that they're not aware of which also affect their behavior.\\n\\n2 0:26:30 So, we first need an explanation for what this trait is that researchers have not been able to identify it but it's so obvious that even unconsciously parents are able to reliably use it as a way to have to be obvious at all because parents\\n\\n1 0:26:47 of information about their children which they are processing in their minds and most of it they\\n\\n2 0:26:57 don't know what it is. Okay, all right. Okay, so I guess let's leave this topic aside for now and then let me bring us to animals. So if creativity is something that doesn't exist in increments, it's or you know the capacity to create explanations, you can just use a simple example go on YouTube and look up cat opening a door right so you'll see for example, a cat develops a theory that applying torque to this handle to this metal thing will open door. Now it hasn't. And then what it'll do is it'll climb onto a countertop and it'll jump on top of that door handle. It hasn't\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" Why did they just happen to make certain abilities, make use of certain abilities that people have. So, if you're setting up a company that is going to employ 10,000 employees, then it's best to make the way that the company works, you know, it's best for example to make the signs above the doors or the signs on the doors or the numbers on the dials all be ones that people in that culture who are highly educated can read. You could in principle make each label on each door a different language, I don't know, you know, there are thousands of human languages, let's say there are 5,000 languages and 5,000 doors in the company, you could, given the same meaning, make them all different languages. The reason that they're all the same language and what's more, not just any old language, it's a language that many educated people know fluently. That's why. And then you can misinterpret that as saying, oh, there is some hardware reason why everybody speaks the same language. Well, no, there isn't. It's a cultural reason.\\n\\n2 0:17:22 Okay, so if the culture was different somehow, maybe if there was some other way of communicating ideas, do you think that the people who are currently designated as not functionally literate could be in a position to learn about quantum computing, for example, if they made the right choices are not the right choices but the choices that could lead to them understanding quantum computing.\\n\\n1 0:17:49 Well, rather begs the question. It's not only language that is like this, it's all knowledge. So, just learning, so if someone doesn't speak English, quantum computing is a field in which English is the standard language. Used to be German, now it's English. Now, someone who doesn't know English is at a disadvantage learning about quantum computers, but not only because of their deficiency in language. If they come from a culture in which the culture of physics and of mathematics and and of logic and so on is is is equivalent and only the language is different, then if they just learn the language, they will find it as easy as anyone else. But if a whole load of things are different, if a person doesn't think in terms of, for example, logic, but thinks in terms of pride and manliness and fear and, you know, all sorts of concepts that fill the lives of, let's say, prehistoric people or pre-enlightenment people, then to be able to understand quantum computers they would have to learn a lot more than just the language of the civilization. They'd have to learn all of other, well not all, but a range of other features of the civilization. And on that basis, the people who can't read driving licenses are similarly in a different culture, which they would also have to learn if they are to increase their IQ, i.e. their ability to function at a high level in intellectual culture in our civilization. Okay, they would be able to.\\n\\n2 0:20:07 Okay. So if it's those kinds of differences, then how do you explain the fact that identical twins separated at birth and adopted by different families, they tend to have the most of the variance that does exist between humans in terms of IQ doesn't exist between identical twins. In fact, the correlation is point eight, which is the correlation that you, you would have when you took the test on different days like depending on how well good a day you were having. And these are, you know, people who are adopted by different families.\\n\\n1 0:20:51 The hardware theory explains it in the sense that it might be hardware might be true. So, it doesn't, it doesn't have an explanation beyond that, and nor the software theory.\\n\\n2 0:21:04 Sorry, go on. I mean, so there are actually like differences at the level of brain that are correlated with IQ right so you actual skull size is like a point three correlation with IQ. There's a few more like this they don't explain the entire the entire variance in human intelligence or the entire genetic variance in human intelligence, but we do have, we have identified a few actual Harvard differences that correlate.\\n\\n1 0:21:30 Suppose, suppose, on the contrary, that suppose that the result was that people who are brought up in the same family and differ only in the amount of hair they have or in their appearance in any other way, that none of those differences make any difference to their IQ. Only who their parents were makes a difference. Now, wouldn't that be surprising? Wouldn't it be surprising that there's nothing else correlated with IQ other than who your parents are?\\n\\n4\\n\\n0:22:21\\n\\nYes.\\n\\n1 0:\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" the result was that people who are brought up in the same family and differ only in the amount of hair they have or in their appearance in any other way, that none of those differences make any difference to their IQ. Only who their parents were makes a difference. Now, wouldn't that be surprising? Wouldn't it be surprising that there's nothing else correlated with IQ other than who your parents are?\\n\\n4\\n\\n0:22:21\\n\\nYes.\\n\\n1 0:22:23 Now, how much correlation should we expect? There are correlations everywhere. There are these things on the Internet which joke, memes or whatever you call it, but they make a serious point where they correlate things like how many adventure movies have been made in a given year correlated with how much the GNP per capita, and that's a bad example because there's an obvious relation, but you know what I mean. It's the number of films made by a particular actor against the number of outbreaks of bird flu. Part being surprised by randomness is the fact that correlations are everywhere. It's not just that correlation isn't causation, it's that correlations are everywhere. It's not a rare event to get a correlation between two things, and the more things you ask about, the more you are going to get correlations. So, the, the, it's, it's not, what is surprising is that the things that are correlated are, are things that you expect to be correlated and measured. For example, when they do these twin studies and measure the IQ, they control for certain things. And like you said, identical twins reared together, they've got to be reared together. Or apart. Or apart, yes. But there's infinitely more things that they don't control for. So, it could be that the real determinant of IQ is, for example, how well a child is treated between the ages of three and a half and four and a half, where well is defined by something that we don't know yet, but you know, something like that.\\n\\nThen you would expect that thing, which we don't know about and nobody has bothered to control for in these experiments, we would expect that thing to be correlated with IQ. But unfortunately that thing is also correlated with whether something, someone's an identical twin or not. twinness that is causing the similarity, it's this other thing. Right. This, say, an aspect of appearance or something. And if you were to surgically change a person with a view, if you knew what this thing was and surgically changed the person, you would be able to have the same effect as making an identical twin would have. Right.\\n\\n2 0:25:31 But, I mean, as you say, in science, or to explain any phenomenon, there's an infinite amount of possible explanations, right? You got to pick the best one. So it could be that there's some unknown trait which is so obvious to adopted parents, different adoptive parents, that they can use it as a basis for discrimination or for different treatment. But that is... I mean, I would assume they don't know what it is. But then aren't they using it as a basis to treat kids differently at the age of three, for example?\\n\\n1 0:25:58 Not by consciously identifying it. It's like, it would be something like getting the idea that this child is really smart. But I'm just trying to show you that it could be something that the parents are not aware of. If you ask parents to list the traits in their children that cause them to behave differently towards their children, they might list like 10 traits. But then there are another thousand traits that they're not aware of which also affect their behavior.\\n\\n2 0:26:30 So, we first need an explanation for what this trait is that researchers have not been able to identify it but it's so obvious that even unconsciously parents are able to reliably use it as a way to have to be obvious at all because parents\\n\\n1 0:26:47 of information about their children which they are processing in their minds and most of it they\\n\\n2 0:26:57 don't know what it is. Okay, all right. Okay, so I guess let's leave this topic aside for now and then let me bring us to animals. So if creativity is something that doesn't exist in increments, it's or you know the capacity to create explanations, you can just use a simple example go on YouTube and look up cat opening a door right so you'll see for example, a cat develops a theory that applying torque to this handle to this metal thing will open door. Now it hasn't. And then what it'll do is it'll climb onto a countertop and it'll jump on top of that door handle. It hasn't\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" Why did they just happen to make certain abilities, make use of certain abilities that people have. So, if you're setting up a company that is going to employ 10,000 employees, then it's best to make the way that the company works, you know, it's best for example to make the signs above the doors or the signs on the doors or the numbers on the dials all be ones that people in that culture who are highly educated can read. You could in principle make each label on each door a different language, I don't know, you know, there are thousands of human languages, let's say there are 5,000 languages and 5,000 doors in the company, you could, given the same meaning, make them all different languages. The reason that they're all the same language and what's more, not just any old language, it's a language that many educated people know fluently. That's why. And then you can misinterpret that as saying, oh, there is some hardware reason why everybody speaks the same language. Well, no, there isn't. It's a cultural reason.\\n\\n2 0:17:22 Okay, so if the culture was different somehow, maybe if there was some other way of communicating ideas, do you think that the people who are currently designated as not functionally literate could be in a position to learn about quantum computing, for example, if they made the right choices are not the right choices but the choices that could lead to them understanding quantum computing.\\n\\n1 0:17:49 Well, rather begs the question. It's not only language that is like this, it's all knowledge. So, just learning, so if someone doesn't speak English, quantum computing is a field in which English is the standard language. Used to be German, now it's English. Now, someone who doesn't know English is at a disadvantage learning about quantum computers, but not only because of their deficiency in language. If they come from a culture in which the culture of physics and of mathematics and and of logic and so on is is is equivalent and only the language is different, then if they just learn the language, they will find it as easy as anyone else. But if a whole load of things are different, if a person doesn't think in terms of, for example, logic, but thinks in terms of pride and manliness and fear and, you know, all sorts of concepts that fill the lives of, let's say, prehistoric people or pre-enlightenment people, then to be able to understand quantum computers they would have to learn a lot more than just the language of the civilization. They'd have to learn all of other, well not all, but a range of other features of the civilization. And on that basis, the people who can't read driving licenses are similarly in a different culture, which they would also have to learn if they are to increase their IQ, i.e. their ability to function at a high level in intellectual culture in our civilization. Okay, they would be able to.\\n\\n2 0:20:07 Okay. So if it's those kinds of differences, then how do you explain the fact that identical twins separated at birth and adopted by different families, they tend to have the most of the variance that does exist between humans in terms of IQ doesn't exist between identical twins. In fact, the correlation is point eight, which is the correlation that you, you would have when you took the test on different days like depending on how well good a day you were having. And these are, you know, people who are adopted by different families.\\n\\n1 0:20:51 The hardware theory explains it in the sense that it might be hardware might be true. So, it doesn't, it doesn't have an explanation beyond that, and nor the software theory.\\n\\n2 0:21:04 Sorry, go on. I mean, so there are actually like differences at the level of brain that are correlated with IQ right so you actual skull size is like a point three correlation with IQ. There's a few more like this they don't explain the entire the entire variance in human intelligence or the entire genetic variance in human intelligence, but we do have, we have identified a few actual Harvard differences that correlate.\\n\\n1 0:21:30 Suppose, suppose, on the contrary, that suppose that the result was that people who are brought up in the same family and differ only in the amount of hair they have or in their appearance in any other way, that none of those differences make any difference to their IQ. Only who their parents were makes a difference. Now, wouldn't that be surprising? Wouldn't it be surprising that there's nothing else correlated with IQ other than who your parents are?\\n\\n4\\n\\n0:22:21\\n\\nYes.\\n\\n1 0:\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" a different emotion?\\n\\n1 1:07:29 I don't think it's an emotion. And and I all these things are not very well defined. They can't possibly be very well defined until we have a satisfactory theory of qualia, at least, and probably more satisfactory theory of creativity, how creativity works and so on. I think that the choice of the word fun for the thing that I explain more precisely, but still not very precisely, as a creation of knowledge without, where the different kinds of knowledge, inexplicit, unconscious, conscious, explicit, I think that is actually the only way in which the everyday usage of the word fun differs from that is that fun is considered frivolous or seeking fun is considered as seeking frivolity. But I think that isn't so much a different use of the word, it's just a different pejorative theory about whether this is a good or a bad thing. But nevertheless, I can't define it precisely. The important thing is that there is a thing which has this property of fun that you can't compulsorily enact it. So in some views, you know, no pain, no gain. Well, then you can find out mechanically whether the thing is causing pain and whether it's doing it according to the theory that says that you will have gain if you have that pain. So that can all be done mechanically. And therefore it is subject to the criticism.\\n\\ncriticism and another way of looking at the fun theory is that it's a mode of criticism um is subject to the criticism that this isn't fun i.e this is making an uh privileging one kind of knowledge arbitrarily over another rather than being rational and letting content\\n\\n2 1:09:58 decide is this placing a limitation on universal explainers then if they can create some sort of theory about why I think could or should be fun, why anything could be fun. And it seems to me that sometimes we actually can make things fun that aren't like, for example, take exercise no pain no gain, it's like when you first go it's not fun but you know once you start going you understand the mechanics, you develop a theory for why\\n\\n1 1:10:21 can and should be fun. Yes, yes. Well, that's quite a good example because there you see that fun cannot be defined as the absence of pain. So you can be having fun while experiencing physical pain and that physical pain is not sparking joy, as Marie Kondo would say. And that's important because if you are dogmatically or uncritically in your life, a theory of the good that involves pain and which excludes the criticism that maybe this can't be fun or maybe this isn't yet fun or maybe I should make it fun and if I can't that's a reason to stop, you know, all those things. If all those things are excluded because by definition the thing is good and your pain, your suffering doesn't matter, then that opens the door to not only to suffering, but to stasis. You won't be able to get to a better theory.\\n\\n4 1:11:43 And then why is fun central to this\\n\\n1 1:11:46 instead of another emotion? So, like for example, Aristotle thought that,\\n\\n2 1:11:50 like I guess, a sort of widely defined sense of happiness is what should be the goal of our endeavors. Why fun instead of something like that?\\n\\n1 1:12:10 Well, that's defining it vaguely enough so that what you said might very well be fun. The point is the underlying thing is as far as, you know, going one level below, we're really to understand that we'd need to go about seven levels below that, which we can't do yet. But the important thing is that there are several kinds of knowledge in our brains. And the one that is written down in the exercise book that says, you should do this number of reps, and you should power through this, and it doesn't matter if you feel that, and so on. That's an explicit theory, and it contains some knowledge, but it also contains error. That's like all our knowledge is like that. We also have other knowledge, which is which is contained in our biology, it's contained in our genes. We have knowledge that is inexplicit, like our knowledge of, of grammar is my was my favorite example, as we know why certain sentences are acceptable and why they're unacceptable, but we can't state explicitly, or in every case, why it isn't or why it is. And then there's explicit and inexplicit knowledge, there's conscious and unconscious knowledge. All those are bits of program in the brain, they're ideas, they are bits of knowledge in this, if you define knowledge as information with causal power, they are all\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" So if creativity is something that doesn't exist in increments, it's or you know the capacity to create explanations, you can just use a simple example go on YouTube and look up cat opening a door right so you'll see for example, a cat develops a theory that applying torque to this handle to this metal thing will open door. Now it hasn't. And then what it'll do is it'll climb onto a countertop and it'll jump on top of that door handle. It hasn't seen another cat to it it hasn't seen another human like get on a countertop and try to open the door that way. But it conjectures that this is a way, given its morphology, that it can access the door. And then, you know, so that's the theory. And then the experiment is, will the door open? This seems like a classic cycle of conjecture and reputation. Is this compatible with the cats not being, at least having some bounded form of creativity?\\n\\n1 0:27:57 So, animals are amazing things and instinctive animal knowledge is designed to make animals easily capable of thriving in environments that they've never seen before. for avoiding each tree, and not only that, for actually catching the rabbit that it's running after as well, in a way that has never been done before. So, the way to understand this, I think, now this is because of a vast amount of knowledge that is in the wolf's genes. What kind of knowledge is this? Well, it's not the kind of knowledge that says first turn left, then turn right, then jump, and so on. It's not that kind of instruction. It's instruction that takes input from the outside and then generates a behavior that is relevant to that input. sophistication in the program that human robotics has not yet reached anywhere near that. And by the way, then when it sees a wolf of the opposite sex, it may decide to leave the rabbit and go and have sex instead. And a program for a robot to locate another robot of the right species and then have sex with it is again, I think, beyond present day robotics. But it will be done. And it does not, it clearly does not require creativity because that same program will lead the next wolf to do the same thing in the same circumstances. It's the fact that the circumstances are ones that is never seen before and it can still function, is a testimony to the incredible sophistication of that program, but it has nothing to do with creativity.\\n\\nSo, humans do tasks that require much, much less programming sophistication than that, such as sitting around a campfire, telling each other a scary story about a wolf that almost ate them. Now, animals can do the wolf running away thing. They can enact a story that's more complicated even than the one the human is telling, but they can't tell a story. They don't tell a story. Telling a story is a sort of typical creative activity. It's the same kind of activity as forming an explanation. So I don't think it's at all surprising that cats can jump on handles, because it's the same. I can easily imagine that the same amazingly sophisticated program that lets it jump on a branch so that the branch will get out of its way in some sense, will also function in this new environment that's never seen before. But there are all sorts of other things that it can't do.\\n\\n2 0:31:55 Oh, that's definitely true, which was my point, is that it has a bounded form of creativity, and if bounded forms of creativity can exist, then humans could be in one such, but I'm having a hard time imagining the ancestral circumstance in which a cat couldn't would have genetic gain the genetic knowledge that jumping on a metal rod would get a wooden plank to open\\n\\n1 0:32:17 and give it access to the you know the other side. Well, I thought I just gave an example. I mean, if we don't know, at least I don't know what kind of environment the ancestor of the domestic cat lived in. But if it was for example if it contained undergrowth, then dealing with undergrowth requires some very sophisticated programs otherwise you will just get stuck somewhere and starve to death. Now, I think a dog, if it gets stuck in a bush, it has no program to get out other than to shake itself about until it gets out. It doesn't have a concept of doing something which temporarily makes matters worse and then allows you to get\\n\\n2 0:33:10 I think dogs can't do that. But it's just, it's not because that's a particularly complicated thing, it's just that it's programming just doesn't have that. But an animal's programming easily could have that if it lived in an environment in which that happened a lot of power. So\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" our knowledge of, of grammar is my was my favorite example, as we know why certain sentences are acceptable and why they're unacceptable, but we can't state explicitly, or in every case, why it isn't or why it is. And then there's explicit and inexplicit knowledge, there's conscious and unconscious knowledge. All those are bits of program in the brain, they're ideas, they are bits of knowledge in this, if you define knowledge as information with causal power, they are all information with causal power. They all contain truth and they all contain error and it's always a mistake to shield something, to shield one of them from criticism or replacement.\\n\\nNot doing that is what I call the fun criterion. Now you might say that's a bad name, but you know, it's the best I can find.\\n\\n2 1:14:11 So why would creating an AGI through evolution necessarily entail suffering? Because the way I see it, or it seems to me your theory is that you need to be a general intelligence in order to feel suffering, but by the point an evolved, simulated being is a general intelligence, we can just stop the simulation.\\n\\n1 1:14:31 And so where is the suffering coming from? Okay, so the kind of simulation by evolution that I'm thinking of, I mean, there may be several kinds, but the kind that I'm thinking of, and which I said would be the greatest crime in history, is the kind that just simulates the actual evolution of humans from pre humans that weren't people. So you have you have a population of non people which in this simulation would be some kind of NPCs. And then they would they would just evolve we don't know what the criterion would be we just have an artificial universe which simulated the surface of the earth and they'd be walking around and some of them might or might not become people. And now the thing is when you're part of the way there, what is happening is that you have the way that I, the only way that I can imagine the evolution of personhood or create the explanatory creativity happened was that the hardware needed for it was first needed for something else. I have proposed that it was needed to transmit memes. So there'd be people who were transmitting memes creatively but they were running out of resources before it managed to increase their stock of memes. So in every generation, there was a stock of memes that was being passed down to the next generation. And once they got beyond a certain complexity, they had to be passed down by the use of creativity by the recipient.\\n\\nSo, there may well have been a time and as I say I can't think of any other way it could have been, where there was genuine creativity being used, but it ran out of resources, very quickly, but not so quickly that it didn't increase the mean bandwidth. Then in the next generation, there was more mean bandwidth, and then after, you know, certain number of generations, there would have been some opportunity to use this hardware or whatever it is, you know, firmware, I expect, to use this firmware for something other than just trying blindly transmitting memes or rather creatively transmitting memes, but they were blind memes. So in that time, it would have been very unpleasant to be alive. It was already very unpleasant to, sorry, it was very unpleasant to be alive when we did have enough resources to think as well as do the memes. But I don't think there would have been a moment at which you would say, yes, now the suffering begins to matter because it's not just blind memes. I think the people were already suffering at the time when they were blindly transmitting memes because they were using genuine creativity. They were just not using it to any good effect.\\n\\n2 1:17:56 Gotcha. Would being in the experience machine be compatible with the fun criterion so you're not aware that you're in, you know, the experience machine it's all virtual reality, but you're still doing the things that would make you have fun in fact more so than in the real world.\\n\\n1 1:18:15 So, would you be tempted to get into experience machine, what do we can put a compatible with the fun criteria, I mean, is it just a virtual reality world in which things work better than in the real world or something?\\n\\n2 1:18:35 Yeah, so it's a thought experiment by Robert Nozick, and the idea is that you would enter this world, but you would forget that you're in virtual reality. So all, I mean, the world would be perfect in every possible way that it could be perfect or not perfect, but it would be better in every possible way it could be better. But you would think the relationships you have here are real, the knowledge you're discovering here is novel\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" than to shake itself about until it gets out. It doesn't have a concept of doing something which temporarily makes matters worse and then allows you to get\\n\\n2 0:33:10 I think dogs can't do that. But it's just, it's not because that's a particularly complicated thing, it's just that it's programming just doesn't have that. But an animal's programming easily could have that if it lived in an environment in which that happened a lot of power. So if, for example, I wrote a deep learning program, I traded over financial history, and I asked it, make me a trillion dollars on the stock market. Do you think that this would be impossible? And if you think this would be possible, then it seems like I do, it's not an AGI, but it seems like a very powerful\\n\\n1 0:33:56 AI, right? So it seems like AI is getting somewhere. Yeah, well, if you want to be powerful, you might do better inventing a weapon or something. But, but, or, or a better mousetrap is even better, because it's non violent so you can invent a paperclip. To use an example, it's often used in this context, you can invent it, if paperclips hadn't been invented, you can invent a paperclip and make a fortune. And that's an idea, which is, but it's not an AI, because it's not the paperclip that's going out there. It's really your idea in the first place, that has caused the whole value of the paperclip. And similarly, if you invent a dumb arbitrage machine, which seeks out complicated trades to make, which are more complicated than anyone else is trying to do. And that makes you a fortune. Well, the thing that made you a fortune was not the arbitrage machine, it was your idea for how to search for arbitrage opportunities that no one else sees.\\n\\n7\\n\\n0:35:08\\n\\nRight.\\n\\n6\\n\\n0:35:08\\n\\nThat's what was valuable.\\n\\n1 0:35:10 And that's the usual way of making money in the economy. You have an idea, and then you implement it. Right. That was an AI is beside the point. It could have been a paper clip.\\n\\n2 0:35:24 But the thing is, so the models that are used nowadays are not expert systems like the chess engines of the 90s. Something like AlphaZero or AlphaGo, this is just like almost a blank neural net. And they were able to help let it win go. Or so if such a neural network that was kind of blank, and if you just arbitrarily throw a financial history at it, wouldn't it be fair to say that the AI actually figured out what the right trades were? Even though it's not a general intelligence?\\n\\n1 0:35:52 Well, I think it's possible in chess, but not in the economy, because the value in the economy is being created by creativity and most, you know, arbitrage is one thing that can sort of skim value off the top by taking opportunities that were too expensive for other people to take. So you can, you know, you can make money, you make a lot of money that way if you know, if you have a good idea about how to do it. But most of the value in the economy is created by the creation of knowledge. Somebody has the idea that a smartphone would be good to have, even though most people think that that's not going to work. And that idea cannot be anticipated by anything less than an AGI. An AGI could have that idea, but no AI could. Okay.\\n\\n2 0:36:48 So there's definitely other topics I want to get to. So let's talk about virtual reality. So in the fabric of reality, you discuss the possibility that virtual reality generators could plug in directly into our nervous system and give us sense data that way. Now, as you might know, many meditators, you know, people like Sam Harris, speak of both thoughts and senses as intrusions into consciousness that have a sort of similar, they can be welcome intrusions, but they are both things that come into consciousness. So, do you think that a virtual reality generator could also place thoughts, as well as sense data,\\n\\n1 0:37:24 into the mind? Yes, but that's only because I think that this model is wrong. It's basically the Cartesian theater, as Daniel Dennett puts it, with the stage cleared of all the characters. So, that's pure consciousness without content, as Sam Harris envisages it. But I think that all that's happening there is that you are conscious of this theater, and you\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" So if creativity is something that doesn't exist in increments, it's or you know the capacity to create explanations, you can just use a simple example go on YouTube and look up cat opening a door right so you'll see for example, a cat develops a theory that applying torque to this handle to this metal thing will open door. Now it hasn't. And then what it'll do is it'll climb onto a countertop and it'll jump on top of that door handle. It hasn't seen another cat to it it hasn't seen another human like get on a countertop and try to open the door that way. But it conjectures that this is a way, given its morphology, that it can access the door. And then, you know, so that's the theory. And then the experiment is, will the door open? This seems like a classic cycle of conjecture and reputation. Is this compatible with the cats not being, at least having some bounded form of creativity?\\n\\n1 0:27:57 So, animals are amazing things and instinctive animal knowledge is designed to make animals easily capable of thriving in environments that they've never seen before. for avoiding each tree, and not only that, for actually catching the rabbit that it's running after as well, in a way that has never been done before. So, the way to understand this, I think, now this is because of a vast amount of knowledge that is in the wolf's genes. What kind of knowledge is this? Well, it's not the kind of knowledge that says first turn left, then turn right, then jump, and so on. It's not that kind of instruction. It's instruction that takes input from the outside and then generates a behavior that is relevant to that input. sophistication in the program that human robotics has not yet reached anywhere near that. And by the way, then when it sees a wolf of the opposite sex, it may decide to leave the rabbit and go and have sex instead. And a program for a robot to locate another robot of the right species and then have sex with it is again, I think, beyond present day robotics. But it will be done. And it does not, it clearly does not require creativity because that same program will lead the next wolf to do the same thing in the same circumstances. It's the fact that the circumstances are ones that is never seen before and it can still function, is a testimony to the incredible sophistication of that program, but it has nothing to do with creativity.\\n\\nSo, humans do tasks that require much, much less programming sophistication than that, such as sitting around a campfire, telling each other a scary story about a wolf that almost ate them. Now, animals can do the wolf running away thing. They can enact a story that's more complicated even than the one the human is telling, but they can't tell a story. They don't tell a story. Telling a story is a sort of typical creative activity. It's the same kind of activity as forming an explanation. So I don't think it's at all surprising that cats can jump on handles, because it's the same. I can easily imagine that the same amazingly sophisticated program that lets it jump on a branch so that the branch will get out of its way in some sense, will also function in this new environment that's never seen before. But there are all sorts of other things that it can't do.\\n\\n2 0:31:55 Oh, that's definitely true, which was my point, is that it has a bounded form of creativity, and if bounded forms of creativity can exist, then humans could be in one such, but I'm having a hard time imagining the ancestral circumstance in which a cat couldn't would have genetic gain the genetic knowledge that jumping on a metal rod would get a wooden plank to open\\n\\n1 0:32:17 and give it access to the you know the other side. Well, I thought I just gave an example. I mean, if we don't know, at least I don't know what kind of environment the ancestor of the domestic cat lived in. But if it was for example if it contained undergrowth, then dealing with undergrowth requires some very sophisticated programs otherwise you will just get stuck somewhere and starve to death. Now, I think a dog, if it gets stuck in a bush, it has no program to get out other than to shake itself about until it gets out. It doesn't have a concept of doing something which temporarily makes matters worse and then allows you to get\\n\\n2 0:33:10 I think dogs can't do that. But it's just, it's not because that's a particularly complicated thing, it's just that it's programming just doesn't have that. But an animal's programming easily could have that if it lived in an environment in which that happened a lot of power. So\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" than to shake itself about until it gets out. It doesn't have a concept of doing something which temporarily makes matters worse and then allows you to get\\n\\n2 0:33:10 I think dogs can't do that. But it's just, it's not because that's a particularly complicated thing, it's just that it's programming just doesn't have that. But an animal's programming easily could have that if it lived in an environment in which that happened a lot of power. So if, for example, I wrote a deep learning program, I traded over financial history, and I asked it, make me a trillion dollars on the stock market. Do you think that this would be impossible? And if you think this would be possible, then it seems like I do, it's not an AGI, but it seems like a very powerful\\n\\n1 0:33:56 AI, right? So it seems like AI is getting somewhere. Yeah, well, if you want to be powerful, you might do better inventing a weapon or something. But, but, or, or a better mousetrap is even better, because it's non violent so you can invent a paperclip. To use an example, it's often used in this context, you can invent it, if paperclips hadn't been invented, you can invent a paperclip and make a fortune. And that's an idea, which is, but it's not an AI, because it's not the paperclip that's going out there. It's really your idea in the first place, that has caused the whole value of the paperclip. And similarly, if you invent a dumb arbitrage machine, which seeks out complicated trades to make, which are more complicated than anyone else is trying to do. And that makes you a fortune. Well, the thing that made you a fortune was not the arbitrage machine, it was your idea for how to search for arbitrage opportunities that no one else sees.\\n\\n7\\n\\n0:35:08\\n\\nRight.\\n\\n6\\n\\n0:35:08\\n\\nThat's what was valuable.\\n\\n1 0:35:10 And that's the usual way of making money in the economy. You have an idea, and then you implement it. Right. That was an AI is beside the point. It could have been a paper clip.\\n\\n2 0:35:24 But the thing is, so the models that are used nowadays are not expert systems like the chess engines of the 90s. Something like AlphaZero or AlphaGo, this is just like almost a blank neural net. And they were able to help let it win go. Or so if such a neural network that was kind of blank, and if you just arbitrarily throw a financial history at it, wouldn't it be fair to say that the AI actually figured out what the right trades were? Even though it's not a general intelligence?\\n\\n1 0:35:52 Well, I think it's possible in chess, but not in the economy, because the value in the economy is being created by creativity and most, you know, arbitrage is one thing that can sort of skim value off the top by taking opportunities that were too expensive for other people to take. So you can, you know, you can make money, you make a lot of money that way if you know, if you have a good idea about how to do it. But most of the value in the economy is created by the creation of knowledge. Somebody has the idea that a smartphone would be good to have, even though most people think that that's not going to work. And that idea cannot be anticipated by anything less than an AGI. An AGI could have that idea, but no AI could. Okay.\\n\\n2 0:36:48 So there's definitely other topics I want to get to. So let's talk about virtual reality. So in the fabric of reality, you discuss the possibility that virtual reality generators could plug in directly into our nervous system and give us sense data that way. Now, as you might know, many meditators, you know, people like Sam Harris, speak of both thoughts and senses as intrusions into consciousness that have a sort of similar, they can be welcome intrusions, but they are both things that come into consciousness. So, do you think that a virtual reality generator could also place thoughts, as well as sense data,\\n\\n1 0:37:24 into the mind? Yes, but that's only because I think that this model is wrong. It's basically the Cartesian theater, as Daniel Dennett puts it, with the stage cleared of all the characters. So, that's pure consciousness without content, as Sam Harris envisages it. But I think that all that's happening there is that you are conscious of this theater, and you\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.\\n\\nPerhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.\\n\\nAGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.\\n\\nIronically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’\\n\\n‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.\\n\\nSome, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science.)\\n\\nThat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non-cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='t is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.\\n\\nBut no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.\\n\\nWhy? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.\\n\\nDespite this long record of failure, AGI must be possible. And that is because of a deep property of the laws of physics, namely the universality of computation. This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.\\n\\nBabbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.\\n\\nUnfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.\\n\\nSlow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?\\n\\nOne immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans�', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" is correct and another thing isn't correct it will talk nonsense and yet it knows which of them is correct. I've seen several cases of that and I was very impressed because it means that it's knowledge of language. It's a thing that I don't actually understand how it can be as good at speaking English, writing English, as it is. It's much better than most people, but more interestingly, it's much better than the average or the typical text on the internet. So it makes mistakes, but it makes far fewer mistakes than a typical text on the internet. Do you think this is going to revolutionise the economy and I can't prophesy the applications of modern AIs, chatbots and so on. I have found, for what it's worth, as it were, I have found it useful but not revolutionary in my own work and in my writing and whatever it is useful but I can't see possibility for it to revolutionize what I do. Whether it can revolutionize the economy depends on something slightly different because for that it doesn't need to really have a fundamental new functionality. It could be that a lot of existing jobs, and people are scared that computer programming is one of them, where only a proportion of the job, let's say 10%, of a particular programming job involves human creativity, and the rest is basically hack work.\\n\\nwhich is a big if, because I'm not convinced of this either. If chatbots can reliably perform the hack work, then it might be argued, I think again wrongly, that if a given task can be done with only a tenth as much work, then we might need only a tenth as many programmers in the long run.\\n\\nLulie And unfortunately we got interrupted, so David never finished his thought about why programmer jobs might be safer. But if you have any questions about anything in this episode, leave them jobs might be safer. But if you have any questions about anything in this episode, leave them on the tweet, which I will link in the show notes about this episode. Thank you.2 0:00:00 Okay, today I'm speaking with David Deutsch. Now this is a conversation that I've been eagerly wanting to have for years so this is very exciting for me. So, first let's talk about AI. Can you briefly explain why you anticipate that AI's will be no more fundamentally intelligent than humans.\\n\\n1 0:00:19 I suppose you mean capable of all the same types of cognition as humans are in principle.\\n\\n4\\n\\n0:00:37\\n\\nYes.\\n\\n1 0:00:38 So that would include, you know, doing science and doing art and in principle also falling in love and being good and being evil and all that. So the reason is twofold and one half is about computation hardware, computation hardware, and the other is about hardware. We know that our, our brains are Turing complete bits of hardware, and therefore can exhibit the functionality of running any computable function program for any computable function. Now, when I say any, I don't really mean any, because you and I sitting here, you know, we're having conversation and we could say, you know, we could have any conversation. Well, we can assume that maybe in a hundred years time, we'll both be dead. And therefore, the number of conversations we could have is strictly limited. And also, some conversations depend on speed of computation. So, you know, if we're going to be solving the traveling salesman problem, then there are there are many traveling salesman problems that we wouldn't be able to solve in the programs we can run, apart from by speed and memory capacity. So all limitations on us, hardware limitations on us, boil down to speed and memory capacity. And both those can be augmented to the level of any other entity that is in the universe. Because, you know, if somebody builds a computer that can think faster than the brain, then we can use that very computer or that very technology to make our thinking go just as fast as that. So that's the hardware.\\n\\nAs far as explanations go, can we reach the same kind of explanations as any other entity, let's say, usually this is said not in terms of AGIs but in terms of extraterrestrial intelligences, but also it's said about AGIs, you know, what if they are to us as we are to ants, and so on. Well, again, part of that is just hardware, which is easily fixable by adding more hardware. So let's forget about that. So, really, the idea is, is there are there are there concepts that we are inherently incapable of comprehending. I think Martin Rees believes this. He thinks that, you know, we can comprehend quantum mechanics. Apes can't. And maybe the extraterrestri\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.\\n\\nThe thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.\\n\\nAn AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.\\n\\nIn 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.\\n\\nThis does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.\\n\\nThe very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.\\n\\nAnother class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity.)\\n\\nAnd in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.\\n\\nThe upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.\\n\\nSuch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified, true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.\\n\\nHow could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first. Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" reality generator could also place thoughts, as well as sense data,\\n\\n1 0:37:24 into the mind? Yes, but that's only because I think that this model is wrong. It's basically the Cartesian theater, as Daniel Dennett puts it, with the stage cleared of all the characters. So, that's pure consciousness without content, as Sam Harris envisages it. But I think that all that's happening there is that you are conscious of this theater, and you're envisaging it as having certain properties, which, by the way, it doesn't have, but that doesn't matter. We can imagine lots of things that don't happen. You know, in fact, you know, that's in a way characterizes what we do all the time. So, one can interpret one's thoughts about this empty stage as being pure, contentless consciousness. But it's not. It has the content of a stage or a space or,\\n\\n2 0:38:40 you know, however you want to envisage it. Okay, and then let's talk about the Turing principle. So this is a term you coined. It's otherwise been called the church touring Deutsch principle. With this principle imply that you could. So, by the way, it states that any universal computer can simulate any physical process with this principle imply that you could simulate the whole of the universe for example in a compact efficient computer that\\n\\n1 0:39:06 was smaller than the universe itself, or is a constraint to physical processes of a task where it was computationally able to do it, but it wouldn't have enough memory or time. So the more memory and time you gave it, the more closely it could simulate the whole universe or anything near the whole universe probably because it, well, if you wanted to simulate itself as well, then there are logical reasons why there are limits to that. But even if you wanted to simulate the whole universe apart from itself, just the sheer size of the universe makes that impossible, even if we discovered ways of encoding information extremely densely like some people have said maybe quantum gravity would allow you know the rest of the universe as well so. from being limited by computational capacity, because it's only when you separate those that you realize what computational universality is. And I think that's universality, like Turing or quantum universality is the most important thing in the theory of computation, because computation doesn't even make sense, unless you have a concept of a universal computer.\\n\\n2 0:41:20 What could falsify your theory that all interesting problems are soluble so I asked this because, as I'm not convinced they're right. But do you have a strong reason for in principle believing that they're wrong.\\n\\n1 0:41:47 No. So, this, this is a bad explanation. So let's say that some people say, for example, that simulating a human brain is impossible. Now, I can't prove this possible. Nobody can prove this possible until they actually do it or unless they have a design for it which they prove will work. So, pending that there is there is no way of proving that, that it's not true that this is a fundamental limitation. is that it could be applied to anything. For example, it could be applied to the theory that you have recently, just a minute ago, been replaced by a humanoid robot, which is going to say for the next few minutes, just a prearranged set of things, and you're no longer a person. I can't believe you figured it out. Yeah, well, that's the first thing you'd say. So there is no way to refute that by experiment, short of actually doing it, short of actually talking to you and so on. So it's the same with all these other things. In order for it to make sense to have a theory that something is impossible, you have to have an explanation for why it is impossible. So we know that, for example, almost all mathematical propositions are undecidable. So that's not because somebody has said, oh maybe we can't decide everything because thinking we could decide everything is hubris. That's not an argument.\\n\\nYou need an actual functional argument to prove that that is so, and then, at being a functional argument in which the steps of the argument makes sense and relate to other things and so on. You can then say well what does this actually mean. Does this mean that maybe we can never understand the laws of physics included an undecidable function, then we would simply write, you know, f of x and f of x and undecidable function. We couldn't evaluate x it would limit our ability to make predictions, but then, lots of our ability to make predictions is totally limited anyway. to understand the properties of the function f and therefore the\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" than to shake itself about until it gets out. It doesn't have a concept of doing something which temporarily makes matters worse and then allows you to get\\n\\n2 0:33:10 I think dogs can't do that. But it's just, it's not because that's a particularly complicated thing, it's just that it's programming just doesn't have that. But an animal's programming easily could have that if it lived in an environment in which that happened a lot of power. So if, for example, I wrote a deep learning program, I traded over financial history, and I asked it, make me a trillion dollars on the stock market. Do you think that this would be impossible? And if you think this would be possible, then it seems like I do, it's not an AGI, but it seems like a very powerful\\n\\n1 0:33:56 AI, right? So it seems like AI is getting somewhere. Yeah, well, if you want to be powerful, you might do better inventing a weapon or something. But, but, or, or a better mousetrap is even better, because it's non violent so you can invent a paperclip. To use an example, it's often used in this context, you can invent it, if paperclips hadn't been invented, you can invent a paperclip and make a fortune. And that's an idea, which is, but it's not an AI, because it's not the paperclip that's going out there. It's really your idea in the first place, that has caused the whole value of the paperclip. And similarly, if you invent a dumb arbitrage machine, which seeks out complicated trades to make, which are more complicated than anyone else is trying to do. And that makes you a fortune. Well, the thing that made you a fortune was not the arbitrage machine, it was your idea for how to search for arbitrage opportunities that no one else sees.\\n\\n7\\n\\n0:35:08\\n\\nRight.\\n\\n6\\n\\n0:35:08\\n\\nThat's what was valuable.\\n\\n1 0:35:10 And that's the usual way of making money in the economy. You have an idea, and then you implement it. Right. That was an AI is beside the point. It could have been a paper clip.\\n\\n2 0:35:24 But the thing is, so the models that are used nowadays are not expert systems like the chess engines of the 90s. Something like AlphaZero or AlphaGo, this is just like almost a blank neural net. And they were able to help let it win go. Or so if such a neural network that was kind of blank, and if you just arbitrarily throw a financial history at it, wouldn't it be fair to say that the AI actually figured out what the right trades were? Even though it's not a general intelligence?\\n\\n1 0:35:52 Well, I think it's possible in chess, but not in the economy, because the value in the economy is being created by creativity and most, you know, arbitrage is one thing that can sort of skim value off the top by taking opportunities that were too expensive for other people to take. So you can, you know, you can make money, you make a lot of money that way if you know, if you have a good idea about how to do it. But most of the value in the economy is created by the creation of knowledge. Somebody has the idea that a smartphone would be good to have, even though most people think that that's not going to work. And that idea cannot be anticipated by anything less than an AGI. An AGI could have that idea, but no AI could. Okay.\\n\\n2 0:36:48 So there's definitely other topics I want to get to. So let's talk about virtual reality. So in the fabric of reality, you discuss the possibility that virtual reality generators could plug in directly into our nervous system and give us sense data that way. Now, as you might know, many meditators, you know, people like Sam Harris, speak of both thoughts and senses as intrusions into consciousness that have a sort of similar, they can be welcome intrusions, but they are both things that come into consciousness. So, do you think that a virtual reality generator could also place thoughts, as well as sense data,\\n\\n1 0:37:24 into the mind? Yes, but that's only because I think that this model is wrong. It's basically the Cartesian theater, as Daniel Dennett puts it, with the stage cleared of all the characters. So, that's pure consciousness without content, as Sam Harris envisages it. But I think that all that's happening there is that you are conscious of this theater, and you\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" our knowledge of, of grammar is my was my favorite example, as we know why certain sentences are acceptable and why they're unacceptable, but we can't state explicitly, or in every case, why it isn't or why it is. And then there's explicit and inexplicit knowledge, there's conscious and unconscious knowledge. All those are bits of program in the brain, they're ideas, they are bits of knowledge in this, if you define knowledge as information with causal power, they are all information with causal power. They all contain truth and they all contain error and it's always a mistake to shield something, to shield one of them from criticism or replacement.\\n\\nNot doing that is what I call the fun criterion. Now you might say that's a bad name, but you know, it's the best I can find.\\n\\n2 1:14:11 So why would creating an AGI through evolution necessarily entail suffering? Because the way I see it, or it seems to me your theory is that you need to be a general intelligence in order to feel suffering, but by the point an evolved, simulated being is a general intelligence, we can just stop the simulation.\\n\\n1 1:14:31 And so where is the suffering coming from? Okay, so the kind of simulation by evolution that I'm thinking of, I mean, there may be several kinds, but the kind that I'm thinking of, and which I said would be the greatest crime in history, is the kind that just simulates the actual evolution of humans from pre humans that weren't people. So you have you have a population of non people which in this simulation would be some kind of NPCs. And then they would they would just evolve we don't know what the criterion would be we just have an artificial universe which simulated the surface of the earth and they'd be walking around and some of them might or might not become people. And now the thing is when you're part of the way there, what is happening is that you have the way that I, the only way that I can imagine the evolution of personhood or create the explanatory creativity happened was that the hardware needed for it was first needed for something else. I have proposed that it was needed to transmit memes. So there'd be people who were transmitting memes creatively but they were running out of resources before it managed to increase their stock of memes. So in every generation, there was a stock of memes that was being passed down to the next generation. And once they got beyond a certain complexity, they had to be passed down by the use of creativity by the recipient.\\n\\nSo, there may well have been a time and as I say I can't think of any other way it could have been, where there was genuine creativity being used, but it ran out of resources, very quickly, but not so quickly that it didn't increase the mean bandwidth. Then in the next generation, there was more mean bandwidth, and then after, you know, certain number of generations, there would have been some opportunity to use this hardware or whatever it is, you know, firmware, I expect, to use this firmware for something other than just trying blindly transmitting memes or rather creatively transmitting memes, but they were blind memes. So in that time, it would have been very unpleasant to be alive. It was already very unpleasant to, sorry, it was very unpleasant to be alive when we did have enough resources to think as well as do the memes. But I don't think there would have been a moment at which you would say, yes, now the suffering begins to matter because it's not just blind memes. I think the people were already suffering at the time when they were blindly transmitting memes because they were using genuine creativity. They were just not using it to any good effect.\\n\\n2 1:17:56 Gotcha. Would being in the experience machine be compatible with the fun criterion so you're not aware that you're in, you know, the experience machine it's all virtual reality, but you're still doing the things that would make you have fun in fact more so than in the real world.\\n\\n1 1:18:15 So, would you be tempted to get into experience machine, what do we can put a compatible with the fun criteria, I mean, is it just a virtual reality world in which things work better than in the real world or something?\\n\\n2 1:18:35 Yeah, so it's a thought experiment by Robert Nozick, and the idea is that you would enter this world, but you would forget that you're in virtual reality. So all, I mean, the world would be perfect in every possible way that it could be perfect or not perfect, but it would be better in every possible way it could be better. But you would think the relationships you have here are real, the knowledge you're discovering here is novel\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\", but it seems to be in an expanded universe are, there will be like a finite amount of total matter that will ever exist in our light cone right there's a limit. And that means that there's a limit on the amount of computation that this matter can, you know, execute the amount of energy can provide, perhaps even the amount of economic value we can sustain right so we, it would be weird if the GDP per atom could be arbitrarily large. So, does this impose some sort of limit on your concept of the beginning of infinity.\\n\\n1 0:50:32 So, what you've just recounted is a cosmological theory. This, this, the universe could be like that. know very little about the universes in the large, like theories of cosmology are changing on a time scale of about a decade. So it doesn't make all that much sense to speculate about what the ultimate asymptotic form of very small things like we know that our conception of physical processes must break down somehow at the level of quantum gravity, like 10 to the minus 42 seconds and that kind of thing but, but we have no idea what happens below There's no, there's no argument for that at all. It's just that we don't know what happens beyond that. Now what happens beyond that may be a finite limit. from it's being imposed by inherent hardware limitations. For example, if there's a finite amount of GNP available in the distant future, then it's still up to us whether we spend that on mathematics or music or political systems or any of the thousands of even more worthwhile things that have yet to be invented. So it's up to us which ideas we fill the 10 to the 10 to the 10 to the 10 bits with. Now, my guess is that there are no such limits, but my worldview is not affected by whether there are such limits, because as I said, it's still up to us what to fill them with. And then if we get chopped off at some point in the future, then everything will have been worthwhile up to then.\\n\\n2 0:53:07 Gotcha. Okay, so the way I understand your concept of gaining infinity, it seems to me that the more knowledge we gain, the more knowledge we're in a position to gain. So there should be like an exponential growth of knowledge. But if we look at the last 50 years, it seems that there's been a slowdown in or decrease in research productivity, economic growth, productivity growth. And this seems compatible with the story that, you know, that there's a limited amount of fruit on the tree that we picked the low hanging fruit. And now there's less and less fruit and harder and harder fruit to pick, and you know eventually well the orchard will be empty. So do you have an alternative explanation for what's going on\\n\\n1 0:53:45 in the last 50 years? Yes, I think it's very simple. There are sociological factors in academic life which have stultified the culture, and not totally and not everywhere, but that has been a tendency in what has happened, and it has resulted in a loss of productivity in many sectors, in many ways, but not in every sector, uh... the the uh... for example i i think that was a i've often said there was a stultification in uh... theoretical physics\\n\\n5\\n\\n0:54:35\\n\\nuh...\\n\\n1 0:54:36 starting in let's say the nineteen twenties and it still hasn't fully dissipated if it wasn't for that quantum computers would have been invented in the 1930s and built in the 1960s. So that is just an accidental fact, but it just goes to show that there are no guarantees. The fact that our horizons are unlimited, does not guarantee that we won't start declining tomorrow. I don't think we are currently declining. I think the these declines that we see are parochial effects caused by specific mistakes that\\n\\n2 0:55:24 have been made and which can be undone. Okay, so I want to ask you a question about Bayesianism versus Popperianism. So one reason why people prefer base is because there seems to be a way of describing changes and changes in epic static status, when their relative status of a theory hasn't changed so give you an example. Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" reality generator could also place thoughts, as well as sense data,\\n\\n1 0:37:24 into the mind? Yes, but that's only because I think that this model is wrong. It's basically the Cartesian theater, as Daniel Dennett puts it, with the stage cleared of all the characters. So, that's pure consciousness without content, as Sam Harris envisages it. But I think that all that's happening there is that you are conscious of this theater, and you're envisaging it as having certain properties, which, by the way, it doesn't have, but that doesn't matter. We can imagine lots of things that don't happen. You know, in fact, you know, that's in a way characterizes what we do all the time. So, one can interpret one's thoughts about this empty stage as being pure, contentless consciousness. But it's not. It has the content of a stage or a space or,\\n\\n2 0:38:40 you know, however you want to envisage it. Okay, and then let's talk about the Turing principle. So this is a term you coined. It's otherwise been called the church touring Deutsch principle. With this principle imply that you could. So, by the way, it states that any universal computer can simulate any physical process with this principle imply that you could simulate the whole of the universe for example in a compact efficient computer that\\n\\n1 0:39:06 was smaller than the universe itself, or is a constraint to physical processes of a task where it was computationally able to do it, but it wouldn't have enough memory or time. So the more memory and time you gave it, the more closely it could simulate the whole universe or anything near the whole universe probably because it, well, if you wanted to simulate itself as well, then there are logical reasons why there are limits to that. But even if you wanted to simulate the whole universe apart from itself, just the sheer size of the universe makes that impossible, even if we discovered ways of encoding information extremely densely like some people have said maybe quantum gravity would allow you know the rest of the universe as well so. from being limited by computational capacity, because it's only when you separate those that you realize what computational universality is. And I think that's universality, like Turing or quantum universality is the most important thing in the theory of computation, because computation doesn't even make sense, unless you have a concept of a universal computer.\\n\\n2 0:41:20 What could falsify your theory that all interesting problems are soluble so I asked this because, as I'm not convinced they're right. But do you have a strong reason for in principle believing that they're wrong.\\n\\n1 0:41:47 No. So, this, this is a bad explanation. So let's say that some people say, for example, that simulating a human brain is impossible. Now, I can't prove this possible. Nobody can prove this possible until they actually do it or unless they have a design for it which they prove will work. So, pending that there is there is no way of proving that, that it's not true that this is a fundamental limitation. is that it could be applied to anything. For example, it could be applied to the theory that you have recently, just a minute ago, been replaced by a humanoid robot, which is going to say for the next few minutes, just a prearranged set of things, and you're no longer a person. I can't believe you figured it out. Yeah, well, that's the first thing you'd say. So there is no way to refute that by experiment, short of actually doing it, short of actually talking to you and so on. So it's the same with all these other things. In order for it to make sense to have a theory that something is impossible, you have to have an explanation for why it is impossible. So we know that, for example, almost all mathematical propositions are undecidable. So that's not because somebody has said, oh maybe we can't decide everything because thinking we could decide everything is hubris. That's not an argument.\\n\\nYou need an actual functional argument to prove that that is so, and then, at being a functional argument in which the steps of the argument makes sense and relate to other things and so on. You can then say well what does this actually mean. Does this mean that maybe we can never understand the laws of physics included an undecidable function, then we would simply write, you know, f of x and f of x and undecidable function. We couldn't evaluate x it would limit our ability to make predictions, but then, lots of our ability to make predictions is totally limited anyway. to understand the properties of the function f and therefore the\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.\\n\\nDespite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force, laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.\\n\\nTuring fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.\\n\\nThis astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.\\n\\nWhat is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations\\n\\nBut it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.\\n\\nWhy? I call the core functionality in question creativity: the ability to produce new explanations. For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.\\n\\nNow imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.\\n\\nSuch a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!\\n\\nI’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey. Courtesy MGM Traditionally, discussions of AGI have evaded that issue by imagining only a test', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" reality generator could also place thoughts, as well as sense data,\\n\\n1 0:37:24 into the mind? Yes, but that's only because I think that this model is wrong. It's basically the Cartesian theater, as Daniel Dennett puts it, with the stage cleared of all the characters. So, that's pure consciousness without content, as Sam Harris envisages it. But I think that all that's happening there is that you are conscious of this theater, and you're envisaging it as having certain properties, which, by the way, it doesn't have, but that doesn't matter. We can imagine lots of things that don't happen. You know, in fact, you know, that's in a way characterizes what we do all the time. So, one can interpret one's thoughts about this empty stage as being pure, contentless consciousness. But it's not. It has the content of a stage or a space or,\\n\\n2 0:38:40 you know, however you want to envisage it. Okay, and then let's talk about the Turing principle. So this is a term you coined. It's otherwise been called the church touring Deutsch principle. With this principle imply that you could. So, by the way, it states that any universal computer can simulate any physical process with this principle imply that you could simulate the whole of the universe for example in a compact efficient computer that\\n\\n1 0:39:06 was smaller than the universe itself, or is a constraint to physical processes of a task where it was computationally able to do it, but it wouldn't have enough memory or time. So the more memory and time you gave it, the more closely it could simulate the whole universe or anything near the whole universe probably because it, well, if you wanted to simulate itself as well, then there are logical reasons why there are limits to that. But even if you wanted to simulate the whole universe apart from itself, just the sheer size of the universe makes that impossible, even if we discovered ways of encoding information extremely densely like some people have said maybe quantum gravity would allow you know the rest of the universe as well so. from being limited by computational capacity, because it's only when you separate those that you realize what computational universality is. And I think that's universality, like Turing or quantum universality is the most important thing in the theory of computation, because computation doesn't even make sense, unless you have a concept of a universal computer.\\n\\n2 0:41:20 What could falsify your theory that all interesting problems are soluble so I asked this because, as I'm not convinced they're right. But do you have a strong reason for in principle believing that they're wrong.\\n\\n1 0:41:47 No. So, this, this is a bad explanation. So let's say that some people say, for example, that simulating a human brain is impossible. Now, I can't prove this possible. Nobody can prove this possible until they actually do it or unless they have a design for it which they prove will work. So, pending that there is there is no way of proving that, that it's not true that this is a fundamental limitation. is that it could be applied to anything. For example, it could be applied to the theory that you have recently, just a minute ago, been replaced by a humanoid robot, which is going to say for the next few minutes, just a prearranged set of things, and you're no longer a person. I can't believe you figured it out. Yeah, well, that's the first thing you'd say. So there is no way to refute that by experiment, short of actually doing it, short of actually talking to you and so on. So it's the same with all these other things. In order for it to make sense to have a theory that something is impossible, you have to have an explanation for why it is impossible. So we know that, for example, almost all mathematical propositions are undecidable. So that's not because somebody has said, oh maybe we can't decide everything because thinking we could decide everything is hubris. That's not an argument.\\n\\nYou need an actual functional argument to prove that that is so, and then, at being a functional argument in which the steps of the argument makes sense and relate to other things and so on. You can then say well what does this actually mean. Does this mean that maybe we can never understand the laws of physics included an undecidable function, then we would simply write, you know, f of x and f of x and undecidable function. We couldn't evaluate x it would limit our ability to make predictions, but then, lots of our ability to make predictions is totally limited anyway. to understand the properties of the function f and therefore the\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\", but it seems to be in an expanded universe are, there will be like a finite amount of total matter that will ever exist in our light cone right there's a limit. And that means that there's a limit on the amount of computation that this matter can, you know, execute the amount of energy can provide, perhaps even the amount of economic value we can sustain right so we, it would be weird if the GDP per atom could be arbitrarily large. So, does this impose some sort of limit on your concept of the beginning of infinity.\\n\\n1 0:50:32 So, what you've just recounted is a cosmological theory. This, this, the universe could be like that. know very little about the universes in the large, like theories of cosmology are changing on a time scale of about a decade. So it doesn't make all that much sense to speculate about what the ultimate asymptotic form of very small things like we know that our conception of physical processes must break down somehow at the level of quantum gravity, like 10 to the minus 42 seconds and that kind of thing but, but we have no idea what happens below There's no, there's no argument for that at all. It's just that we don't know what happens beyond that. Now what happens beyond that may be a finite limit. from it's being imposed by inherent hardware limitations. For example, if there's a finite amount of GNP available in the distant future, then it's still up to us whether we spend that on mathematics or music or political systems or any of the thousands of even more worthwhile things that have yet to be invented. So it's up to us which ideas we fill the 10 to the 10 to the 10 to the 10 bits with. Now, my guess is that there are no such limits, but my worldview is not affected by whether there are such limits, because as I said, it's still up to us what to fill them with. And then if we get chopped off at some point in the future, then everything will have been worthwhile up to then.\\n\\n2 0:53:07 Gotcha. Okay, so the way I understand your concept of gaining infinity, it seems to me that the more knowledge we gain, the more knowledge we're in a position to gain. So there should be like an exponential growth of knowledge. But if we look at the last 50 years, it seems that there's been a slowdown in or decrease in research productivity, economic growth, productivity growth. And this seems compatible with the story that, you know, that there's a limited amount of fruit on the tree that we picked the low hanging fruit. And now there's less and less fruit and harder and harder fruit to pick, and you know eventually well the orchard will be empty. So do you have an alternative explanation for what's going on\\n\\n1 0:53:45 in the last 50 years? Yes, I think it's very simple. There are sociological factors in academic life which have stultified the culture, and not totally and not everywhere, but that has been a tendency in what has happened, and it has resulted in a loss of productivity in many sectors, in many ways, but not in every sector, uh... the the uh... for example i i think that was a i've often said there was a stultification in uh... theoretical physics\\n\\n5\\n\\n0:54:35\\n\\nuh...\\n\\n1 0:54:36 starting in let's say the nineteen twenties and it still hasn't fully dissipated if it wasn't for that quantum computers would have been invented in the 1930s and built in the 1960s. So that is just an accidental fact, but it just goes to show that there are no guarantees. The fact that our horizons are unlimited, does not guarantee that we won't start declining tomorrow. I don't think we are currently declining. I think the these declines that we see are parochial effects caused by specific mistakes that\\n\\n2 0:55:24 have been made and which can be undone. Okay, so I want to ask you a question about Bayesianism versus Popperianism. So one reason why people prefer base is because there seems to be a way of describing changes and changes in epic static status, when their relative status of a theory hasn't changed so give you an example. Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" our knowledge of, of grammar is my was my favorite example, as we know why certain sentences are acceptable and why they're unacceptable, but we can't state explicitly, or in every case, why it isn't or why it is. And then there's explicit and inexplicit knowledge, there's conscious and unconscious knowledge. All those are bits of program in the brain, they're ideas, they are bits of knowledge in this, if you define knowledge as information with causal power, they are all information with causal power. They all contain truth and they all contain error and it's always a mistake to shield something, to shield one of them from criticism or replacement.\\n\\nNot doing that is what I call the fun criterion. Now you might say that's a bad name, but you know, it's the best I can find.\\n\\n2 1:14:11 So why would creating an AGI through evolution necessarily entail suffering? Because the way I see it, or it seems to me your theory is that you need to be a general intelligence in order to feel suffering, but by the point an evolved, simulated being is a general intelligence, we can just stop the simulation.\\n\\n1 1:14:31 And so where is the suffering coming from? Okay, so the kind of simulation by evolution that I'm thinking of, I mean, there may be several kinds, but the kind that I'm thinking of, and which I said would be the greatest crime in history, is the kind that just simulates the actual evolution of humans from pre humans that weren't people. So you have you have a population of non people which in this simulation would be some kind of NPCs. And then they would they would just evolve we don't know what the criterion would be we just have an artificial universe which simulated the surface of the earth and they'd be walking around and some of them might or might not become people. And now the thing is when you're part of the way there, what is happening is that you have the way that I, the only way that I can imagine the evolution of personhood or create the explanatory creativity happened was that the hardware needed for it was first needed for something else. I have proposed that it was needed to transmit memes. So there'd be people who were transmitting memes creatively but they were running out of resources before it managed to increase their stock of memes. So in every generation, there was a stock of memes that was being passed down to the next generation. And once they got beyond a certain complexity, they had to be passed down by the use of creativity by the recipient.\\n\\nSo, there may well have been a time and as I say I can't think of any other way it could have been, where there was genuine creativity being used, but it ran out of resources, very quickly, but not so quickly that it didn't increase the mean bandwidth. Then in the next generation, there was more mean bandwidth, and then after, you know, certain number of generations, there would have been some opportunity to use this hardware or whatever it is, you know, firmware, I expect, to use this firmware for something other than just trying blindly transmitting memes or rather creatively transmitting memes, but they were blind memes. So in that time, it would have been very unpleasant to be alive. It was already very unpleasant to, sorry, it was very unpleasant to be alive when we did have enough resources to think as well as do the memes. But I don't think there would have been a moment at which you would say, yes, now the suffering begins to matter because it's not just blind memes. I think the people were already suffering at the time when they were blindly transmitting memes because they were using genuine creativity. They were just not using it to any good effect.\\n\\n2 1:17:56 Gotcha. Would being in the experience machine be compatible with the fun criterion so you're not aware that you're in, you know, the experience machine it's all virtual reality, but you're still doing the things that would make you have fun in fact more so than in the real world.\\n\\n1 1:18:15 So, would you be tempted to get into experience machine, what do we can put a compatible with the fun criteria, I mean, is it just a virtual reality world in which things work better than in the real world or something?\\n\\n2 1:18:35 Yeah, so it's a thought experiment by Robert Nozick, and the idea is that you would enter this world, but you would forget that you're in virtual reality. So all, I mean, the world would be perfect in every possible way that it could be perfect or not perfect, but it would be better in every possible way it could be better. But you would think the relationships you have here are real, the knowledge you're discovering here is novel\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" to other things and so on. You can then say well what does this actually mean. Does this mean that maybe we can never understand the laws of physics included an undecidable function, then we would simply write, you know, f of x and f of x and undecidable function. We couldn't evaluate x it would limit our ability to make predictions, but then, lots of our ability to make predictions is totally limited anyway. to understand the properties of the function f and therefore the properties of the physical world. Okay, is a system of government like America's which has distributed powers and checks and\\n\\n2 0:44:57 balances, is that incompatible with Popper's criterion? So the reason I ask is the last administration had a theory that if you build a wall there will be positive consequences and you know that theory could have been tested and then the person could have been evaluated on whether that theory succeeded. But because our system of government has distributed powers, you know, Congress opposed the testing of that theory, and so it was never tested. So if our American government wanted to fulfill Popper's criterion, would we need to give the president more power, for example?\\n\\n1 0:45:30 It's not as simple as that. So I agree that this is a big defect in the American system No country has a system of government that perfectly fulfills Popper's criterion. We can always improve. I think the British one is actually the best in the world and it's far from optimal. Making a single change like that is not going to be the answer. The constitution of a polity is a very complicated thing, much of which is inexplicit. So, the founding fathers, the American founding fathers realized they had a tremendous problem. What they wanted to do, what they thought of themselves as doing, was to implement the British Constitution. In fact they thought they were the defenders of the British Constitution and that the British King had violated it and was bringing it down. They wanted to retain it. The trouble is that they all, in order to do this, to gain the independence to do this, they had to get rid of the King and then they wondered whether they should get an alternative king. Whichever way they did it, there were problems. The way they decided to do it, I think, made for a system that was inherently much worse than the one they were replacing, but they had no choice. If they wanted to get rid of a king, they had to have a different system for having a head of state. Therefore they had to have... they wanted to be democratic. That meant that the president had a legitimacy in legislation that the king never had. Oh sorry, never had.\\n\\nThe king did used to have it in medieval times, but the king by the time of the Enlightenment and so on, no longer had full legitimacy to legislate. So they had to implement a system where him seizing power was prevented by something other than tradition. And so they instituted these checks and balances. Checks and... so the whole thing the institute was immensely sophisticated. It's an amazing intellectual achievement and that it works as well as it does is something... something of a miracle. But the inherent flaws are there and one of them is this... the fact that there are checks and balances means that responsibility is dissipated and nobody is ever to blame for anything in the American system, which is terrible. In the British system, blame is absolutely focused, you know, everything is sacrificed to the end of focusing blame and responsibility down to the government. That's where it's all focused into. And there are no systems that do that better, but as you well know, the British system also has flaws. And we recently saw with the sequence of events with Brexit referendum and then Parliament balking at implementing some laws they didn't agree with. And then that being referred to the courts. And so there was the courts and the Parliament and the government and the Prime Minister all blaming each other.\\n\\nAnd there was a sort of mini constitutional crisis, which could only be resolved by having an election and then having a majority government, which is by the mathematics of how the government works, that's how it usually is in Britain. Although, you know, we have been unlucky several times recently in not having a majority government.\\n\\n2 0:49:56 Okay, so this could be wrong, but it seems to be in an expanded universe are, there will be like a finite amount of total matter that will ever exist in our light cone right there's a limit. And that means that there's a limit on the amount of computation that this matter can, you know, execute the amount of energy can provide, perhaps even the amount of economic value we can sustain right so we, it would be weird if the GDP per atom could be arbitrarily large. So, does this impose some sort\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I mean, obviously it wouldn't be enough to knock down every possible explanation because for a start we know that quantum theory is false. We don't know for sure that the next theory will have many worlds in it I mean I think it will, but, but, you know, We can't prove anything like that. But I would replace the idea of increased credence with a theory that the experiment will provide a quiver full of arrows or a repertoire of arguments that goes beyond the known arguments, the known bad arguments, and will reach into other types of arguments because the reason I would say that is that Some of the existing misconceptions about quantum theory reside in misconceptions about the methodology of science. Now I've written a paper about what I think is the right methodology of science that's more based on empiricism. Of course, I think that empiricism is a mistake and can be knocked down in its own terms, so we shouldn't, but not everybody thinks that. Now, once we have an experiment, if that was actually done, then people could not use their arguments based on a fallacious idea of empiricism, because their theory would have been refuted even by the standards of empiricism, which shouldn't have been needed in the first place.\\n\\nI think that that's the way I would express that the repertoire of arguments will become more powerful. If that experiment would own successfully.\\n\\n2 0:59:22 The next question I have is, how far do you take the principle that open ended scientific progress is the best way to deal with existential dangers to give it one example. So you have something like gain of function research, right? And it's conceivable that it could lead to more knowledge in how to stop dangerous pathogens. But I guess at least in Bayesian terms, you could say it seems even more likely that it can or has led to the spread of a man-made pathogen that would have not otherwise been naturally developed. So would your belief in open and assigned to progress allow us to say okay let's stop Dana function research.\\n\\n1 1:00:03 No, it wouldn't allow us to say let's stop it. It might make it reasonable to say, let us do research into how to make laboratories more secure before we do gain a function research. It's really part of the same thing it's it's it's like saying, let's do research into how to make the plastic hoses, through which the reagents pass more impermeable before we actually do the experiments with the reagents. So it's all part of the same experiment. I wouldn't want to stop something just because new knowledge might be discovered. the no-no in my view. But which knowledge we need to discover first, that's the problem of scheduling, which is a non-trivial part of any research and of any learning.\\n\\n2 1:00:59 But would it be considerable for you to say that until we figure out how to make sure these laboratories are safe to a certain standard, we will. Meanwhile we'll focus on doing the other kind of research so gain of function can restart, but until then it's not allowed.\\n\\n1 1:01:23 Yes, in principle that will be reasonable I don't know enough about the actual situation to have a view. You know, I don't know how these labs work. what the precautions consist of. And when I hear people talking about, for example, lab leak, I think, well, most likely lab leak is that one of the people who works there walks out of the front door. So the leak is not a leak from the lab to the outside. The leak is from the test tube to the person and then from the person walking out the door. And I don't know enough about what these proportions are or what the state of the art is to know to what extent the risk is actually minimized. It could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I had Tyler Cowen on my podcast, I asked him why he thinks, so he thinks that human civilization is only going to be around for 700 more years, and then so I asked him, I gave him, you know, your rebuttal, or what I understand of your rebuttal, that, you know, creative, optimistic societies will innovate ways of, you know, safety technology is faster than totalitarian static societies can innovate destructive technologies. And he responded, you know, maybe, but the cost of destruction is just so much lower than the cost of building. And, you know, that trend has been going on for a while now. What happens when a new cost, $60,000? Or what happens if there's a mistake like the kinds that we saw many times over in the Cold War? How would you respond to that?\\n\\n1 1:03:38 First of all, I think we've been getting safer and safer throughout the entire history of civilization. The, you know, there were these plagues that wiped out a third of the population of the world or half. And it could have been 99% or 100%. went through some kind of bottleneck 70,000 years ago, I understand, which they can tell from genetics, all our cousin species have been wiped out. So we were much less safe then than now. Also, if a asteroid, 10-kilometer asteroid, had been on target with the earth at any time in the in the past two million year or whatever it is history of the genus homo that would have been the end of it whereas now it'll just mean higher taxation for a while you know that that's the that's how much amazingly safer we are now uh i i would never say that it's impossible that we'll destroy ourselves. That would be the contrary to the universality of the human mind. We can make wrong choices. We can make so many wrong choices that we'll destroy ourselves. And on the other hand, the atomic bomb accident sort of thing would have had no zero chance of destroying civilization. All they would have done is cause a vast amount of suffering. And... but I don't think we have the technology to end civilization even if we wanted to. I think all we would do if we just deliberately unleashed hell all over the world is we would cause a vast amount of suffering. But there would be survivors and they would resolve never to do that again.\\n\\nSo I don't think we're even able to, let alone that we would do it accidentally. But as for the bad guys, well, I think we are doing the wrong thing largely in regard to both external and internal threats. But I don't think we're doing the wrong thing to an existential risk level. And over the next 700 years or whatever it is, well, I don't want to prophesy because I don't know most of the advances that are going to be made in that time. I see no reason why if we are solving problems, we won't solve problems. I don't think this, this, to take another metaphor, Nick Bostrom's jar with white balls and there's one black ball, and you take out a white ball and white ball and white ball, and then you hit the black ball and that's the end of you. I don't think it's like that because every white ball you take out and have reduces the number of black balls in the jar. So again I'm not saying that's the law of nature. It could be that the very next ball we take out will be the black one that'll be the end of us. It could be but I think all arguments that it will be are fallacious.\\n\\n2 1:07:17 I do want to talk about the fun criterion. Is your definition of fun different from how other people define other positive emotions like eudaemonia or well being or satisfaction? Is it fun a different emotion?\\n\\n1 1:07:29 I don't think it's an emotion. And and I all these things are not very well defined. They can't possibly be very well defined until we have a satisfactory theory of qualia, at least, and probably more satisfactory theory of creativity, how creativity works and so on. I think that the choice of the word fun for the thing that I explain more precisely, but still not very precisely, as a creation of knowledge without,\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\", but it seems to be in an expanded universe are, there will be like a finite amount of total matter that will ever exist in our light cone right there's a limit. And that means that there's a limit on the amount of computation that this matter can, you know, execute the amount of energy can provide, perhaps even the amount of economic value we can sustain right so we, it would be weird if the GDP per atom could be arbitrarily large. So, does this impose some sort of limit on your concept of the beginning of infinity.\\n\\n1 0:50:32 So, what you've just recounted is a cosmological theory. This, this, the universe could be like that. know very little about the universes in the large, like theories of cosmology are changing on a time scale of about a decade. So it doesn't make all that much sense to speculate about what the ultimate asymptotic form of very small things like we know that our conception of physical processes must break down somehow at the level of quantum gravity, like 10 to the minus 42 seconds and that kind of thing but, but we have no idea what happens below There's no, there's no argument for that at all. It's just that we don't know what happens beyond that. Now what happens beyond that may be a finite limit. from it's being imposed by inherent hardware limitations. For example, if there's a finite amount of GNP available in the distant future, then it's still up to us whether we spend that on mathematics or music or political systems or any of the thousands of even more worthwhile things that have yet to be invented. So it's up to us which ideas we fill the 10 to the 10 to the 10 to the 10 bits with. Now, my guess is that there are no such limits, but my worldview is not affected by whether there are such limits, because as I said, it's still up to us what to fill them with. And then if we get chopped off at some point in the future, then everything will have been worthwhile up to then.\\n\\n2 0:53:07 Gotcha. Okay, so the way I understand your concept of gaining infinity, it seems to me that the more knowledge we gain, the more knowledge we're in a position to gain. So there should be like an exponential growth of knowledge. But if we look at the last 50 years, it seems that there's been a slowdown in or decrease in research productivity, economic growth, productivity growth. And this seems compatible with the story that, you know, that there's a limited amount of fruit on the tree that we picked the low hanging fruit. And now there's less and less fruit and harder and harder fruit to pick, and you know eventually well the orchard will be empty. So do you have an alternative explanation for what's going on\\n\\n1 0:53:45 in the last 50 years? Yes, I think it's very simple. There are sociological factors in academic life which have stultified the culture, and not totally and not everywhere, but that has been a tendency in what has happened, and it has resulted in a loss of productivity in many sectors, in many ways, but not in every sector, uh... the the uh... for example i i think that was a i've often said there was a stultification in uh... theoretical physics\\n\\n5\\n\\n0:54:35\\n\\nuh...\\n\\n1 0:54:36 starting in let's say the nineteen twenties and it still hasn't fully dissipated if it wasn't for that quantum computers would have been invented in the 1930s and built in the 1960s. So that is just an accidental fact, but it just goes to show that there are no guarantees. The fact that our horizons are unlimited, does not guarantee that we won't start declining tomorrow. I don't think we are currently declining. I think the these declines that we see are parochial effects caused by specific mistakes that\\n\\n2 0:55:24 have been made and which can be undone. Okay, so I want to ask you a question about Bayesianism versus Popperianism. So one reason why people prefer base is because there seems to be a way of describing changes and changes in epic static status, when their relative status of a theory hasn't changed so give you an example. Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" to other things and so on. You can then say well what does this actually mean. Does this mean that maybe we can never understand the laws of physics included an undecidable function, then we would simply write, you know, f of x and f of x and undecidable function. We couldn't evaluate x it would limit our ability to make predictions, but then, lots of our ability to make predictions is totally limited anyway. to understand the properties of the function f and therefore the properties of the physical world. Okay, is a system of government like America's which has distributed powers and checks and\\n\\n2 0:44:57 balances, is that incompatible with Popper's criterion? So the reason I ask is the last administration had a theory that if you build a wall there will be positive consequences and you know that theory could have been tested and then the person could have been evaluated on whether that theory succeeded. But because our system of government has distributed powers, you know, Congress opposed the testing of that theory, and so it was never tested. So if our American government wanted to fulfill Popper's criterion, would we need to give the president more power, for example?\\n\\n1 0:45:30 It's not as simple as that. So I agree that this is a big defect in the American system No country has a system of government that perfectly fulfills Popper's criterion. We can always improve. I think the British one is actually the best in the world and it's far from optimal. Making a single change like that is not going to be the answer. The constitution of a polity is a very complicated thing, much of which is inexplicit. So, the founding fathers, the American founding fathers realized they had a tremendous problem. What they wanted to do, what they thought of themselves as doing, was to implement the British Constitution. In fact they thought they were the defenders of the British Constitution and that the British King had violated it and was bringing it down. They wanted to retain it. The trouble is that they all, in order to do this, to gain the independence to do this, they had to get rid of the King and then they wondered whether they should get an alternative king. Whichever way they did it, there were problems. The way they decided to do it, I think, made for a system that was inherently much worse than the one they were replacing, but they had no choice. If they wanted to get rid of a king, they had to have a different system for having a head of state. Therefore they had to have... they wanted to be democratic. That meant that the president had a legitimacy in legislation that the king never had. Oh sorry, never had.\\n\\nThe king did used to have it in medieval times, but the king by the time of the Enlightenment and so on, no longer had full legitimacy to legislate. So they had to implement a system where him seizing power was prevented by something other than tradition. And so they instituted these checks and balances. Checks and... so the whole thing the institute was immensely sophisticated. It's an amazing intellectual achievement and that it works as well as it does is something... something of a miracle. But the inherent flaws are there and one of them is this... the fact that there are checks and balances means that responsibility is dissipated and nobody is ever to blame for anything in the American system, which is terrible. In the British system, blame is absolutely focused, you know, everything is sacrificed to the end of focusing blame and responsibility down to the government. That's where it's all focused into. And there are no systems that do that better, but as you well know, the British system also has flaws. And we recently saw with the sequence of events with Brexit referendum and then Parliament balking at implementing some laws they didn't agree with. And then that being referred to the courts. And so there was the courts and the Parliament and the government and the Prime Minister all blaming each other.\\n\\nAnd there was a sort of mini constitutional crisis, which could only be resolved by having an election and then having a majority government, which is by the mathematics of how the government works, that's how it usually is in Britain. Although, you know, we have been unlucky several times recently in not having a majority government.\\n\\n2 0:49:56 Okay, so this could be wrong, but it seems to be in an expanded universe are, there will be like a finite amount of total matter that will ever exist in our light cone right there's a limit. And that means that there's a limit on the amount of computation that this matter can, you know, execute the amount of energy can provide, perhaps even the amount of economic value we can sustain right so we, it would be weird if the GDP per atom could be arbitrarily large. So, does this impose some sort\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" reality generator could also place thoughts, as well as sense data,\\n\\n1 0:37:24 into the mind? Yes, but that's only because I think that this model is wrong. It's basically the Cartesian theater, as Daniel Dennett puts it, with the stage cleared of all the characters. So, that's pure consciousness without content, as Sam Harris envisages it. But I think that all that's happening there is that you are conscious of this theater, and you're envisaging it as having certain properties, which, by the way, it doesn't have, but that doesn't matter. We can imagine lots of things that don't happen. You know, in fact, you know, that's in a way characterizes what we do all the time. So, one can interpret one's thoughts about this empty stage as being pure, contentless consciousness. But it's not. It has the content of a stage or a space or,\\n\\n2 0:38:40 you know, however you want to envisage it. Okay, and then let's talk about the Turing principle. So this is a term you coined. It's otherwise been called the church touring Deutsch principle. With this principle imply that you could. So, by the way, it states that any universal computer can simulate any physical process with this principle imply that you could simulate the whole of the universe for example in a compact efficient computer that\\n\\n1 0:39:06 was smaller than the universe itself, or is a constraint to physical processes of a task where it was computationally able to do it, but it wouldn't have enough memory or time. So the more memory and time you gave it, the more closely it could simulate the whole universe or anything near the whole universe probably because it, well, if you wanted to simulate itself as well, then there are logical reasons why there are limits to that. But even if you wanted to simulate the whole universe apart from itself, just the sheer size of the universe makes that impossible, even if we discovered ways of encoding information extremely densely like some people have said maybe quantum gravity would allow you know the rest of the universe as well so. from being limited by computational capacity, because it's only when you separate those that you realize what computational universality is. And I think that's universality, like Turing or quantum universality is the most important thing in the theory of computation, because computation doesn't even make sense, unless you have a concept of a universal computer.\\n\\n2 0:41:20 What could falsify your theory that all interesting problems are soluble so I asked this because, as I'm not convinced they're right. But do you have a strong reason for in principle believing that they're wrong.\\n\\n1 0:41:47 No. So, this, this is a bad explanation. So let's say that some people say, for example, that simulating a human brain is impossible. Now, I can't prove this possible. Nobody can prove this possible until they actually do it or unless they have a design for it which they prove will work. So, pending that there is there is no way of proving that, that it's not true that this is a fundamental limitation. is that it could be applied to anything. For example, it could be applied to the theory that you have recently, just a minute ago, been replaced by a humanoid robot, which is going to say for the next few minutes, just a prearranged set of things, and you're no longer a person. I can't believe you figured it out. Yeah, well, that's the first thing you'd say. So there is no way to refute that by experiment, short of actually doing it, short of actually talking to you and so on. So it's the same with all these other things. In order for it to make sense to have a theory that something is impossible, you have to have an explanation for why it is impossible. So we know that, for example, almost all mathematical propositions are undecidable. So that's not because somebody has said, oh maybe we can't decide everything because thinking we could decide everything is hubris. That's not an argument.\\n\\nYou need an actual functional argument to prove that that is so, and then, at being a functional argument in which the steps of the argument makes sense and relate to other things and so on. You can then say well what does this actually mean. Does this mean that maybe we can never understand the laws of physics included an undecidable function, then we would simply write, you know, f of x and f of x and undecidable function. We couldn't evaluate x it would limit our ability to make predictions, but then, lots of our ability to make predictions is totally limited anyway. to understand the properties of the function f and therefore the\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I had Tyler Cowen on my podcast, I asked him why he thinks, so he thinks that human civilization is only going to be around for 700 more years, and then so I asked him, I gave him, you know, your rebuttal, or what I understand of your rebuttal, that, you know, creative, optimistic societies will innovate ways of, you know, safety technology is faster than totalitarian static societies can innovate destructive technologies. And he responded, you know, maybe, but the cost of destruction is just so much lower than the cost of building. And, you know, that trend has been going on for a while now. What happens when a new cost, $60,000? Or what happens if there's a mistake like the kinds that we saw many times over in the Cold War? How would you respond to that?\\n\\n1 1:03:38 First of all, I think we've been getting safer and safer throughout the entire history of civilization. The, you know, there were these plagues that wiped out a third of the population of the world or half. And it could have been 99% or 100%. went through some kind of bottleneck 70,000 years ago, I understand, which they can tell from genetics, all our cousin species have been wiped out. So we were much less safe then than now. Also, if a asteroid, 10-kilometer asteroid, had been on target with the earth at any time in the in the past two million year or whatever it is history of the genus homo that would have been the end of it whereas now it'll just mean higher taxation for a while you know that that's the that's how much amazingly safer we are now uh i i would never say that it's impossible that we'll destroy ourselves. That would be the contrary to the universality of the human mind. We can make wrong choices. We can make so many wrong choices that we'll destroy ourselves. And on the other hand, the atomic bomb accident sort of thing would have had no zero chance of destroying civilization. All they would have done is cause a vast amount of suffering. And... but I don't think we have the technology to end civilization even if we wanted to. I think all we would do if we just deliberately unleashed hell all over the world is we would cause a vast amount of suffering. But there would be survivors and they would resolve never to do that again.\\n\\nSo I don't think we're even able to, let alone that we would do it accidentally. But as for the bad guys, well, I think we are doing the wrong thing largely in regard to both external and internal threats. But I don't think we're doing the wrong thing to an existential risk level. And over the next 700 years or whatever it is, well, I don't want to prophesy because I don't know most of the advances that are going to be made in that time. I see no reason why if we are solving problems, we won't solve problems. I don't think this, this, to take another metaphor, Nick Bostrom's jar with white balls and there's one black ball, and you take out a white ball and white ball and white ball, and then you hit the black ball and that's the end of you. I don't think it's like that because every white ball you take out and have reduces the number of black balls in the jar. So again I'm not saying that's the law of nature. It could be that the very next ball we take out will be the black one that'll be the end of us. It could be but I think all arguments that it will be are fallacious.\\n\\n2 1:07:17 I do want to talk about the fun criterion. Is your definition of fun different from how other people define other positive emotions like eudaemonia or well being or satisfaction? Is it fun a different emotion?\\n\\n1 1:07:29 I don't think it's an emotion. And and I all these things are not very well defined. They can't possibly be very well defined until we have a satisfactory theory of qualia, at least, and probably more satisfactory theory of creativity, how creativity works and so on. I think that the choice of the word fun for the thing that I explain more precisely, but still not very precisely, as a creation of knowledge without,\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I mean, obviously it wouldn't be enough to knock down every possible explanation because for a start we know that quantum theory is false. We don't know for sure that the next theory will have many worlds in it I mean I think it will, but, but, you know, We can't prove anything like that. But I would replace the idea of increased credence with a theory that the experiment will provide a quiver full of arrows or a repertoire of arguments that goes beyond the known arguments, the known bad arguments, and will reach into other types of arguments because the reason I would say that is that Some of the existing misconceptions about quantum theory reside in misconceptions about the methodology of science. Now I've written a paper about what I think is the right methodology of science that's more based on empiricism. Of course, I think that empiricism is a mistake and can be knocked down in its own terms, so we shouldn't, but not everybody thinks that. Now, once we have an experiment, if that was actually done, then people could not use their arguments based on a fallacious idea of empiricism, because their theory would have been refuted even by the standards of empiricism, which shouldn't have been needed in the first place.\\n\\nI think that that's the way I would express that the repertoire of arguments will become more powerful. If that experiment would own successfully.\\n\\n2 0:59:22 The next question I have is, how far do you take the principle that open ended scientific progress is the best way to deal with existential dangers to give it one example. So you have something like gain of function research, right? And it's conceivable that it could lead to more knowledge in how to stop dangerous pathogens. But I guess at least in Bayesian terms, you could say it seems even more likely that it can or has led to the spread of a man-made pathogen that would have not otherwise been naturally developed. So would your belief in open and assigned to progress allow us to say okay let's stop Dana function research.\\n\\n1 1:00:03 No, it wouldn't allow us to say let's stop it. It might make it reasonable to say, let us do research into how to make laboratories more secure before we do gain a function research. It's really part of the same thing it's it's it's like saying, let's do research into how to make the plastic hoses, through which the reagents pass more impermeable before we actually do the experiments with the reagents. So it's all part of the same experiment. I wouldn't want to stop something just because new knowledge might be discovered. the no-no in my view. But which knowledge we need to discover first, that's the problem of scheduling, which is a non-trivial part of any research and of any learning.\\n\\n2 1:00:59 But would it be considerable for you to say that until we figure out how to make sure these laboratories are safe to a certain standard, we will. Meanwhile we'll focus on doing the other kind of research so gain of function can restart, but until then it's not allowed.\\n\\n1 1:01:23 Yes, in principle that will be reasonable I don't know enough about the actual situation to have a view. You know, I don't know how these labs work. what the precautions consist of. And when I hear people talking about, for example, lab leak, I think, well, most likely lab leak is that one of the people who works there walks out of the front door. So the leak is not a leak from the lab to the outside. The leak is from the test tube to the person and then from the person walking out the door. And I don't know enough about what these proportions are or what the state of the art is to know to what extent the risk is actually minimized. It could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\", but it seems to be in an expanded universe are, there will be like a finite amount of total matter that will ever exist in our light cone right there's a limit. And that means that there's a limit on the amount of computation that this matter can, you know, execute the amount of energy can provide, perhaps even the amount of economic value we can sustain right so we, it would be weird if the GDP per atom could be arbitrarily large. So, does this impose some sort of limit on your concept of the beginning of infinity.\\n\\n1 0:50:32 So, what you've just recounted is a cosmological theory. This, this, the universe could be like that. know very little about the universes in the large, like theories of cosmology are changing on a time scale of about a decade. So it doesn't make all that much sense to speculate about what the ultimate asymptotic form of very small things like we know that our conception of physical processes must break down somehow at the level of quantum gravity, like 10 to the minus 42 seconds and that kind of thing but, but we have no idea what happens below There's no, there's no argument for that at all. It's just that we don't know what happens beyond that. Now what happens beyond that may be a finite limit. from it's being imposed by inherent hardware limitations. For example, if there's a finite amount of GNP available in the distant future, then it's still up to us whether we spend that on mathematics or music or political systems or any of the thousands of even more worthwhile things that have yet to be invented. So it's up to us which ideas we fill the 10 to the 10 to the 10 to the 10 bits with. Now, my guess is that there are no such limits, but my worldview is not affected by whether there are such limits, because as I said, it's still up to us what to fill them with. And then if we get chopped off at some point in the future, then everything will have been worthwhile up to then.\\n\\n2 0:53:07 Gotcha. Okay, so the way I understand your concept of gaining infinity, it seems to me that the more knowledge we gain, the more knowledge we're in a position to gain. So there should be like an exponential growth of knowledge. But if we look at the last 50 years, it seems that there's been a slowdown in or decrease in research productivity, economic growth, productivity growth. And this seems compatible with the story that, you know, that there's a limited amount of fruit on the tree that we picked the low hanging fruit. And now there's less and less fruit and harder and harder fruit to pick, and you know eventually well the orchard will be empty. So do you have an alternative explanation for what's going on\\n\\n1 0:53:45 in the last 50 years? Yes, I think it's very simple. There are sociological factors in academic life which have stultified the culture, and not totally and not everywhere, but that has been a tendency in what has happened, and it has resulted in a loss of productivity in many sectors, in many ways, but not in every sector, uh... the the uh... for example i i think that was a i've often said there was a stultification in uh... theoretical physics\\n\\n5\\n\\n0:54:35\\n\\nuh...\\n\\n1 0:54:36 starting in let's say the nineteen twenties and it still hasn't fully dissipated if it wasn't for that quantum computers would have been invented in the 1930s and built in the 1960s. So that is just an accidental fact, but it just goes to show that there are no guarantees. The fact that our horizons are unlimited, does not guarantee that we won't start declining tomorrow. I don't think we are currently declining. I think the these declines that we see are parochial effects caused by specific mistakes that\\n\\n2 0:55:24 have been made and which can be undone. Okay, so I want to ask you a question about Bayesianism versus Popperianism. So one reason why people prefer base is because there seems to be a way of describing changes and changes in epic static status, when their relative status of a theory hasn't changed so give you an example. Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I mean, obviously it wouldn't be enough to knock down every possible explanation because for a start we know that quantum theory is false. We don't know for sure that the next theory will have many worlds in it I mean I think it will, but, but, you know, We can't prove anything like that. But I would replace the idea of increased credence with a theory that the experiment will provide a quiver full of arrows or a repertoire of arguments that goes beyond the known arguments, the known bad arguments, and will reach into other types of arguments because the reason I would say that is that Some of the existing misconceptions about quantum theory reside in misconceptions about the methodology of science. Now I've written a paper about what I think is the right methodology of science that's more based on empiricism. Of course, I think that empiricism is a mistake and can be knocked down in its own terms, so we shouldn't, but not everybody thinks that. Now, once we have an experiment, if that was actually done, then people could not use their arguments based on a fallacious idea of empiricism, because their theory would have been refuted even by the standards of empiricism, which shouldn't have been needed in the first place.\\n\\nI think that that's the way I would express that the repertoire of arguments will become more powerful. If that experiment would own successfully.\\n\\n2 0:59:22 The next question I have is, how far do you take the principle that open ended scientific progress is the best way to deal with existential dangers to give it one example. So you have something like gain of function research, right? And it's conceivable that it could lead to more knowledge in how to stop dangerous pathogens. But I guess at least in Bayesian terms, you could say it seems even more likely that it can or has led to the spread of a man-made pathogen that would have not otherwise been naturally developed. So would your belief in open and assigned to progress allow us to say okay let's stop Dana function research.\\n\\n1 1:00:03 No, it wouldn't allow us to say let's stop it. It might make it reasonable to say, let us do research into how to make laboratories more secure before we do gain a function research. It's really part of the same thing it's it's it's like saying, let's do research into how to make the plastic hoses, through which the reagents pass more impermeable before we actually do the experiments with the reagents. So it's all part of the same experiment. I wouldn't want to stop something just because new knowledge might be discovered. the no-no in my view. But which knowledge we need to discover first, that's the problem of scheduling, which is a non-trivial part of any research and of any learning.\\n\\n2 1:00:59 But would it be considerable for you to say that until we figure out how to make sure these laboratories are safe to a certain standard, we will. Meanwhile we'll focus on doing the other kind of research so gain of function can restart, but until then it's not allowed.\\n\\n1 1:01:23 Yes, in principle that will be reasonable I don't know enough about the actual situation to have a view. You know, I don't know how these labs work. what the precautions consist of. And when I hear people talking about, for example, lab leak, I think, well, most likely lab leak is that one of the people who works there walks out of the front door. So the leak is not a leak from the lab to the outside. The leak is from the test tube to the person and then from the person walking out the door. And I don't know enough about what these proportions are or what the state of the art is to know to what extent the risk is actually minimized. It could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\", but it seems to be in an expanded universe are, there will be like a finite amount of total matter that will ever exist in our light cone right there's a limit. And that means that there's a limit on the amount of computation that this matter can, you know, execute the amount of energy can provide, perhaps even the amount of economic value we can sustain right so we, it would be weird if the GDP per atom could be arbitrarily large. So, does this impose some sort of limit on your concept of the beginning of infinity.\\n\\n1 0:50:32 So, what you've just recounted is a cosmological theory. This, this, the universe could be like that. know very little about the universes in the large, like theories of cosmology are changing on a time scale of about a decade. So it doesn't make all that much sense to speculate about what the ultimate asymptotic form of very small things like we know that our conception of physical processes must break down somehow at the level of quantum gravity, like 10 to the minus 42 seconds and that kind of thing but, but we have no idea what happens below There's no, there's no argument for that at all. It's just that we don't know what happens beyond that. Now what happens beyond that may be a finite limit. from it's being imposed by inherent hardware limitations. For example, if there's a finite amount of GNP available in the distant future, then it's still up to us whether we spend that on mathematics or music or political systems or any of the thousands of even more worthwhile things that have yet to be invented. So it's up to us which ideas we fill the 10 to the 10 to the 10 to the 10 bits with. Now, my guess is that there are no such limits, but my worldview is not affected by whether there are such limits, because as I said, it's still up to us what to fill them with. And then if we get chopped off at some point in the future, then everything will have been worthwhile up to then.\\n\\n2 0:53:07 Gotcha. Okay, so the way I understand your concept of gaining infinity, it seems to me that the more knowledge we gain, the more knowledge we're in a position to gain. So there should be like an exponential growth of knowledge. But if we look at the last 50 years, it seems that there's been a slowdown in or decrease in research productivity, economic growth, productivity growth. And this seems compatible with the story that, you know, that there's a limited amount of fruit on the tree that we picked the low hanging fruit. And now there's less and less fruit and harder and harder fruit to pick, and you know eventually well the orchard will be empty. So do you have an alternative explanation for what's going on\\n\\n1 0:53:45 in the last 50 years? Yes, I think it's very simple. There are sociological factors in academic life which have stultified the culture, and not totally and not everywhere, but that has been a tendency in what has happened, and it has resulted in a loss of productivity in many sectors, in many ways, but not in every sector, uh... the the uh... for example i i think that was a i've often said there was a stultification in uh... theoretical physics\\n\\n5\\n\\n0:54:35\\n\\nuh...\\n\\n1 0:54:36 starting in let's say the nineteen twenties and it still hasn't fully dissipated if it wasn't for that quantum computers would have been invented in the 1930s and built in the 1960s. So that is just an accidental fact, but it just goes to show that there are no guarantees. The fact that our horizons are unlimited, does not guarantee that we won't start declining tomorrow. I don't think we are currently declining. I think the these declines that we see are parochial effects caused by specific mistakes that\\n\\n2 0:55:24 have been made and which can be undone. Okay, so I want to ask you a question about Bayesianism versus Popperianism. So one reason why people prefer base is because there seems to be a way of describing changes and changes in epic static status, when their relative status of a theory hasn't changed so give you an example. Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" reality generator could also place thoughts, as well as sense data,\\n\\n1 0:37:24 into the mind? Yes, but that's only because I think that this model is wrong. It's basically the Cartesian theater, as Daniel Dennett puts it, with the stage cleared of all the characters. So, that's pure consciousness without content, as Sam Harris envisages it. But I think that all that's happening there is that you are conscious of this theater, and you're envisaging it as having certain properties, which, by the way, it doesn't have, but that doesn't matter. We can imagine lots of things that don't happen. You know, in fact, you know, that's in a way characterizes what we do all the time. So, one can interpret one's thoughts about this empty stage as being pure, contentless consciousness. But it's not. It has the content of a stage or a space or,\\n\\n2 0:38:40 you know, however you want to envisage it. Okay, and then let's talk about the Turing principle. So this is a term you coined. It's otherwise been called the church touring Deutsch principle. With this principle imply that you could. So, by the way, it states that any universal computer can simulate any physical process with this principle imply that you could simulate the whole of the universe for example in a compact efficient computer that\\n\\n1 0:39:06 was smaller than the universe itself, or is a constraint to physical processes of a task where it was computationally able to do it, but it wouldn't have enough memory or time. So the more memory and time you gave it, the more closely it could simulate the whole universe or anything near the whole universe probably because it, well, if you wanted to simulate itself as well, then there are logical reasons why there are limits to that. But even if you wanted to simulate the whole universe apart from itself, just the sheer size of the universe makes that impossible, even if we discovered ways of encoding information extremely densely like some people have said maybe quantum gravity would allow you know the rest of the universe as well so. from being limited by computational capacity, because it's only when you separate those that you realize what computational universality is. And I think that's universality, like Turing or quantum universality is the most important thing in the theory of computation, because computation doesn't even make sense, unless you have a concept of a universal computer.\\n\\n2 0:41:20 What could falsify your theory that all interesting problems are soluble so I asked this because, as I'm not convinced they're right. But do you have a strong reason for in principle believing that they're wrong.\\n\\n1 0:41:47 No. So, this, this is a bad explanation. So let's say that some people say, for example, that simulating a human brain is impossible. Now, I can't prove this possible. Nobody can prove this possible until they actually do it or unless they have a design for it which they prove will work. So, pending that there is there is no way of proving that, that it's not true that this is a fundamental limitation. is that it could be applied to anything. For example, it could be applied to the theory that you have recently, just a minute ago, been replaced by a humanoid robot, which is going to say for the next few minutes, just a prearranged set of things, and you're no longer a person. I can't believe you figured it out. Yeah, well, that's the first thing you'd say. So there is no way to refute that by experiment, short of actually doing it, short of actually talking to you and so on. So it's the same with all these other things. In order for it to make sense to have a theory that something is impossible, you have to have an explanation for why it is impossible. So we know that, for example, almost all mathematical propositions are undecidable. So that's not because somebody has said, oh maybe we can't decide everything because thinking we could decide everything is hubris. That's not an argument.\\n\\nYou need an actual functional argument to prove that that is so, and then, at being a functional argument in which the steps of the argument makes sense and relate to other things and so on. You can then say well what does this actually mean. Does this mean that maybe we can never understand the laws of physics included an undecidable function, then we would simply write, you know, f of x and f of x and undecidable function. We couldn't evaluate x it would limit our ability to make predictions, but then, lots of our ability to make predictions is totally limited anyway. to understand the properties of the function f and therefore the\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I mean, obviously it wouldn't be enough to knock down every possible explanation because for a start we know that quantum theory is false. We don't know for sure that the next theory will have many worlds in it I mean I think it will, but, but, you know, We can't prove anything like that. But I would replace the idea of increased credence with a theory that the experiment will provide a quiver full of arrows or a repertoire of arguments that goes beyond the known arguments, the known bad arguments, and will reach into other types of arguments because the reason I would say that is that Some of the existing misconceptions about quantum theory reside in misconceptions about the methodology of science. Now I've written a paper about what I think is the right methodology of science that's more based on empiricism. Of course, I think that empiricism is a mistake and can be knocked down in its own terms, so we shouldn't, but not everybody thinks that. Now, once we have an experiment, if that was actually done, then people could not use their arguments based on a fallacious idea of empiricism, because their theory would have been refuted even by the standards of empiricism, which shouldn't have been needed in the first place.\\n\\nI think that that's the way I would express that the repertoire of arguments will become more powerful. If that experiment would own successfully.\\n\\n2 0:59:22 The next question I have is, how far do you take the principle that open ended scientific progress is the best way to deal with existential dangers to give it one example. So you have something like gain of function research, right? And it's conceivable that it could lead to more knowledge in how to stop dangerous pathogens. But I guess at least in Bayesian terms, you could say it seems even more likely that it can or has led to the spread of a man-made pathogen that would have not otherwise been naturally developed. So would your belief in open and assigned to progress allow us to say okay let's stop Dana function research.\\n\\n1 1:00:03 No, it wouldn't allow us to say let's stop it. It might make it reasonable to say, let us do research into how to make laboratories more secure before we do gain a function research. It's really part of the same thing it's it's it's like saying, let's do research into how to make the plastic hoses, through which the reagents pass more impermeable before we actually do the experiments with the reagents. So it's all part of the same experiment. I wouldn't want to stop something just because new knowledge might be discovered. the no-no in my view. But which knowledge we need to discover first, that's the problem of scheduling, which is a non-trivial part of any research and of any learning.\\n\\n2 1:00:59 But would it be considerable for you to say that until we figure out how to make sure these laboratories are safe to a certain standard, we will. Meanwhile we'll focus on doing the other kind of research so gain of function can restart, but until then it's not allowed.\\n\\n1 1:01:23 Yes, in principle that will be reasonable I don't know enough about the actual situation to have a view. You know, I don't know how these labs work. what the precautions consist of. And when I hear people talking about, for example, lab leak, I think, well, most likely lab leak is that one of the people who works there walks out of the front door. So the leak is not a leak from the lab to the outside. The leak is from the test tube to the person and then from the person walking out the door. And I don't know enough about what these proportions are or what the state of the art is to know to what extent the risk is actually minimized. It could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I had Tyler Cowen on my podcast, I asked him why he thinks, so he thinks that human civilization is only going to be around for 700 more years, and then so I asked him, I gave him, you know, your rebuttal, or what I understand of your rebuttal, that, you know, creative, optimistic societies will innovate ways of, you know, safety technology is faster than totalitarian static societies can innovate destructive technologies. And he responded, you know, maybe, but the cost of destruction is just so much lower than the cost of building. And, you know, that trend has been going on for a while now. What happens when a new cost, $60,000? Or what happens if there's a mistake like the kinds that we saw many times over in the Cold War? How would you respond to that?\\n\\n1 1:03:38 First of all, I think we've been getting safer and safer throughout the entire history of civilization. The, you know, there were these plagues that wiped out a third of the population of the world or half. And it could have been 99% or 100%. went through some kind of bottleneck 70,000 years ago, I understand, which they can tell from genetics, all our cousin species have been wiped out. So we were much less safe then than now. Also, if a asteroid, 10-kilometer asteroid, had been on target with the earth at any time in the in the past two million year or whatever it is history of the genus homo that would have been the end of it whereas now it'll just mean higher taxation for a while you know that that's the that's how much amazingly safer we are now uh i i would never say that it's impossible that we'll destroy ourselves. That would be the contrary to the universality of the human mind. We can make wrong choices. We can make so many wrong choices that we'll destroy ourselves. And on the other hand, the atomic bomb accident sort of thing would have had no zero chance of destroying civilization. All they would have done is cause a vast amount of suffering. And... but I don't think we have the technology to end civilization even if we wanted to. I think all we would do if we just deliberately unleashed hell all over the world is we would cause a vast amount of suffering. But there would be survivors and they would resolve never to do that again.\\n\\nSo I don't think we're even able to, let alone that we would do it accidentally. But as for the bad guys, well, I think we are doing the wrong thing largely in regard to both external and internal threats. But I don't think we're doing the wrong thing to an existential risk level. And over the next 700 years or whatever it is, well, I don't want to prophesy because I don't know most of the advances that are going to be made in that time. I see no reason why if we are solving problems, we won't solve problems. I don't think this, this, to take another metaphor, Nick Bostrom's jar with white balls and there's one black ball, and you take out a white ball and white ball and white ball, and then you hit the black ball and that's the end of you. I don't think it's like that because every white ball you take out and have reduces the number of black balls in the jar. So again I'm not saying that's the law of nature. It could be that the very next ball we take out will be the black one that'll be the end of us. It could be but I think all arguments that it will be are fallacious.\\n\\n2 1:07:17 I do want to talk about the fun criterion. Is your definition of fun different from how other people define other positive emotions like eudaemonia or well being or satisfaction? Is it fun a different emotion?\\n\\n1 1:07:29 I don't think it's an emotion. And and I all these things are not very well defined. They can't possibly be very well defined until we have a satisfactory theory of qualia, at least, and probably more satisfactory theory of creativity, how creativity works and so on. I think that the choice of the word fun for the thing that I explain more precisely, but still not very precisely, as a creation of knowledge without,\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I had Tyler Cowen on my podcast, I asked him why he thinks, so he thinks that human civilization is only going to be around for 700 more years, and then so I asked him, I gave him, you know, your rebuttal, or what I understand of your rebuttal, that, you know, creative, optimistic societies will innovate ways of, you know, safety technology is faster than totalitarian static societies can innovate destructive technologies. And he responded, you know, maybe, but the cost of destruction is just so much lower than the cost of building. And, you know, that trend has been going on for a while now. What happens when a new cost, $60,000? Or what happens if there's a mistake like the kinds that we saw many times over in the Cold War? How would you respond to that?\\n\\n1 1:03:38 First of all, I think we've been getting safer and safer throughout the entire history of civilization. The, you know, there were these plagues that wiped out a third of the population of the world or half. And it could have been 99% or 100%. went through some kind of bottleneck 70,000 years ago, I understand, which they can tell from genetics, all our cousin species have been wiped out. So we were much less safe then than now. Also, if a asteroid, 10-kilometer asteroid, had been on target with the earth at any time in the in the past two million year or whatever it is history of the genus homo that would have been the end of it whereas now it'll just mean higher taxation for a while you know that that's the that's how much amazingly safer we are now uh i i would never say that it's impossible that we'll destroy ourselves. That would be the contrary to the universality of the human mind. We can make wrong choices. We can make so many wrong choices that we'll destroy ourselves. And on the other hand, the atomic bomb accident sort of thing would have had no zero chance of destroying civilization. All they would have done is cause a vast amount of suffering. And... but I don't think we have the technology to end civilization even if we wanted to. I think all we would do if we just deliberately unleashed hell all over the world is we would cause a vast amount of suffering. But there would be survivors and they would resolve never to do that again.\\n\\nSo I don't think we're even able to, let alone that we would do it accidentally. But as for the bad guys, well, I think we are doing the wrong thing largely in regard to both external and internal threats. But I don't think we're doing the wrong thing to an existential risk level. And over the next 700 years or whatever it is, well, I don't want to prophesy because I don't know most of the advances that are going to be made in that time. I see no reason why if we are solving problems, we won't solve problems. I don't think this, this, to take another metaphor, Nick Bostrom's jar with white balls and there's one black ball, and you take out a white ball and white ball and white ball, and then you hit the black ball and that's the end of you. I don't think it's like that because every white ball you take out and have reduces the number of black balls in the jar. So again I'm not saying that's the law of nature. It could be that the very next ball we take out will be the black one that'll be the end of us. It could be but I think all arguments that it will be are fallacious.\\n\\n2 1:07:17 I do want to talk about the fun criterion. Is your definition of fun different from how other people define other positive emotions like eudaemonia or well being or satisfaction? Is it fun a different emotion?\\n\\n1 1:07:29 I don't think it's an emotion. And and I all these things are not very well defined. They can't possibly be very well defined until we have a satisfactory theory of qualia, at least, and probably more satisfactory theory of creativity, how creativity works and so on. I think that the choice of the word fun for the thing that I explain more precisely, but still not very precisely, as a creation of knowledge without,\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\", but it seems to be in an expanded universe are, there will be like a finite amount of total matter that will ever exist in our light cone right there's a limit. And that means that there's a limit on the amount of computation that this matter can, you know, execute the amount of energy can provide, perhaps even the amount of economic value we can sustain right so we, it would be weird if the GDP per atom could be arbitrarily large. So, does this impose some sort of limit on your concept of the beginning of infinity.\\n\\n1 0:50:32 So, what you've just recounted is a cosmological theory. This, this, the universe could be like that. know very little about the universes in the large, like theories of cosmology are changing on a time scale of about a decade. So it doesn't make all that much sense to speculate about what the ultimate asymptotic form of very small things like we know that our conception of physical processes must break down somehow at the level of quantum gravity, like 10 to the minus 42 seconds and that kind of thing but, but we have no idea what happens below There's no, there's no argument for that at all. It's just that we don't know what happens beyond that. Now what happens beyond that may be a finite limit. from it's being imposed by inherent hardware limitations. For example, if there's a finite amount of GNP available in the distant future, then it's still up to us whether we spend that on mathematics or music or political systems or any of the thousands of even more worthwhile things that have yet to be invented. So it's up to us which ideas we fill the 10 to the 10 to the 10 to the 10 bits with. Now, my guess is that there are no such limits, but my worldview is not affected by whether there are such limits, because as I said, it's still up to us what to fill them with. And then if we get chopped off at some point in the future, then everything will have been worthwhile up to then.\\n\\n2 0:53:07 Gotcha. Okay, so the way I understand your concept of gaining infinity, it seems to me that the more knowledge we gain, the more knowledge we're in a position to gain. So there should be like an exponential growth of knowledge. But if we look at the last 50 years, it seems that there's been a slowdown in or decrease in research productivity, economic growth, productivity growth. And this seems compatible with the story that, you know, that there's a limited amount of fruit on the tree that we picked the low hanging fruit. And now there's less and less fruit and harder and harder fruit to pick, and you know eventually well the orchard will be empty. So do you have an alternative explanation for what's going on\\n\\n1 0:53:45 in the last 50 years? Yes, I think it's very simple. There are sociological factors in academic life which have stultified the culture, and not totally and not everywhere, but that has been a tendency in what has happened, and it has resulted in a loss of productivity in many sectors, in many ways, but not in every sector, uh... the the uh... for example i i think that was a i've often said there was a stultification in uh... theoretical physics\\n\\n5\\n\\n0:54:35\\n\\nuh...\\n\\n1 0:54:36 starting in let's say the nineteen twenties and it still hasn't fully dissipated if it wasn't for that quantum computers would have been invented in the 1930s and built in the 1960s. So that is just an accidental fact, but it just goes to show that there are no guarantees. The fact that our horizons are unlimited, does not guarantee that we won't start declining tomorrow. I don't think we are currently declining. I think the these declines that we see are parochial effects caused by specific mistakes that\\n\\n2 0:55:24 have been made and which can be undone. Okay, so I want to ask you a question about Bayesianism versus Popperianism. So one reason why people prefer base is because there seems to be a way of describing changes and changes in epic static status, when their relative status of a theory hasn't changed so give you an example. Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I had Tyler Cowen on my podcast, I asked him why he thinks, so he thinks that human civilization is only going to be around for 700 more years, and then so I asked him, I gave him, you know, your rebuttal, or what I understand of your rebuttal, that, you know, creative, optimistic societies will innovate ways of, you know, safety technology is faster than totalitarian static societies can innovate destructive technologies. And he responded, you know, maybe, but the cost of destruction is just so much lower than the cost of building. And, you know, that trend has been going on for a while now. What happens when a new cost, $60,000? Or what happens if there's a mistake like the kinds that we saw many times over in the Cold War? How would you respond to that?\\n\\n1 1:03:38 First of all, I think we've been getting safer and safer throughout the entire history of civilization. The, you know, there were these plagues that wiped out a third of the population of the world or half. And it could have been 99% or 100%. went through some kind of bottleneck 70,000 years ago, I understand, which they can tell from genetics, all our cousin species have been wiped out. So we were much less safe then than now. Also, if a asteroid, 10-kilometer asteroid, had been on target with the earth at any time in the in the past two million year or whatever it is history of the genus homo that would have been the end of it whereas now it'll just mean higher taxation for a while you know that that's the that's how much amazingly safer we are now uh i i would never say that it's impossible that we'll destroy ourselves. That would be the contrary to the universality of the human mind. We can make wrong choices. We can make so many wrong choices that we'll destroy ourselves. And on the other hand, the atomic bomb accident sort of thing would have had no zero chance of destroying civilization. All they would have done is cause a vast amount of suffering. And... but I don't think we have the technology to end civilization even if we wanted to. I think all we would do if we just deliberately unleashed hell all over the world is we would cause a vast amount of suffering. But there would be survivors and they would resolve never to do that again.\\n\\nSo I don't think we're even able to, let alone that we would do it accidentally. But as for the bad guys, well, I think we are doing the wrong thing largely in regard to both external and internal threats. But I don't think we're doing the wrong thing to an existential risk level. And over the next 700 years or whatever it is, well, I don't want to prophesy because I don't know most of the advances that are going to be made in that time. I see no reason why if we are solving problems, we won't solve problems. I don't think this, this, to take another metaphor, Nick Bostrom's jar with white balls and there's one black ball, and you take out a white ball and white ball and white ball, and then you hit the black ball and that's the end of you. I don't think it's like that because every white ball you take out and have reduces the number of black balls in the jar. So again I'm not saying that's the law of nature. It could be that the very next ball we take out will be the black one that'll be the end of us. It could be but I think all arguments that it will be are fallacious.\\n\\n2 1:07:17 I do want to talk about the fun criterion. Is your definition of fun different from how other people define other positive emotions like eudaemonia or well being or satisfaction? Is it fun a different emotion?\\n\\n1 1:07:29 I don't think it's an emotion. And and I all these things are not very well defined. They can't possibly be very well defined until we have a satisfactory theory of qualia, at least, and probably more satisfactory theory of creativity, how creativity works and so on. I think that the choice of the word fun for the thing that I explain more precisely, but still not very precisely, as a creation of knowledge without,\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\\nDavid Deutsch Absolutely. I mean, I think probably more than now, it was a complete consensus that everybody was afraid. Let me think. I think everybody was appropriately afraid. Was it like the pandemic? It's unlike today.\\n\\nLulie Was it like the pandemic freak-out? Because you also got people actually freaking out around the pandemic and doing all sorts of things like covering their door handles in copper and not knowing whether masks work or not and so on and these battles with their friends about whether\\n\\nDavid Deutsch they leave their house and whether that is akin to killing people. The interesting thing, I think there's an interesting difference between being afraid of something like nuclear war or a pandemic and being afraid of something that one imagines, like AI risk or climate risk. So those are both things that that might happen in the future and different people might imagine different things about them.\\n\\nLulie You mean AGI risk in the future?\\n\\nDavid Deutsch Well there's AGI risk and nowadays there seems to be AI risk as well. I mean there is AI risk. People are using it to scam people and to fake voices of Barack Obama and ring people's grandmothers and so what about those? Yes, well those are real risks and you know there's the electric car risk and the self-driving car risk and so on and that's that's not in the same league as as having a greatly increased ability to scam and get like dodgy information it's interesting is it greatly increased i mean i i wonder whether anyone has statistics about how many scams are currently advanced ai enabled and how many are simply the same old scams of saying hello we're the police we want you to transfer all your money into this account.\\n\\nLulie I imagine the good scams, as in the effective scams, would be AI enabled, like you want to be\\n\\nDavid Deutsch on the leading edge of making scams. Well, I don't know, I'm not an expert on scams, but the thing is dangers, including scams and everything, will always be caused by new technology. I mean, sorry, new technology will always cause dangers, including scams. And to try to mitigate that by preventing new technology, in case it produces new dangers, is much more dangerous than any of the new technologies themselves.\\n\\nLulie What about just slowing it down such that people can adapt? Because like right now we've got something that is going so quickly that people are getting confused like old people if they see an image then they will assume that it's real and whereas if you have time that people are kind of adapt to, ah yes there are these deep fakes and so on.\\n\\nDavid Deutsch Well I'm not sure that time causes better adaptation because if things are happening fast, then also, news stories about how people have been scammed will be seen by your old people, and whereas if we slowed it down so that only one scam occurs every few months, then it might not be news.\\n\\nLulie issue. People are worried that AGI is, you know, maybe next week or just around the corner or in, like they used to say, in a few years, and now that we have these very good language models they say, maybe, like, small number of years, months, like, possibly weeks, and hence the proposed moratorium. So, what is the thing that makes you so chill? Why couldn't it lead to AGI? What's the problem with the idea of emergence? Because, you know, intelligence emerged once. Yes. So emergence isn't magic.\\n\\nDavid Deutsch It is, of course, possible that in the deep ocean a new form of life is emerging at this very moment and it will break the surface weeks from now.\\n\\nLulie Or on Pluto. Did you hear about Pluto? They discovered that there's ice or something and that they've sent another probe but it's going to take eight years for it to come back and find out whether there's life on Pluto.\\n\\nDavid Deutsch I hadn't heard that but there certainly is a possibility of life in various places in the solar system but not, I think, not intelligent life. But, if you're going to say emergence can do unexpected things, then you might as well say it about the deep ocean, because\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I mean, obviously it wouldn't be enough to knock down every possible explanation because for a start we know that quantum theory is false. We don't know for sure that the next theory will have many worlds in it I mean I think it will, but, but, you know, We can't prove anything like that. But I would replace the idea of increased credence with a theory that the experiment will provide a quiver full of arrows or a repertoire of arguments that goes beyond the known arguments, the known bad arguments, and will reach into other types of arguments because the reason I would say that is that Some of the existing misconceptions about quantum theory reside in misconceptions about the methodology of science. Now I've written a paper about what I think is the right methodology of science that's more based on empiricism. Of course, I think that empiricism is a mistake and can be knocked down in its own terms, so we shouldn't, but not everybody thinks that. Now, once we have an experiment, if that was actually done, then people could not use their arguments based on a fallacious idea of empiricism, because their theory would have been refuted even by the standards of empiricism, which shouldn't have been needed in the first place.\\n\\nI think that that's the way I would express that the repertoire of arguments will become more powerful. If that experiment would own successfully.\\n\\n2 0:59:22 The next question I have is, how far do you take the principle that open ended scientific progress is the best way to deal with existential dangers to give it one example. So you have something like gain of function research, right? And it's conceivable that it could lead to more knowledge in how to stop dangerous pathogens. But I guess at least in Bayesian terms, you could say it seems even more likely that it can or has led to the spread of a man-made pathogen that would have not otherwise been naturally developed. So would your belief in open and assigned to progress allow us to say okay let's stop Dana function research.\\n\\n1 1:00:03 No, it wouldn't allow us to say let's stop it. It might make it reasonable to say, let us do research into how to make laboratories more secure before we do gain a function research. It's really part of the same thing it's it's it's like saying, let's do research into how to make the plastic hoses, through which the reagents pass more impermeable before we actually do the experiments with the reagents. So it's all part of the same experiment. I wouldn't want to stop something just because new knowledge might be discovered. the no-no in my view. But which knowledge we need to discover first, that's the problem of scheduling, which is a non-trivial part of any research and of any learning.\\n\\n2 1:00:59 But would it be considerable for you to say that until we figure out how to make sure these laboratories are safe to a certain standard, we will. Meanwhile we'll focus on doing the other kind of research so gain of function can restart, but until then it's not allowed.\\n\\n1 1:01:23 Yes, in principle that will be reasonable I don't know enough about the actual situation to have a view. You know, I don't know how these labs work. what the precautions consist of. And when I hear people talking about, for example, lab leak, I think, well, most likely lab leak is that one of the people who works there walks out of the front door. So the leak is not a leak from the lab to the outside. The leak is from the test tube to the person and then from the person walking out the door. And I don't know enough about what these proportions are or what the state of the art is to know to what extent the risk is actually minimized. It could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\\nDavid Deutsch Absolutely. I mean, I think probably more than now, it was a complete consensus that everybody was afraid. Let me think. I think everybody was appropriately afraid. Was it like the pandemic? It's unlike today.\\n\\nLulie Was it like the pandemic freak-out? Because you also got people actually freaking out around the pandemic and doing all sorts of things like covering their door handles in copper and not knowing whether masks work or not and so on and these battles with their friends about whether\\n\\nDavid Deutsch they leave their house and whether that is akin to killing people. The interesting thing, I think there's an interesting difference between being afraid of something like nuclear war or a pandemic and being afraid of something that one imagines, like AI risk or climate risk. So those are both things that that might happen in the future and different people might imagine different things about them.\\n\\nLulie You mean AGI risk in the future?\\n\\nDavid Deutsch Well there's AGI risk and nowadays there seems to be AI risk as well. I mean there is AI risk. People are using it to scam people and to fake voices of Barack Obama and ring people's grandmothers and so what about those? Yes, well those are real risks and you know there's the electric car risk and the self-driving car risk and so on and that's that's not in the same league as as having a greatly increased ability to scam and get like dodgy information it's interesting is it greatly increased i mean i i wonder whether anyone has statistics about how many scams are currently advanced ai enabled and how many are simply the same old scams of saying hello we're the police we want you to transfer all your money into this account.\\n\\nLulie I imagine the good scams, as in the effective scams, would be AI enabled, like you want to be\\n\\nDavid Deutsch on the leading edge of making scams. Well, I don't know, I'm not an expert on scams, but the thing is dangers, including scams and everything, will always be caused by new technology. I mean, sorry, new technology will always cause dangers, including scams. And to try to mitigate that by preventing new technology, in case it produces new dangers, is much more dangerous than any of the new technologies themselves.\\n\\nLulie What about just slowing it down such that people can adapt? Because like right now we've got something that is going so quickly that people are getting confused like old people if they see an image then they will assume that it's real and whereas if you have time that people are kind of adapt to, ah yes there are these deep fakes and so on.\\n\\nDavid Deutsch Well I'm not sure that time causes better adaptation because if things are happening fast, then also, news stories about how people have been scammed will be seen by your old people, and whereas if we slowed it down so that only one scam occurs every few months, then it might not be news.\\n\\nLulie issue. People are worried that AGI is, you know, maybe next week or just around the corner or in, like they used to say, in a few years, and now that we have these very good language models they say, maybe, like, small number of years, months, like, possibly weeks, and hence the proposed moratorium. So, what is the thing that makes you so chill? Why couldn't it lead to AGI? What's the problem with the idea of emergence? Because, you know, intelligence emerged once. Yes. So emergence isn't magic.\\n\\nDavid Deutsch It is, of course, possible that in the deep ocean a new form of life is emerging at this very moment and it will break the surface weeks from now.\\n\\nLulie Or on Pluto. Did you hear about Pluto? They discovered that there's ice or something and that they've sent another probe but it's going to take eight years for it to come back and find out whether there's life on Pluto.\\n\\nDavid Deutsch I hadn't heard that but there certainly is a possibility of life in various places in the solar system but not, I think, not intelligent life. But, if you're going to say emergence can do unexpected things, then you might as well say it about the deep ocean, because\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I had Tyler Cowen on my podcast, I asked him why he thinks, so he thinks that human civilization is only going to be around for 700 more years, and then so I asked him, I gave him, you know, your rebuttal, or what I understand of your rebuttal, that, you know, creative, optimistic societies will innovate ways of, you know, safety technology is faster than totalitarian static societies can innovate destructive technologies. And he responded, you know, maybe, but the cost of destruction is just so much lower than the cost of building. And, you know, that trend has been going on for a while now. What happens when a new cost, $60,000? Or what happens if there's a mistake like the kinds that we saw many times over in the Cold War? How would you respond to that?\\n\\n1 1:03:38 First of all, I think we've been getting safer and safer throughout the entire history of civilization. The, you know, there were these plagues that wiped out a third of the population of the world or half. And it could have been 99% or 100%. went through some kind of bottleneck 70,000 years ago, I understand, which they can tell from genetics, all our cousin species have been wiped out. So we were much less safe then than now. Also, if a asteroid, 10-kilometer asteroid, had been on target with the earth at any time in the in the past two million year or whatever it is history of the genus homo that would have been the end of it whereas now it'll just mean higher taxation for a while you know that that's the that's how much amazingly safer we are now uh i i would never say that it's impossible that we'll destroy ourselves. That would be the contrary to the universality of the human mind. We can make wrong choices. We can make so many wrong choices that we'll destroy ourselves. And on the other hand, the atomic bomb accident sort of thing would have had no zero chance of destroying civilization. All they would have done is cause a vast amount of suffering. And... but I don't think we have the technology to end civilization even if we wanted to. I think all we would do if we just deliberately unleashed hell all over the world is we would cause a vast amount of suffering. But there would be survivors and they would resolve never to do that again.\\n\\nSo I don't think we're even able to, let alone that we would do it accidentally. But as for the bad guys, well, I think we are doing the wrong thing largely in regard to both external and internal threats. But I don't think we're doing the wrong thing to an existential risk level. And over the next 700 years or whatever it is, well, I don't want to prophesy because I don't know most of the advances that are going to be made in that time. I see no reason why if we are solving problems, we won't solve problems. I don't think this, this, to take another metaphor, Nick Bostrom's jar with white balls and there's one black ball, and you take out a white ball and white ball and white ball, and then you hit the black ball and that's the end of you. I don't think it's like that because every white ball you take out and have reduces the number of black balls in the jar. So again I'm not saying that's the law of nature. It could be that the very next ball we take out will be the black one that'll be the end of us. It could be but I think all arguments that it will be are fallacious.\\n\\n2 1:07:17 I do want to talk about the fun criterion. Is your definition of fun different from how other people define other positive emotions like eudaemonia or well being or satisfaction? Is it fun a different emotion?\\n\\n1 1:07:29 I don't think it's an emotion. And and I all these things are not very well defined. They can't possibly be very well defined until we have a satisfactory theory of qualia, at least, and probably more satisfactory theory of creativity, how creativity works and so on. I think that the choice of the word fun for the thing that I explain more precisely, but still not very precisely, as a creation of knowledge without,\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I had Tyler Cowen on my podcast, I asked him why he thinks, so he thinks that human civilization is only going to be around for 700 more years, and then so I asked him, I gave him, you know, your rebuttal, or what I understand of your rebuttal, that, you know, creative, optimistic societies will innovate ways of, you know, safety technology is faster than totalitarian static societies can innovate destructive technologies. And he responded, you know, maybe, but the cost of destruction is just so much lower than the cost of building. And, you know, that trend has been going on for a while now. What happens when a new cost, $60,000? Or what happens if there's a mistake like the kinds that we saw many times over in the Cold War? How would you respond to that?\\n\\n1 1:03:38 First of all, I think we've been getting safer and safer throughout the entire history of civilization. The, you know, there were these plagues that wiped out a third of the population of the world or half. And it could have been 99% or 100%. went through some kind of bottleneck 70,000 years ago, I understand, which they can tell from genetics, all our cousin species have been wiped out. So we were much less safe then than now. Also, if a asteroid, 10-kilometer asteroid, had been on target with the earth at any time in the in the past two million year or whatever it is history of the genus homo that would have been the end of it whereas now it'll just mean higher taxation for a while you know that that's the that's how much amazingly safer we are now uh i i would never say that it's impossible that we'll destroy ourselves. That would be the contrary to the universality of the human mind. We can make wrong choices. We can make so many wrong choices that we'll destroy ourselves. And on the other hand, the atomic bomb accident sort of thing would have had no zero chance of destroying civilization. All they would have done is cause a vast amount of suffering. And... but I don't think we have the technology to end civilization even if we wanted to. I think all we would do if we just deliberately unleashed hell all over the world is we would cause a vast amount of suffering. But there would be survivors and they would resolve never to do that again.\\n\\nSo I don't think we're even able to, let alone that we would do it accidentally. But as for the bad guys, well, I think we are doing the wrong thing largely in regard to both external and internal threats. But I don't think we're doing the wrong thing to an existential risk level. And over the next 700 years or whatever it is, well, I don't want to prophesy because I don't know most of the advances that are going to be made in that time. I see no reason why if we are solving problems, we won't solve problems. I don't think this, this, to take another metaphor, Nick Bostrom's jar with white balls and there's one black ball, and you take out a white ball and white ball and white ball, and then you hit the black ball and that's the end of you. I don't think it's like that because every white ball you take out and have reduces the number of black balls in the jar. So again I'm not saying that's the law of nature. It could be that the very next ball we take out will be the black one that'll be the end of us. It could be but I think all arguments that it will be are fallacious.\\n\\n2 1:07:17 I do want to talk about the fun criterion. Is your definition of fun different from how other people define other positive emotions like eudaemonia or well being or satisfaction? Is it fun a different emotion?\\n\\n1 1:07:29 I don't think it's an emotion. And and I all these things are not very well defined. They can't possibly be very well defined until we have a satisfactory theory of qualia, at least, and probably more satisfactory theory of creativity, how creativity works and so on. I think that the choice of the word fun for the thing that I explain more precisely, but still not very precisely, as a creation of knowledge without,\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" Currently the many worlds. Now it seems that even though many worlds remains the best or the only explanation, somehow, it's epistemic status has changed. increased. How would you describe these sorts of changes in a popular interview.\\n\\n1 0:56:25 So what what has happened there is that at the moment we have only one explanation that can't be immediately knocked down. will provide the ammunition to knock down, even ideas for alternative explanations that have not been thought of yet. I mean, obviously it wouldn't be enough to knock down every possible explanation because for a start we know that quantum theory is false. We don't know for sure that the next theory will have many worlds in it I mean I think it will, but, but, you know, We can't prove anything like that. But I would replace the idea of increased credence with a theory that the experiment will provide a quiver full of arrows or a repertoire of arguments that goes beyond the known arguments, the known bad arguments, and will reach into other types of arguments because the reason I would say that is that Some of the existing misconceptions about quantum theory reside in misconceptions about the methodology of science. Now I've written a paper about what I think is the right methodology of science that's more based on empiricism. Of course, I think that empiricism is a mistake and can be knocked down in its own terms, so we shouldn't, but not everybody thinks that. Now, once we have an experiment, if that was actually done, then people could not use their arguments based on a fallacious idea of empiricism, because their theory would have been refuted even by the standards of empiricism, which shouldn't have been needed in the first place.\\n\\nI think that that's the way I would express that the repertoire of arguments will become more powerful. If that experiment would own successfully.\\n\\n2 0:59:22 The next question I have is, how far do you take the principle that open ended scientific progress is the best way to deal with existential dangers to give it one example. So you have something like gain of function research, right? And it's conceivable that it could lead to more knowledge in how to stop dangerous pathogens. But I guess at least in Bayesian terms, you could say it seems even more likely that it can or has led to the spread of a man-made pathogen that would have not otherwise been naturally developed. So would your belief in open and assigned to progress allow us to say okay let's stop Dana function research.\\n\\n1 1:00:03 No, it wouldn't allow us to say let's stop it. It might make it reasonable to say, let us do research into how to make laboratories more secure before we do gain a function research. It's really part of the same thing it's it's it's like saying, let's do research into how to make the plastic hoses, through which the reagents pass more impermeable before we actually do the experiments with the reagents. So it's all part of the same experiment. I wouldn't want to stop something just because new knowledge might be discovered. the no-no in my view. But which knowledge we need to discover first, that's the problem of scheduling, which is a non-trivial part of any research and of any learning.\\n\\n2 1:00:59 But would it be considerable for you to say that until we figure out how to make sure these laboratories are safe to a certain standard, we will. Meanwhile we'll focus on doing the other kind of research so gain of function can restart, but until then it's not allowed.\\n\\n1 1:01:23 Yes, in principle that will be reasonable I don't know enough about the actual situation to have a view. You know, I don't know how these labs work. what the precautions consist of. And when I hear people talking about, for example, lab leak, I think, well, most likely lab leak is that one of the people who works there walks out of the front door. So the leak is not a leak from the lab to the outside. The leak is from the test tube to the person and then from the person walking out the door. And I don't know enough about what these proportions are or what the state of the art is to know to what extent the risk is actually minimized. It could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" a different emotion?\\n\\n1 1:07:29 I don't think it's an emotion. And and I all these things are not very well defined. They can't possibly be very well defined until we have a satisfactory theory of qualia, at least, and probably more satisfactory theory of creativity, how creativity works and so on. I think that the choice of the word fun for the thing that I explain more precisely, but still not very precisely, as a creation of knowledge without, where the different kinds of knowledge, inexplicit, unconscious, conscious, explicit, I think that is actually the only way in which the everyday usage of the word fun differs from that is that fun is considered frivolous or seeking fun is considered as seeking frivolity. But I think that isn't so much a different use of the word, it's just a different pejorative theory about whether this is a good or a bad thing. But nevertheless, I can't define it precisely. The important thing is that there is a thing which has this property of fun that you can't compulsorily enact it. So in some views, you know, no pain, no gain. Well, then you can find out mechanically whether the thing is causing pain and whether it's doing it according to the theory that says that you will have gain if you have that pain. So that can all be done mechanically. And therefore it is subject to the criticism.\\n\\ncriticism and another way of looking at the fun theory is that it's a mode of criticism um is subject to the criticism that this isn't fun i.e this is making an uh privileging one kind of knowledge arbitrarily over another rather than being rational and letting content\\n\\n2 1:09:58 decide is this placing a limitation on universal explainers then if they can create some sort of theory about why I think could or should be fun, why anything could be fun. And it seems to me that sometimes we actually can make things fun that aren't like, for example, take exercise no pain no gain, it's like when you first go it's not fun but you know once you start going you understand the mechanics, you develop a theory for why\\n\\n1 1:10:21 can and should be fun. Yes, yes. Well, that's quite a good example because there you see that fun cannot be defined as the absence of pain. So you can be having fun while experiencing physical pain and that physical pain is not sparking joy, as Marie Kondo would say. And that's important because if you are dogmatically or uncritically in your life, a theory of the good that involves pain and which excludes the criticism that maybe this can't be fun or maybe this isn't yet fun or maybe I should make it fun and if I can't that's a reason to stop, you know, all those things. If all those things are excluded because by definition the thing is good and your pain, your suffering doesn't matter, then that opens the door to not only to suffering, but to stasis. You won't be able to get to a better theory.\\n\\n4 1:11:43 And then why is fun central to this\\n\\n1 1:11:46 instead of another emotion? So, like for example, Aristotle thought that,\\n\\n2 1:11:50 like I guess, a sort of widely defined sense of happiness is what should be the goal of our endeavors. Why fun instead of something like that?\\n\\n1 1:12:10 Well, that's defining it vaguely enough so that what you said might very well be fun. The point is the underlying thing is as far as, you know, going one level below, we're really to understand that we'd need to go about seven levels below that, which we can't do yet. But the important thing is that there are several kinds of knowledge in our brains. And the one that is written down in the exercise book that says, you should do this number of reps, and you should power through this, and it doesn't matter if you feel that, and so on. That's an explicit theory, and it contains some knowledge, but it also contains error. That's like all our knowledge is like that. We also have other knowledge, which is which is contained in our biology, it's contained in our genes. We have knowledge that is inexplicit, like our knowledge of, of grammar is my was my favorite example, as we know why certain sentences are acceptable and why they're unacceptable, but we can't state explicitly, or in every case, why it isn't or why it is. And then there's explicit and inexplicit knowledge, there's conscious and unconscious knowledge. All those are bits of program in the brain, they're ideas, they are bits of knowledge in this, if you define knowledge as information with causal power, they are all\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" our knowledge of, of grammar is my was my favorite example, as we know why certain sentences are acceptable and why they're unacceptable, but we can't state explicitly, or in every case, why it isn't or why it is. And then there's explicit and inexplicit knowledge, there's conscious and unconscious knowledge. All those are bits of program in the brain, they're ideas, they are bits of knowledge in this, if you define knowledge as information with causal power, they are all information with causal power. They all contain truth and they all contain error and it's always a mistake to shield something, to shield one of them from criticism or replacement.\\n\\nNot doing that is what I call the fun criterion. Now you might say that's a bad name, but you know, it's the best I can find.\\n\\n2 1:14:11 So why would creating an AGI through evolution necessarily entail suffering? Because the way I see it, or it seems to me your theory is that you need to be a general intelligence in order to feel suffering, but by the point an evolved, simulated being is a general intelligence, we can just stop the simulation.\\n\\n1 1:14:31 And so where is the suffering coming from? Okay, so the kind of simulation by evolution that I'm thinking of, I mean, there may be several kinds, but the kind that I'm thinking of, and which I said would be the greatest crime in history, is the kind that just simulates the actual evolution of humans from pre humans that weren't people. So you have you have a population of non people which in this simulation would be some kind of NPCs. And then they would they would just evolve we don't know what the criterion would be we just have an artificial universe which simulated the surface of the earth and they'd be walking around and some of them might or might not become people. And now the thing is when you're part of the way there, what is happening is that you have the way that I, the only way that I can imagine the evolution of personhood or create the explanatory creativity happened was that the hardware needed for it was first needed for something else. I have proposed that it was needed to transmit memes. So there'd be people who were transmitting memes creatively but they were running out of resources before it managed to increase their stock of memes. So in every generation, there was a stock of memes that was being passed down to the next generation. And once they got beyond a certain complexity, they had to be passed down by the use of creativity by the recipient.\\n\\nSo, there may well have been a time and as I say I can't think of any other way it could have been, where there was genuine creativity being used, but it ran out of resources, very quickly, but not so quickly that it didn't increase the mean bandwidth. Then in the next generation, there was more mean bandwidth, and then after, you know, certain number of generations, there would have been some opportunity to use this hardware or whatever it is, you know, firmware, I expect, to use this firmware for something other than just trying blindly transmitting memes or rather creatively transmitting memes, but they were blind memes. So in that time, it would have been very unpleasant to be alive. It was already very unpleasant to, sorry, it was very unpleasant to be alive when we did have enough resources to think as well as do the memes. But I don't think there would have been a moment at which you would say, yes, now the suffering begins to matter because it's not just blind memes. I think the people were already suffering at the time when they were blindly transmitting memes because they were using genuine creativity. They were just not using it to any good effect.\\n\\n2 1:17:56 Gotcha. Would being in the experience machine be compatible with the fun criterion so you're not aware that you're in, you know, the experience machine it's all virtual reality, but you're still doing the things that would make you have fun in fact more so than in the real world.\\n\\n1 1:18:15 So, would you be tempted to get into experience machine, what do we can put a compatible with the fun criteria, I mean, is it just a virtual reality world in which things work better than in the real world or something?\\n\\n2 1:18:35 Yeah, so it's a thought experiment by Robert Nozick, and the idea is that you would enter this world, but you would forget that you're in virtual reality. So all, I mean, the world would be perfect in every possible way that it could be perfect or not perfect, but it would be better in every possible way it could be better. But you would think the relationships you have here are real, the knowledge you're discovering here is novel\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I had Tyler Cowen on my podcast, I asked him why he thinks, so he thinks that human civilization is only going to be around for 700 more years, and then so I asked him, I gave him, you know, your rebuttal, or what I understand of your rebuttal, that, you know, creative, optimistic societies will innovate ways of, you know, safety technology is faster than totalitarian static societies can innovate destructive technologies. And he responded, you know, maybe, but the cost of destruction is just so much lower than the cost of building. And, you know, that trend has been going on for a while now. What happens when a new cost, $60,000? Or what happens if there's a mistake like the kinds that we saw many times over in the Cold War? How would you respond to that?\\n\\n1 1:03:38 First of all, I think we've been getting safer and safer throughout the entire history of civilization. The, you know, there were these plagues that wiped out a third of the population of the world or half. And it could have been 99% or 100%. went through some kind of bottleneck 70,000 years ago, I understand, which they can tell from genetics, all our cousin species have been wiped out. So we were much less safe then than now. Also, if a asteroid, 10-kilometer asteroid, had been on target with the earth at any time in the in the past two million year or whatever it is history of the genus homo that would have been the end of it whereas now it'll just mean higher taxation for a while you know that that's the that's how much amazingly safer we are now uh i i would never say that it's impossible that we'll destroy ourselves. That would be the contrary to the universality of the human mind. We can make wrong choices. We can make so many wrong choices that we'll destroy ourselves. And on the other hand, the atomic bomb accident sort of thing would have had no zero chance of destroying civilization. All they would have done is cause a vast amount of suffering. And... but I don't think we have the technology to end civilization even if we wanted to. I think all we would do if we just deliberately unleashed hell all over the world is we would cause a vast amount of suffering. But there would be survivors and they would resolve never to do that again.\\n\\nSo I don't think we're even able to, let alone that we would do it accidentally. But as for the bad guys, well, I think we are doing the wrong thing largely in regard to both external and internal threats. But I don't think we're doing the wrong thing to an existential risk level. And over the next 700 years or whatever it is, well, I don't want to prophesy because I don't know most of the advances that are going to be made in that time. I see no reason why if we are solving problems, we won't solve problems. I don't think this, this, to take another metaphor, Nick Bostrom's jar with white balls and there's one black ball, and you take out a white ball and white ball and white ball, and then you hit the black ball and that's the end of you. I don't think it's like that because every white ball you take out and have reduces the number of black balls in the jar. So again I'm not saying that's the law of nature. It could be that the very next ball we take out will be the black one that'll be the end of us. It could be but I think all arguments that it will be are fallacious.\\n\\n2 1:07:17 I do want to talk about the fun criterion. Is your definition of fun different from how other people define other positive emotions like eudaemonia or well being or satisfaction? Is it fun a different emotion?\\n\\n1 1:07:29 I don't think it's an emotion. And and I all these things are not very well defined. They can't possibly be very well defined until we have a satisfactory theory of qualia, at least, and probably more satisfactory theory of creativity, how creativity works and so on. I think that the choice of the word fun for the thing that I explain more precisely, but still not very precisely, as a creation of knowledge without,\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" our knowledge of, of grammar is my was my favorite example, as we know why certain sentences are acceptable and why they're unacceptable, but we can't state explicitly, or in every case, why it isn't or why it is. And then there's explicit and inexplicit knowledge, there's conscious and unconscious knowledge. All those are bits of program in the brain, they're ideas, they are bits of knowledge in this, if you define knowledge as information with causal power, they are all information with causal power. They all contain truth and they all contain error and it's always a mistake to shield something, to shield one of them from criticism or replacement.\\n\\nNot doing that is what I call the fun criterion. Now you might say that's a bad name, but you know, it's the best I can find.\\n\\n2 1:14:11 So why would creating an AGI through evolution necessarily entail suffering? Because the way I see it, or it seems to me your theory is that you need to be a general intelligence in order to feel suffering, but by the point an evolved, simulated being is a general intelligence, we can just stop the simulation.\\n\\n1 1:14:31 And so where is the suffering coming from? Okay, so the kind of simulation by evolution that I'm thinking of, I mean, there may be several kinds, but the kind that I'm thinking of, and which I said would be the greatest crime in history, is the kind that just simulates the actual evolution of humans from pre humans that weren't people. So you have you have a population of non people which in this simulation would be some kind of NPCs. And then they would they would just evolve we don't know what the criterion would be we just have an artificial universe which simulated the surface of the earth and they'd be walking around and some of them might or might not become people. And now the thing is when you're part of the way there, what is happening is that you have the way that I, the only way that I can imagine the evolution of personhood or create the explanatory creativity happened was that the hardware needed for it was first needed for something else. I have proposed that it was needed to transmit memes. So there'd be people who were transmitting memes creatively but they were running out of resources before it managed to increase their stock of memes. So in every generation, there was a stock of memes that was being passed down to the next generation. And once they got beyond a certain complexity, they had to be passed down by the use of creativity by the recipient.\\n\\nSo, there may well have been a time and as I say I can't think of any other way it could have been, where there was genuine creativity being used, but it ran out of resources, very quickly, but not so quickly that it didn't increase the mean bandwidth. Then in the next generation, there was more mean bandwidth, and then after, you know, certain number of generations, there would have been some opportunity to use this hardware or whatever it is, you know, firmware, I expect, to use this firmware for something other than just trying blindly transmitting memes or rather creatively transmitting memes, but they were blind memes. So in that time, it would have been very unpleasant to be alive. It was already very unpleasant to, sorry, it was very unpleasant to be alive when we did have enough resources to think as well as do the memes. But I don't think there would have been a moment at which you would say, yes, now the suffering begins to matter because it's not just blind memes. I think the people were already suffering at the time when they were blindly transmitting memes because they were using genuine creativity. They were just not using it to any good effect.\\n\\n2 1:17:56 Gotcha. Would being in the experience machine be compatible with the fun criterion so you're not aware that you're in, you know, the experience machine it's all virtual reality, but you're still doing the things that would make you have fun in fact more so than in the real world.\\n\\n1 1:18:15 So, would you be tempted to get into experience machine, what do we can put a compatible with the fun criteria, I mean, is it just a virtual reality world in which things work better than in the real world or something?\\n\\n2 1:18:35 Yeah, so it's a thought experiment by Robert Nozick, and the idea is that you would enter this world, but you would forget that you're in virtual reality. So all, I mean, the world would be perfect in every possible way that it could be perfect or not perfect, but it would be better in every possible way it could be better. But you would think the relationships you have here are real, the knowledge you're discovering here is novel\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or practical aim. This isn’t good. Never mind the terminology; change it if you like, and there are indeed reasons for treating various entities with respect, protecting them from harm and so on. All the same, the distinction between actual people, defined by that objective criterion, and other entities has enormous moral and practical significance, and is going to become vital to the functioning of a civilisation that includes AGIs.\\n\\nThe battle between good and evil ideas is as old as our species and will go on regardless of the hardware on which it is running\\n\\nFor example, the mere fact that it is not the computer but the running program that is a person, raises unsolved philosophical problems that will become practical, political controversies as soon as AGIs exist. Once an AGI program is running in a computer, to deprive it of that computer would be murder (or at least false imprisonment or slavery, as the case may be), just like depriving a human mind of its body. But unlike a human body, an AGI program can be copied into multiple computers at the touch of a button. Are those programs, while they are still executing identical steps (ie before they have become differentiated due to random choices or different experiences), the same person or many different people? Do they get one vote, or many? Is deleting one of them murder, or a minor assault? And if some rogue programmer, perhaps illegally, creates billions of different AGI people, either on one computer or on many, what happens next? They are still people, with rights. Do they all get the vote?\\n\\nFurthermore, in regard to AGIs, like any other entities with creativity, we have to forget almost all existing connotations of the word ‘programming’. To treat AGIs like any other computer programs would constitute brainwashing, slavery, and tyranny. And cruelty to children, too, for ‘programming’ an already-running AGI, unlike all other programming, constitutes education. And it constitutes debate, moral as well as factual. To ignore the rights and personhood of AGIs would not only be the epitome of evil, but also a recipe for disaster: creative beings cannot be enslaved forever.\\n\\nSome people are wondering whether we should welcome our new robot overlords. Some hope to learn how we can rig their programming to make them constitutionally unable to harm humans (as in Isaac Asimov’s ‘laws of robotics’), or to prevent them from acquiring the theory that the universe should be converted into paper clips (as imagined by Nick Bostrom). None of these are the real problem. It has always been the case that a single exceptionally creative person can be thousands of times as productive — economically, intellectually or whatever — as most people; and that such a person could do enormous harm were he to turn his powers to evil instead of good.\\n\\nThese phenomena have nothing to do with AGIs. The battle between good and evil ideas is as old as our species and will continue regardless of the hardware on which it is running. The issue is: we want the intelligences with (morally) good ideas always to defeat the evil intelligences, biological and artificial; but we are fallible, and our own conception of ‘good’ needs continual improvement. How should society be organised so as to promote that improvement? ‘Enslave all intelligence’ would be a catastrophically wrong answer, and ‘enslave all intelligence that doesn’t look like us’ would not be much better.\\n\\nOne implication is that we must stop regarding education (of humans or AGIs alike) as instruction — as a means of transmitting existing knowledge unaltered, and causing existing values to be enacted obediently. As Popper wrote (in the context of scientific discovery, but it applies equally to the programming of AGIs and the education of children): ‘there is no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AG', metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=' ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …\\n\\n… And self\\n\\nawareness.’\\n\\nRemember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.\\n\\nPerhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.\\n\\nAGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.\\n\\nIronically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’\\n\\n‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.\\n\\nSome, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science.)\\n\\nThat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non-cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.\\n\\nCurrently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or', metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\"2 1:18:35 Yeah, so it's a thought experiment by Robert Nozick, and the idea is that you would enter this world, but you would forget that you're in virtual reality. So all, I mean, the world would be perfect in every possible way that it could be perfect or not perfect, but it would be better in every possible way it could be better. But you would think the relationships you have here are real, the knowledge you're discovering here is novel, and so on. Would you be tempted to enter such a world?\\n\\n1 1:19:02 Well, no. I certainly wouldn't want to enter a world, any world, which involves erasing the memory that I have come from this world. Related to that is the fact that the laws of physics in this virtual world couldn't be the true ones, because the true ones aren't yet known. So I'd be in a world in which I was trying to learn laws of physics which aren't the actual laws, and they would have been designed by somebody for some purpose to manipulate me, as it were. Maybe it would be designed to, like, be a puzzle that would take 50 years to solve, but it would have to be, by definition, a finite puzzle, and it wouldn't be the actual world. And meanwhile, in the actual world, things are going wrong, and I don't know about this, and eventually they go so wrong that my computer runs out of power and then where will I be?\\n\\n2 1:20:04 The final question I always like to ask people I interview is what advice would you give to young people? So somebody in their 20s, is there something that you would like to,\\n\\n1 1:20:12 some advice you would give them advice. I can have opinions about things. So, for example, I may have an opinion that it's dangerous to condition your short term goals by reference to some long-term goal. And I have what I think is a good epistemological reason for that, namely that if your short-term goals are subordinate to your long-term goal, then if your long-term goal is wrong or deficient in some way, you won't find out until you're dead. So it's a bad idea because it is subordinating the things that you could error correct now, or in six months time or in a year's time, to something that you could only error correct on a 50-year timescale, and then it'll be too late. So I'm suspicious of advice of the form, set your goal, and even more suspicious of make your goal be so-and-so.\\n\\n2\\n\\n1:21:38\\n\\nHuh. Interesting.\\n\\n1\\n\\n1:21:40\\n\\nSo that's an example.\\n\\n4\\n\\n1:21:42\\n\\nBut why is it,\\n\\n2 1:21:44 why do you think the relationship between advicee and advice-giver is dangerous?\\n\\n1 1:21:52 Oh, well, because it's one of authority. Again, you know, I tried to make this example of quote advice that I just gave. I tried to make it non-authoritative. I just gave an argument for why certain other arguments are bad. So, but if it's a non-argument. If I have an argument, I can give the argument and not tell the person what to do. Who knows what somebody might do with an argument? They might change it to a better argument, which actually implies different behavior. I can contribute to the world arguments, make arguments as best I can. I don't claim that they are privileged over other arguments. I just put them out because I think that this argument works. And I expect other people not to think that they work. I mean, we've just done this in this very podcast. I put out an argument about AI and that kind of thing, and you criticize it. Now, if I was in the position of making that argument and saying that therefore you should do so and so, that's a relationship of authority, which I think is immoral to have. Well, David, thanks so much for coming on the podcast and thanks so much for giving me so much of your time. Well, it's been fascinating. Thank you for inviting me. for giving me so much of your time. Well, it's been fascinating. Thank you for inviting me.\\n\\n3\\n\\n1:23:49\\n\\n.\\n\\nTranscribed with Cockatoo\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" our knowledge of, of grammar is my was my favorite example, as we know why certain sentences are acceptable and why they're unacceptable, but we can't state explicitly, or in every case, why it isn't or why it is. And then there's explicit and inexplicit knowledge, there's conscious and unconscious knowledge. All those are bits of program in the brain, they're ideas, they are bits of knowledge in this, if you define knowledge as information with causal power, they are all information with causal power. They all contain truth and they all contain error and it's always a mistake to shield something, to shield one of them from criticism or replacement.\\n\\nNot doing that is what I call the fun criterion. Now you might say that's a bad name, but you know, it's the best I can find.\\n\\n2 1:14:11 So why would creating an AGI through evolution necessarily entail suffering? Because the way I see it, or it seems to me your theory is that you need to be a general intelligence in order to feel suffering, but by the point an evolved, simulated being is a general intelligence, we can just stop the simulation.\\n\\n1 1:14:31 And so where is the suffering coming from? Okay, so the kind of simulation by evolution that I'm thinking of, I mean, there may be several kinds, but the kind that I'm thinking of, and which I said would be the greatest crime in history, is the kind that just simulates the actual evolution of humans from pre humans that weren't people. So you have you have a population of non people which in this simulation would be some kind of NPCs. And then they would they would just evolve we don't know what the criterion would be we just have an artificial universe which simulated the surface of the earth and they'd be walking around and some of them might or might not become people. And now the thing is when you're part of the way there, what is happening is that you have the way that I, the only way that I can imagine the evolution of personhood or create the explanatory creativity happened was that the hardware needed for it was first needed for something else. I have proposed that it was needed to transmit memes. So there'd be people who were transmitting memes creatively but they were running out of resources before it managed to increase their stock of memes. So in every generation, there was a stock of memes that was being passed down to the next generation. And once they got beyond a certain complexity, they had to be passed down by the use of creativity by the recipient.\\n\\nSo, there may well have been a time and as I say I can't think of any other way it could have been, where there was genuine creativity being used, but it ran out of resources, very quickly, but not so quickly that it didn't increase the mean bandwidth. Then in the next generation, there was more mean bandwidth, and then after, you know, certain number of generations, there would have been some opportunity to use this hardware or whatever it is, you know, firmware, I expect, to use this firmware for something other than just trying blindly transmitting memes or rather creatively transmitting memes, but they were blind memes. So in that time, it would have been very unpleasant to be alive. It was already very unpleasant to, sorry, it was very unpleasant to be alive when we did have enough resources to think as well as do the memes. But I don't think there would have been a moment at which you would say, yes, now the suffering begins to matter because it's not just blind memes. I think the people were already suffering at the time when they were blindly transmitting memes because they were using genuine creativity. They were just not using it to any good effect.\\n\\n2 1:17:56 Gotcha. Would being in the experience machine be compatible with the fun criterion so you're not aware that you're in, you know, the experience machine it's all virtual reality, but you're still doing the things that would make you have fun in fact more so than in the real world.\\n\\n1 1:18:15 So, would you be tempted to get into experience machine, what do we can put a compatible with the fun criteria, I mean, is it just a virtual reality world in which things work better than in the real world or something?\\n\\n2 1:18:35 Yeah, so it's a thought experiment by Robert Nozick, and the idea is that you would enter this world, but you would forget that you're in virtual reality. So all, I mean, the world would be perfect in every possible way that it could be perfect or not perfect, but it would be better in every possible way it could be better. But you would think the relationships you have here are real, the knowledge you're discovering here is novel\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" reality generator could also place thoughts, as well as sense data,\\n\\n1 0:37:24 into the mind? Yes, but that's only because I think that this model is wrong. It's basically the Cartesian theater, as Daniel Dennett puts it, with the stage cleared of all the characters. So, that's pure consciousness without content, as Sam Harris envisages it. But I think that all that's happening there is that you are conscious of this theater, and you're envisaging it as having certain properties, which, by the way, it doesn't have, but that doesn't matter. We can imagine lots of things that don't happen. You know, in fact, you know, that's in a way characterizes what we do all the time. So, one can interpret one's thoughts about this empty stage as being pure, contentless consciousness. But it's not. It has the content of a stage or a space or,\\n\\n2 0:38:40 you know, however you want to envisage it. Okay, and then let's talk about the Turing principle. So this is a term you coined. It's otherwise been called the church touring Deutsch principle. With this principle imply that you could. So, by the way, it states that any universal computer can simulate any physical process with this principle imply that you could simulate the whole of the universe for example in a compact efficient computer that\\n\\n1 0:39:06 was smaller than the universe itself, or is a constraint to physical processes of a task where it was computationally able to do it, but it wouldn't have enough memory or time. So the more memory and time you gave it, the more closely it could simulate the whole universe or anything near the whole universe probably because it, well, if you wanted to simulate itself as well, then there are logical reasons why there are limits to that. But even if you wanted to simulate the whole universe apart from itself, just the sheer size of the universe makes that impossible, even if we discovered ways of encoding information extremely densely like some people have said maybe quantum gravity would allow you know the rest of the universe as well so. from being limited by computational capacity, because it's only when you separate those that you realize what computational universality is. And I think that's universality, like Turing or quantum universality is the most important thing in the theory of computation, because computation doesn't even make sense, unless you have a concept of a universal computer.\\n\\n2 0:41:20 What could falsify your theory that all interesting problems are soluble so I asked this because, as I'm not convinced they're right. But do you have a strong reason for in principle believing that they're wrong.\\n\\n1 0:41:47 No. So, this, this is a bad explanation. So let's say that some people say, for example, that simulating a human brain is impossible. Now, I can't prove this possible. Nobody can prove this possible until they actually do it or unless they have a design for it which they prove will work. So, pending that there is there is no way of proving that, that it's not true that this is a fundamental limitation. is that it could be applied to anything. For example, it could be applied to the theory that you have recently, just a minute ago, been replaced by a humanoid robot, which is going to say for the next few minutes, just a prearranged set of things, and you're no longer a person. I can't believe you figured it out. Yeah, well, that's the first thing you'd say. So there is no way to refute that by experiment, short of actually doing it, short of actually talking to you and so on. So it's the same with all these other things. In order for it to make sense to have a theory that something is impossible, you have to have an explanation for why it is impossible. So we know that, for example, almost all mathematical propositions are undecidable. So that's not because somebody has said, oh maybe we can't decide everything because thinking we could decide everything is hubris. That's not an argument.\\n\\nYou need an actual functional argument to prove that that is so, and then, at being a functional argument in which the steps of the argument makes sense and relate to other things and so on. You can then say well what does this actually mean. Does this mean that maybe we can never understand the laws of physics included an undecidable function, then we would simply write, you know, f of x and f of x and undecidable function. We couldn't evaluate x it would limit our ability to make predictions, but then, lots of our ability to make predictions is totally limited anyway. to understand the properties of the function f and therefore the\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\" to other things and so on. You can then say well what does this actually mean. Does this mean that maybe we can never understand the laws of physics included an undecidable function, then we would simply write, you know, f of x and f of x and undecidable function. We couldn't evaluate x it would limit our ability to make predictions, but then, lots of our ability to make predictions is totally limited anyway. to understand the properties of the function f and therefore the properties of the physical world. Okay, is a system of government like America's which has distributed powers and checks and\\n\\n2 0:44:57 balances, is that incompatible with Popper's criterion? So the reason I ask is the last administration had a theory that if you build a wall there will be positive consequences and you know that theory could have been tested and then the person could have been evaluated on whether that theory succeeded. But because our system of government has distributed powers, you know, Congress opposed the testing of that theory, and so it was never tested. So if our American government wanted to fulfill Popper's criterion, would we need to give the president more power, for example?\\n\\n1 0:45:30 It's not as simple as that. So I agree that this is a big defect in the American system No country has a system of government that perfectly fulfills Popper's criterion. We can always improve. I think the British one is actually the best in the world and it's far from optimal. Making a single change like that is not going to be the answer. The constitution of a polity is a very complicated thing, much of which is inexplicit. So, the founding fathers, the American founding fathers realized they had a tremendous problem. What they wanted to do, what they thought of themselves as doing, was to implement the British Constitution. In fact they thought they were the defenders of the British Constitution and that the British King had violated it and was bringing it down. They wanted to retain it. The trouble is that they all, in order to do this, to gain the independence to do this, they had to get rid of the King and then they wondered whether they should get an alternative king. Whichever way they did it, there were problems. The way they decided to do it, I think, made for a system that was inherently much worse than the one they were replacing, but they had no choice. If they wanted to get rid of a king, they had to have a different system for having a head of state. Therefore they had to have... they wanted to be democratic. That meant that the president had a legitimacy in legislation that the king never had. Oh sorry, never had.\\n\\nThe king did used to have it in medieval times, but the king by the time of the Enlightenment and so on, no longer had full legitimacy to legislate. So they had to implement a system where him seizing power was prevented by something other than tradition. And so they instituted these checks and balances. Checks and... so the whole thing the institute was immensely sophisticated. It's an amazing intellectual achievement and that it works as well as it does is something... something of a miracle. But the inherent flaws are there and one of them is this... the fact that there are checks and balances means that responsibility is dissipated and nobody is ever to blame for anything in the American system, which is terrible. In the British system, blame is absolutely focused, you know, everything is sacrificed to the end of focusing blame and responsibility down to the government. That's where it's all focused into. And there are no systems that do that better, but as you well know, the British system also has flaws. And we recently saw with the sequence of events with Brexit referendum and then Parliament balking at implementing some laws they didn't agree with. And then that being referred to the courts. And so there was the courts and the Parliament and the government and the Prime Minister all blaming each other.\\n\\nAnd there was a sort of mini constitutional crisis, which could only be resolved by having an election and then having a majority government, which is by the mathematics of how the government works, that's how it usually is in Britain. Although, you know, we have been unlucky several times recently in not having a majority government.\\n\\n2 0:49:56 Okay, so this could be wrong, but it seems to be in an expanded universe are, there will be like a finite amount of total matter that will ever exist in our light cone right there's a limit. And that means that there's a limit on the amount of computation that this matter can, you know, execute the amount of energy can provide, perhaps even the amount of economic value we can sustain right so we, it would be weird if the GDP per atom could be arbitrarily large. So, does this impose some sort\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" could be that the culture of these labs is not good enough, in which case it would be part of the next experiment to improve the culture in the labs. But I am very suspicious of saying that all labs have to stop and meet a criterion, because I'm sure that the, well, I suspect that the stopping wouldn't be necessary and the criterion wouldn't be appropriate. Again, which criterion to use depends on the actual research being done.\\n\\n2 1:02:51 When I had Tyler Cowen on my podcast, I asked him why he thinks, so he thinks that human civilization is only going to be around for 700 more years, and then so I asked him, I gave him, you know, your rebuttal, or what I understand of your rebuttal, that, you know, creative, optimistic societies will innovate ways of, you know, safety technology is faster than totalitarian static societies can innovate destructive technologies. And he responded, you know, maybe, but the cost of destruction is just so much lower than the cost of building. And, you know, that trend has been going on for a while now. What happens when a new cost, $60,000? Or what happens if there's a mistake like the kinds that we saw many times over in the Cold War? How would you respond to that?\\n\\n1 1:03:38 First of all, I think we've been getting safer and safer throughout the entire history of civilization. The, you know, there were these plagues that wiped out a third of the population of the world or half. And it could have been 99% or 100%. went through some kind of bottleneck 70,000 years ago, I understand, which they can tell from genetics, all our cousin species have been wiped out. So we were much less safe then than now. Also, if a asteroid, 10-kilometer asteroid, had been on target with the earth at any time in the in the past two million year or whatever it is history of the genus homo that would have been the end of it whereas now it'll just mean higher taxation for a while you know that that's the that's how much amazingly safer we are now uh i i would never say that it's impossible that we'll destroy ourselves. That would be the contrary to the universality of the human mind. We can make wrong choices. We can make so many wrong choices that we'll destroy ourselves. And on the other hand, the atomic bomb accident sort of thing would have had no zero chance of destroying civilization. All they would have done is cause a vast amount of suffering. And... but I don't think we have the technology to end civilization even if we wanted to. I think all we would do if we just deliberately unleashed hell all over the world is we would cause a vast amount of suffering. But there would be survivors and they would resolve never to do that again.\\n\\nSo I don't think we're even able to, let alone that we would do it accidentally. But as for the bad guys, well, I think we are doing the wrong thing largely in regard to both external and internal threats. But I don't think we're doing the wrong thing to an existential risk level. And over the next 700 years or whatever it is, well, I don't want to prophesy because I don't know most of the advances that are going to be made in that time. I see no reason why if we are solving problems, we won't solve problems. I don't think this, this, to take another metaphor, Nick Bostrom's jar with white balls and there's one black ball, and you take out a white ball and white ball and white ball, and then you hit the black ball and that's the end of you. I don't think it's like that because every white ball you take out and have reduces the number of black balls in the jar. So again I'm not saying that's the law of nature. It could be that the very next ball we take out will be the black one that'll be the end of us. It could be but I think all arguments that it will be are fallacious.\\n\\n2 1:07:17 I do want to talk about the fun criterion. Is your definition of fun different from how other people define other positive emotions like eudaemonia or well being or satisfaction? Is it fun a different emotion?\\n\\n1 1:07:29 I don't think it's an emotion. And and I all these things are not very well defined. They can't possibly be very well defined until we have a satisfactory theory of qualia, at least, and probably more satisfactory theory of creativity, how creativity works and so on. I think that the choice of the word fun for the thing that I explain more precisely, but still not very precisely, as a creation of knowledge without,\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'})]\n",
      "[Document(page_content=\"2 1:18:35 Yeah, so it's a thought experiment by Robert Nozick, and the idea is that you would enter this world, but you would forget that you're in virtual reality. So all, I mean, the world would be perfect in every possible way that it could be perfect or not perfect, but it would be better in every possible way it could be better. But you would think the relationships you have here are real, the knowledge you're discovering here is novel, and so on. Would you be tempted to enter such a world?\\n\\n1 1:19:02 Well, no. I certainly wouldn't want to enter a world, any world, which involves erasing the memory that I have come from this world. Related to that is the fact that the laws of physics in this virtual world couldn't be the true ones, because the true ones aren't yet known. So I'd be in a world in which I was trying to learn laws of physics which aren't the actual laws, and they would have been designed by somebody for some purpose to manipulate me, as it were. Maybe it would be designed to, like, be a puzzle that would take 50 years to solve, but it would have to be, by definition, a finite puzzle, and it wouldn't be the actual world. And meanwhile, in the actual world, things are going wrong, and I don't know about this, and eventually they go so wrong that my computer runs out of power and then where will I be?\\n\\n2 1:20:04 The final question I always like to ask people I interview is what advice would you give to young people? So somebody in their 20s, is there something that you would like to,\\n\\n1 1:20:12 some advice you would give them advice. I can have opinions about things. So, for example, I may have an opinion that it's dangerous to condition your short term goals by reference to some long-term goal. And I have what I think is a good epistemological reason for that, namely that if your short-term goals are subordinate to your long-term goal, then if your long-term goal is wrong or deficient in some way, you won't find out until you're dead. So it's a bad idea because it is subordinating the things that you could error correct now, or in six months time or in a year's time, to something that you could only error correct on a 50-year timescale, and then it'll be too late. So I'm suspicious of advice of the form, set your goal, and even more suspicious of make your goal be so-and-so.\\n\\n2\\n\\n1:21:38\\n\\nHuh. Interesting.\\n\\n1\\n\\n1:21:40\\n\\nSo that's an example.\\n\\n4\\n\\n1:21:42\\n\\nBut why is it,\\n\\n2 1:21:44 why do you think the relationship between advicee and advice-giver is dangerous?\\n\\n1 1:21:52 Oh, well, because it's one of authority. Again, you know, I tried to make this example of quote advice that I just gave. I tried to make it non-authoritative. I just gave an argument for why certain other arguments are bad. So, but if it's a non-argument. If I have an argument, I can give the argument and not tell the person what to do. Who knows what somebody might do with an argument? They might change it to a better argument, which actually implies different behavior. I can contribute to the world arguments, make arguments as best I can. I don't claim that they are privileged over other arguments. I just put them out because I think that this argument works. And I expect other people not to think that they work. I mean, we've just done this in this very podcast. I put out an argument about AI and that kind of thing, and you criticize it. Now, if I was in the position of making that argument and saying that therefore you should do so and so, that's a relationship of authority, which I think is immoral to have. Well, David, thanks so much for coming on the podcast and thanks so much for giving me so much of your time. Well, it's been fascinating. Thank you for inviting me. for giving me so much of your time. Well, it's been fascinating. Thank you for inviting me.\\n\\n3\\n\\n1:23:49\\n\\n.\\n\\nTranscribed with Cockatoo\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content=\" no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.\\n\\nI do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.\\n\\nThe lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\\n\\nWithout understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.\\n\\nClearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever.Lulie Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.\\n\\nDavid Deutsch Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.\\n\\nLulie Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.\\n\\nDavid Deutsch is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.\\n\\nLulie Did you see a similar freak-out in the last time this happened?\\n\", metadata={'folder': 'david_deutsch_transcripts'}), Document(page_content='the same thing happening again’ under some explanation.\\n\\nSo, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.\\n\\nIn regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world: not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.\\n\\nNow, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.\\n\\nPresent-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability\\n\\nCurrently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.\\n\\nFurthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything, including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.\\n\\nNowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the', metadata={'folder': 'david_deutsch_transcripts'})]\n"
     ]
    }
   ],
   "source": [
    "dicts_with_arguments_and_explanations = third_cycle_of_extracting_arguments(dicts_with_isolated_improved_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8c825fd1-8012-402d-83db-ef597ad6d8d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:38.841567Z",
     "start_time": "2024-04-11T15:30:38.832998Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dict_to_json(dicts_with_arguments_and_explanations, \"./sources//json/third_cycle_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2ab9f15f-524b-4fdf-88d4-d7aa7011997f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:41.208291Z",
     "start_time": "2024-04-11T15:30:41.199556Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_third_step(dicts):\n",
    "    for dict in dicts:\n",
    "        dict[\"final_arguments\"] = []\n",
    "        for i, isolated_argument_group in enumerate(dict[\"isolated_arguments\"]):\n",
    "            for j, single_isolated_arg in enumerate(isolated_argument_group):\n",
    "                final_arg = \"\\n\".join([single_isolated_arg, dict[\"explanations\"][i][j]])\n",
    "                dict[\"final_arguments\"].append(final_arg)\n",
    "                final_arg_yaml_format = \" ```yaml\\n\" + final_arg + \"\\n```\\n\\n\"\n",
    "                filename = f\"{dict['path']}/steps/third_step.md\"\n",
    "                write_to_file(filename, final_arg_yaml_format)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c0154e3c-3746-40e2-ad9f-ae5d6dd4ae2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:44.191326Z",
     "start_time": "2024-04-11T15:30:44.177897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n",
      "Text successfully written to ./sources/david_deutsch_transcripts/steps/third_step.md\n"
     ]
    }
   ],
   "source": [
    "final_dicts = save_third_step(dicts_with_arguments_and_explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "36b27afc-ee82-4e3e-9cce-0f3e4ce2e5e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:47.458411Z",
     "start_time": "2024-04-11T15:30:47.445456Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_directory_structure_for_chatbots(directory_name):\n",
    "    script_directory = os.getcwd()\n",
    "    new_directory_path = os.path.join(script_directory, directory_name)\n",
    "    if not os.path.exists(new_directory_path):\n",
    "        os.makedirs(new_directory_path)\n",
    "        yaml_file_path = os.path.join(new_directory_path, \"metadata.yaml\")\n",
    "        with open(yaml_file_path, \"w\") as yaml_file:\n",
    "            with open(yaml_file_path, \"w\") as yaml_file:\n",
    "                yaml.dump({}, yaml_file)\n",
    "        prompts_folder_path = os.path.join(new_directory_path, \"prompts\")\n",
    "        knowledge_base_folder_path = os.path.join(new_directory_path, \"knowledge_base\")\n",
    "        for folder_path in [prompts_folder_path, knowledge_base_folder_path]:\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "                system_prompt_file_path = os.path.join(prompts_folder_path, \"system_prompt.md\")\n",
    "                with open(system_prompt_file_path, \"w\") as system_prompt_file:\n",
    "                    system_prompt_file.write(\"System Prompt\")\n",
    "    if os.path.exists(new_directory_path):\n",
    "        yaml_file_path = os.path.join(new_directory_path, \"metadata.yaml\")\n",
    "        with open(yaml_file_path, \"w\") as yaml_file:\n",
    "            yaml.dump({\"name\": None, #add your chatbot name here don't use quotes when naming chatbot \n",
    "                       \"tags\": None, #tag your chatbot can be #optimistic or #pessimistic\n",
    "                       \"based_on\": None #provide the uri to source of the raw_text used for arguments extraction, add it in new line with dash in front uri (- https://uri_to_source.com) \n",
    "                      }, yaml_file, sort_keys=False)\n",
    "        prompts_folder_path = os.path.join(new_directory_path, \"prompts\")\n",
    "        knowledge_base_folder_path = os.path.join(new_directory_path, \"knowledge_base\")\n",
    "        for folder_path in [prompts_folder_path, knowledge_base_folder_path]:\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f7d0f150-8dd7-4f22-b4e3-5a90fe87ef9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:50.396061Z",
     "start_time": "2024-04-11T15:30:50.390986Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_chatbots(dicts):\n",
    "    for dict in dicts:\n",
    "        create_directory_structure_for_chatbots(dict[\"name\"])\n",
    "        prompt_filename = f'./{dict[\"name\"]}/prompts/system_prompt.md'\n",
    "        with open(prompt_filename, 'w') as file:\n",
    "            file.write(\"Use arguments provided to answer the question.\\n\\nArguments:\\n\\n{arguments}\")\n",
    "        for i, final_arg in enumerate(dict[\"final_arguments\"]):\n",
    "            filename = f'./{dict[\"name\"]}/knowledge_base/{dict[\"name\"]}-{str(i + 1)}.md'\n",
    "            with open(filename, 'w') as file:\n",
    "                file.write(final_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "09e910d2-05b9-4ba3-9dd6-b7864137f4fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:53.428704Z",
     "start_time": "2024-04-11T15:30:53.412736Z"
    }
   },
   "outputs": [],
   "source": [
    "create_chatbots(final_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "29732691-f432-41bc-91ca-255740d541d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:55.623582Z",
     "start_time": "2024-04-11T15:30:55.612017Z"
    }
   },
   "outputs": [],
   "source": [
    "dicts_with_isolated_improved_arguments = load_results_from_json(2)\n",
    "dicts_with_arguments_and_explanations = load_results_from_json(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11187a69-7f67-4195-8806-ccd4251f3589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
