{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f418b28d-060a-4e1c-b804-d76264ca0e0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:15:31.499674Z",
     "start_time": "2024-04-11T08:15:23.830030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.1.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.8)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (1.4.47)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (0.5.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (0.0.21)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.24 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (0.1.25)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (0.1.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (2.27.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.8) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.8) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.8) (0.8.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.8) (2.3)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.24->langchain==0.1.8) (3.6.2)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.24->langchain==0.1.8) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.8) (4.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain==0.1.8) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain==0.1.8) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain==0.1.8) (2.10)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.8) (2.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.24->langchain==0.1.8) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.8) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: dropbox 11.36.0 has a non-standard dependency specifier stone>=2.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of dropbox or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai==0.0.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.6)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-openai==0.0.6) (0.1.25)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-openai==0.0.6) (1.23.5)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-openai==0.0.6) (1.12.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-openai==0.0.6) (0.6.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (6.0)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (3.6.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (0.1.5)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.27.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.0.6) (2023.3.23)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.0.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: dropbox 11.36.0 has a non-standard dependency specifier stone>=2.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of dropbox or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv==1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: dropbox 11.36.0 has a non-standard dependency specifier stone>=2.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of dropbox or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb==0.4.18 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.18)\n",
      "Collecting requests>=2.28 (from chromadb==0.4.18)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.10.7)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (0.7.3)\n",
      "Collecting fastapi>=0.95.2 (from chromadb==0.4.18)\n",
      "  Downloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb==0.4.18)\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (4.9.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (3.3.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.15.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (0.42b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.21.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (0.13.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (4.65.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (6.1.1)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.59.3)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (4.0.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (28.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (6.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (4.0.1)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.23.5)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb==0.4.18)\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb==0.4.18)\n",
      "  Downloading fastapi_cli-0.0.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.18) (0.27.0)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.18) (3.1.2)\n",
      "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb==0.4.18)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb==0.4.18)\n",
      "  Downloading ujson-5.10.0-cp310-cp310-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting orjson>=3.2.1 (from fastapi>=0.95.2->chromadb==0.4.18)\n",
      "  Downloading orjson-3.10.3-cp310-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.9/50.9 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: email_validator>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.18) (2.0.0.post2)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (2022.12.7)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (2.23.4)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (1.5.1)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (3.2.2)\n",
      "Requirement already satisfied: urllib3<2.0,>=1.24.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (1.26.15)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (23.5.26)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (23.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (4.23.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.18) (1.2.13)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.18) (6.6.0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.18) (2.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.18) (1.61.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.21.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.18) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.21.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.18) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.42b0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (0.42b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.42b0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (0.42b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.42b0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (0.42b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.42b0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (0.42b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (67.8.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (3.7.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.4.18) (1.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.4.18) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.4.18) (2.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.65.0->chromadb==0.4.18) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer>=0.9.0->chromadb==0.4.18) (8.1.3)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.18) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (0.5.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (1.0.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (0.19.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (12.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb==0.4.18) (2.3.0)\n",
      "Collecting typer>=0.9.0 (from chromadb==0.4.18)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb==0.4.18)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer>=0.9.0->chromadb==0.4.18) (13.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.18) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.18) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.18) (4.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb==0.4.18) (3.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb==0.4.18) (1.0.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb==0.4.18) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.18) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb==0.4.18) (2.1.2)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.18) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.18) (2.15.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.18) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.18) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.18) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.18) (0.5.1)\n",
      "Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "   ---------------------------------------- 0.0/92.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 92.0/92.0 kB 5.1 MB/s eta 0:00:00\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading fastapi_cli-0.0.3-py3-none-any.whl (9.2 kB)\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.2/47.2 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading orjson-3.10.3-cp310-none-win_amd64.whl (138 kB)\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 138.8/138.8 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.9/71.9 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading ujson-5.10.0-cp310-cp310-win_amd64.whl (42 kB)\n",
      "   ---------------------------------------- 0.0/42.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 42.1/42.1 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: ujson, shellingham, requests, python-multipart, orjson, uvicorn, typer, starlette, fastapi-cli, fastapi\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.27.1\n",
      "    Uninstalling requests-2.27.1:\n",
      "      Successfully uninstalled requests-2.27.1\n",
      "  Attempting uninstall: python-multipart\n",
      "    Found existing installation: python-multipart 0.0.6\n",
      "    Uninstalling python-multipart-0.0.6:\n",
      "      Successfully uninstalled python-multipart-0.0.6\n",
      "  Attempting uninstall: uvicorn\n",
      "    Found existing installation: uvicorn 0.17.6\n",
      "    Uninstalling uvicorn-0.17.6:\n",
      "      Successfully uninstalled uvicorn-0.17.6\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.9.0\n",
      "    Uninstalling typer-0.9.0:\n",
      "      Successfully uninstalled typer-0.9.0\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.17.1\n",
      "    Uninstalling starlette-0.17.1:\n",
      "      Successfully uninstalled starlette-0.17.1\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.74.1\n",
      "    Uninstalling fastapi-0.74.1:\n",
      "      Successfully uninstalled fastapi-0.74.1\n",
      "Successfully installed fastapi-0.111.0 fastapi-cli-0.0.3 orjson-3.10.3 python-multipart-0.0.9 requests-2.31.0 shellingham-1.5.4 starlette-0.37.2 typer-0.12.3 ujson-5.10.0 uvicorn-0.29.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: dropbox 11.36.0 has a non-standard dependency specifier stone>=2.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of dropbox or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autoresearcher 0.0.6 requires openai==0.27.0, but you have openai 1.12.0 which is incompatible.\n",
      "autoresearcher 0.0.6 requires requests==2.26.0, but you have requests 2.31.0 which is incompatible.\n",
      "autoresearcher 0.0.6 requires tiktoken==0.3.3, but you have tiktoken 0.6.0 which is incompatible.\n",
      "supabase 1.0.3 requires httpx<0.24.0,>=0.23.0, but you have httpx 0.27.0 which is incompatible.\n",
      "supabase-py 0.0.2 requires gotrue==0.2.0, but you have gotrue 1.3.1 which is incompatible.\n",
      "supabase-py 0.0.2 requires requests==2.25.1, but you have requests 2.31.0 which is incompatible.\n",
      "torchvision 0.15.2 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.1.8\n",
    "!pip install langchain-openai==0.0.6\n",
    "!pip install python-dotenv==1.0.0\n",
    "!pip install chromadb==0.4.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98356761-e795-4338-9aef-8d05b212388f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:15:50.956757Z",
     "start_time": "2024-04-11T08:15:50.952541Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "106d34e5-61c0-4667-81ea-18bd8437d645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:06.618973Z",
     "start_time": "2024-04-11T14:17:06.598094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7dbba34-6940-430b-8057-419f77abf163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:09.015199Z",
     "start_time": "2024-04-11T14:17:08.944252Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-turbo\",\n",
    "                 temperature=0.7,\n",
    "                 model_kwargs={\n",
    "                    \"frequency_penalty\": 0.0,\n",
    "                     \"presence_penalty\": 0.0,\n",
    "                     \"top_p\": 1.0,\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729fece9-860b-4fd9-9e40-10f2962026bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:11.592018Z",
     "start_time": "2024-04-11T14:17:11.577003Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_to_file(filename, text):\n",
    "    try:\n",
    "        directory = os.path.dirname(filename)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        with open(filename, 'a') as file:\n",
    "            file.write(text)\n",
    "        print(\"Text successfully written to\", filename)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd30f70b-2af9-4a99-8018-f7ebbe34108f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:14.291166Z",
     "start_time": "2024-04-11T14:17:14.287058Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_dict_to_json(data, filename):\n",
    "    directory = os.path.dirname(filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78957f9-1683-4c12-ac90-136b14419ee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:16.344374Z",
     "start_time": "2024-04-11T14:17:16.340455Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_results_from_json(int):\n",
    "    if int == 1:\n",
    "        with open(\"./sources/json/first_cycle_results.json\", 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    if int == 2:\n",
    "        with open(\"./sources/json/second_cycle_results.json\", 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    if int == 3:\n",
    "        with open(\"./sources/json/third_cycle_results.json\", 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8360efe1-f307-466d-aa59-312cf30f0cdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:22.226198Z",
     "start_time": "2024-04-11T14:17:22.220334Z"
    }
   },
   "outputs": [],
   "source": [
    "first_cycle_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"\"\"\n",
    "    Read the following transcript and extract all the arguments made about AI safety. Make sure they are self-contained.\n",
    "\n",
    "  You must stick as close as possible to the transcript - use the author's own words and tone of voice.\n",
    "\n",
    "  You must write each argument in a valid YAML format, surrounded with backticks.\n",
    "  You must separate each argument with a new line.\n",
    "\n",
    "\n",
    "  The simplest possible argument must at least contain three claims:\n",
    "\n",
    "  ```yaml\n",
    "  claim: \"Top-level claim\"\n",
    "  premises:\n",
    "    - claim: \"First independent premise supporting the top-level claim\"\n",
    "    - claim: \"Second independent premise supporting the top-level claim\"\n",
    "  ```\n",
    "\n",
    "  And here's an example of a more complex argument, which also includes examples to illustrate lower-level claims:\n",
    "\n",
    "  ```yaml\n",
    "  claim: \"Top-level claim\"\n",
    "  premises:\n",
    "    - claim: \"First independent premise supporting the top-level claim\"\n",
    "    - claim: \"Supporting premise for the top-level claim\"\n",
    "      example: \"Example supporting this premise\"\n",
    "    - claim: \"Another supporting premise for the top-level claim\"\n",
    "    - claim: \"Second independent premise supporting the top-level claim\"\n",
    "      premises:\n",
    "        - claim: \"Supporting premise for the second independent premise\"\n",
    "        - claim: \"Another supporting premise for the second independent premise\"\n",
    "        - claim: \"Independent premise supporting the second independent premise\"\n",
    "          example: \"Example supporting this independent premise\"\n",
    "  ```\n",
    "\n",
    "  Here's how to read this structure:\n",
    "\n",
    "  The top-level claim is the main argument.\n",
    "  Directly nested under the claim are independent premises. These provide justification independently of other premises.\n",
    "  An example can be nested directly under a claim to provide further context or support.\n",
    "  Just like the top-level claim, each premise can itself be supported by further individual premises, or examples, creating a nested structure.\n",
    "\n",
    "  # Here is the transcript:\n",
    "\n",
    "  {transcript}\n",
    "  \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ae46fd-4e4d-4780-9b11-f839d2a02e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:23.702811Z",
     "start_time": "2024-04-11T14:17:23.696224Z"
    }
   },
   "outputs": [],
   "source": [
    "second_cycle_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"\"\"\n",
    "  Based on the following transcript, make the arguments clear and distinct. \n",
    "  You may need to merge similar arguments to create a better, more logical argument.\n",
    "\n",
    "  Create the best, strongest possible version of the arguments, here's what to do:\n",
    "\n",
    "  - Make sure the arguments is self-contained\n",
    "  - Make the arguments understandable on their own, out-of-context\n",
    "  - Remember, arguments are not a description or an explanation\n",
    "  - Premise must always give a reason to believe the claim above\n",
    "  - Avoid using pronouns in premises\n",
    "  - A claim can have a maximum of two child claims (premises), rewrite if needed\n",
    "\n",
    "  # Argument format \n",
    "\n",
    "  You must write each argument in valid YAML format, surrounded with backticks.\n",
    "  Separate each argument with new line.\n",
    "  You must stick as closely as possible to the transcript. \n",
    "  Above all, you must express the argument in the words of the author, stick as close as possible to the tone of voice and phrases used in the transcript.\n",
    "\n",
    "\n",
    "\n",
    "  Here are some examples:\n",
    "\n",
    "  A simple argument might look like this: \n",
    "\n",
    "  ```yaml\n",
    "  claim: \"Top-level claim\"\n",
    "  premises:\n",
    "    - claim: \"First independent premise supporting the top-level claim\"\n",
    "    - claim: \"Second independent premise supporting the top-level claim\"\n",
    "  ```\n",
    "\n",
    "  And here's an example of a more complex argument, which also includes examples to illustrate lower-level claims:\n",
    "\n",
    "  ```yaml\n",
    "  claim: \"Top-level claim\"\n",
    "  premises:\n",
    "    - claim: \"First independent premise supporting the top-level claim\"\n",
    "    - claim: \"Supporting premise for the top-level claim\"\n",
    "      example: \"Example supporting this premise\"\n",
    "    - claim: \"Another supporting premise for the top-level claim\"\n",
    "    - claim: \"Second independent premise supporting the top-level claim\"\n",
    "      premises:\n",
    "        - claim: \"Supporting premise for the second independent premise\"\n",
    "        - claim: \"Another supporting premise for the second independent premise\"\n",
    "        - claim: \"Independent premise supporting the second independent premise\"\n",
    "          example: \"Example supporting this independent premise\"\n",
    "  ```\n",
    "\n",
    "  Here's how to read this structure:\n",
    "\n",
    "  The top-level claim is the main argument.\n",
    "  Directly nested under the claim are independent premises. These provide justification independently of other premises.\n",
    "  An example can be nested directly under a claim to provide further context or support.\n",
    "  Just like the top-level claim, each premise can itself be supported by further individual premises, or examples, creating a nested structure.\n",
    "\n",
    "\n",
    "  # Arguments to improve:\n",
    "\n",
    "  {all_arguments}\n",
    "\n",
    "  # Transcript\n",
    "\n",
    "  {transcript}\n",
    "  \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa75b11-6b3b-493c-8c32-01ef58a562cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:26.711954Z",
     "start_time": "2024-04-11T14:17:26.706788Z"
    }
   },
   "outputs": [],
   "source": [
    "third_cycle_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world expert at creating accessible, persuasive explanations.\"),\n",
    "    (\"user\", \"\"\"\n",
    "  Based on your own knowledge and the transcript, create a structured explanation for the following argument. Use the context from the transcript for the explanation.\n",
    "\n",
    "  # Argument to use for the explanation\n",
    "\n",
    "  {argument}\n",
    "\n",
    "\n",
    "  The structured explanation must be directly based on the argument. You can also use the provided transcript for context.\n",
    "\n",
    "  You must follow this YAML format:\n",
    "\n",
    "  ```yaml\n",
    "  counteragument_to: (what would be the argument, to which this argument is a counterargument? use your own knowledge. use bullet points)\n",
    "\n",
    "  strongest_objection: (what is the strongest, good-faith, honest objection that a thoughful person might have? use bullet points)\n",
    "  consequences_if_true: (if true, what would be the consequences? write in causal language,  use bullet points, max 3)\n",
    "\n",
    "  link_to_ai_safety: (how is this linked to AI safety? 1 sentence.)\n",
    "\n",
    "  simple_explanation: (explain this clearly to a college student in max. 4 sentences, speak persuasively as the author of this argument. don't use bullet points)\n",
    "\n",
    "  examples: (max 3 examples, use bullet points)\n",
    "\n",
    "  ```\n",
    "\n",
    "  # Here is the transcript:\n",
    "\n",
    "  {transcript}\n",
    "  \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "274af6ed-0d16-4e5b-ae57-868d7418efa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:22:12.279536Z",
     "start_time": "2024-04-11T08:22:12.272852Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_text_from_sources_and_make_chunks(directory):\n",
    "    folder_names = []\n",
    "    raw_texts = []\n",
    "    for entry in os.listdir(directory):\n",
    "        folder_names.append(entry)\n",
    "        print(folder_names)\n",
    "    for folder_name in folder_names:\n",
    "        loader = DirectoryLoader(f'./sources/{folder_name}/source', glob=\"**/*.md\")\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=10000,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len)\n",
    "        raw_text = loader.load()\n",
    "        entire_text_from_a_source = \"\"\n",
    "        for doc in raw_text:\n",
    "            entire_text_from_a_source += doc.page_content\n",
    "        chunks = text_splitter.create_documents([entire_text_from_a_source])\n",
    "        formatted_chunks = []\n",
    "        for chunk in chunks:\n",
    "            formatted_chunks.append(chunk.page_content)\n",
    "        temp = {\n",
    "            \"name\" : folder_name,\n",
    "            \"path\": f'./sources/{folder_name}',\n",
    "            \"chunks\": formatted_chunks,\n",
    "        }\n",
    "        raw_texts.append(temp)\n",
    "    return raw_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cffef20-d5b3-474c-995c-636f00c05cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:22:27.776139Z",
     "start_time": "2024-04-11T08:22:14.374332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brian_chau_06_01']\n"
     ]
    }
   ],
   "source": [
    "sources_dicts = load_text_from_sources_and_make_chunks(\"./sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d3166a2-6f4d-4b3d-8d48-79097c7afaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'brian_chau_06_01', 'path': './sources/brian_chau_06_01', 'chunks': ['Brian Chau RAW transcripts\\n\\nFor decades, stagnation has been the root cause of our greatest national problems: the loss of the American dream for the normal person and the spiteful, zero-sum thinking which dominates politics.\\n\\nArtificial intelligence is here to reverse this trend. It is here to give us back our future.\\n\\nAs Netscape co-founder Marc Andreesen puts it,\\n\\nTechnology is the glory of human ambition and achievement, the spearhead of progress, and the realization of our potential.\\n\\nProgress in AI is far from a done deal. It is still in a research and development phase, and only just beginning to be adopted by industry. We face a well-funded opposition that is willing and able to strangle AI in its crib or limit its availability to a small number of well-connected elites.\\n\\nPer venture capitalist Peter Thiel:\\n\\nThe future of technology is not pre-determined, and we must resist the temptation of technological utopianism — the notion that technology has a momentum or will of its own, that it will guarantee a more free future, and therefore that we can ignore the terrible arc of the political in our world.\\n\\nEscalating panic and reckless regulation around artificial intelligence will cause more harm than benefit. AFTF was founded to be the voice of ordinary users, builders, and founders, who want the basic freedom to use machine learning in their day to day lives.\\n\\nThe Coming Overreach\\n\\nHighly-motivated, well-funded activists have called for the implementation of dystopian measures to slow or stop the development of AI. They have proposed:\\n\\nCreating a global UN regulatory agency\\n\\nGovernment control of all AI development\\n\\nPartnering with the Chinese Communist Party\\n\\nDeveloping AI for China\\n\\nBombing “rogue” datacenters\\n\\nWhile these claims seem absurd, they come from established research centers, universities, and think tanks, with billions of dollars in backing and connections to the halls of power in Washington, London, Brussels, and elsewhere.\\n\\nEvery age has its techno-nihilists — those who focus exclusively on fears of technology, while ignoring its real benefits. There were people who wanted to stop\\xa0electricity,\\xa0the car,\\xa0the airplane,\\xa0vaccines, and\\xa0computers. Now, there are people who want to stop machine learning.\\n\\nEverything from the food we eat to your ability to read this sentence is because society has triumphed over fearmongers. But the story doesn’t always end this way. Dozens of technologies we hold dear are banned in more authoritarian societies. Even in the United States, transformative technologies such as nuclear energy are banned as a policy choice.\\n\\nGoverning Philosophy\\n\\nWe’ve seen this story across history – legacy players try to regulate their newer competition out of existence. We oppose all forms of\\xa0regulatory capture\\xa0– the establishment of new agencies, organizations, and licensing schemes to lock out competitors. We believe in regulation with a scalpel, not a hammer. What techno-pessimists have proposed is closer to regulation with a wrecking ball.\\n\\nLike every technology, there are negative uses of AI. What if someone uses AI for child pornography, identity theft, or fraud? This question has been asked throughout history. The same question can be asked for cars, electricity, the printing press, and certainly the internet.\\n\\nThroughout history, we have always solved this problem the same way: by banning the negative action, not the technology itself. Child pornography, identity theft, or fraud should be illegal, regardless of what technologies criminals use.\\n\\nWhat you are protected from today, you will be protected from in the future. What you are free to do today, you will be free to do faster and better tomorrow.\\n\\nWe believe in a positive vision for AI policy. Within the government itself, AI can be used to modernize public service, defend our country from cyberattacks, collect nuanced public feedback, enable new scientific discoveries, and accelerate the approval of infrastructure projects.\\n\\nIt’s far easier to be proactive than reactive. Once the crackdown begins, like with nuclear energy, it becomes much harder to undo.\\n\\nDefending your ability to use AI must start now.\\n\\nAI will be a process in every part of the economy\\n\\nPoliticians and regulators fundamentally misunderstand AI as a category.\\n\\nArtificial intelligence is a process, not an object. It is not oil, shovels, or images. It is more like mathematical equations, programming languages, or speech. Artificial intelligence is a set of statistical methods used to turn information and energy into output, like new images or emails. It is a wide research area that includes simple methods a single software engineer could manually write in one night and billion-dollar models such as OpenAI’s ChatGPT.\\n\\nRegulating or licensing “artificial intelligence” is like regulating statistics or economics. It would be the equivalent of assigning government bureaucrats to go into each and every company where AI is used (virtually all of them within decades) and micromanaging what software they can download, what mathematical formulas they could write down, or what emails they could send. In other words, it is completely infeasible.\\n\\nAs Jim Pethokoukis put it, “If AI is used across the whole economy, then having a department manage AI is like having a department manage the economy”.\\n\\nIn the past, a targeted, consequence-based approach has been extremely successful. If a person writes software in the C++ programming language to hack or defraud another, that person is punished. There is no collective punishment doled out to every C++ programmer or the inventors of the C++ language. If a person uses email to scam another, that person is punished, not the email server or protocol inventor.\\n\\nThe intuitive reaction to these statements is “of course, it would be devastating to punish programmers or email users as a whole”. It’s also the morally good approach in the tradition of American justice: punishing culpable individuals rather than collective groups.\\n\\nNational Security Needs a Strong Private Sector\\n\\nNationality security is impossible without technological supremacy. America is never more than a decade away from losing that technological race to our adversaries.\\n\\nThe American way is twofold:\\n\\nA public sector which understands the path to strength is through a free and open private sector\\n\\nA private sector with the civic responsibility to protect the safety of their countrymen\\n\\nWe work to encourage both these mindsets in lawmakers and founders alike.\\n\\nThroughout history, strategic technologies often have both military and civilian uses. We won the Second World War, the cold war, and countless other conflicts by accelerating these technologies, not restricting them.\\n\\nThese ‘dual-use’ technologies are ones we are already comfortable using in everyday life. A transmitter on a phone or laptop is little different than one on a remote bomb. The wires, microchips, and batteries are only slight variations of each other. Cars, planes, and ships all have vast uses in military conflict, but are all crucial to the way of life of every American.\\n\\nMachine learning comes in a long line of technologies that drastically improves civilian and military effectiveness. This is why policies restricting “dual-use” machine learning models will cost not just dollars, but American lives.\\n\\nFor centuries, free and open civilian research and entrepreneurship has enabled more efficient production, research, and leadership in the military. The accumulated knowledge, organizational processes, and practical training of American civilians gained from the private sector enables efficient and effective military development. This is why policy must encourage positive collaboation with developers, engineers, and entrepreneurs.\\n\\nAI is the Free Speech Issue of the Decade\\n\\nRecent scandals at Google shows that government policy is inflicting extreme political biases upon the most important technological developments in AI.\\n\\nFrom a\\xa0Pirate Wires exclusive:\\n\\n“Three entire models all kind of designed for adding diversity,” I asked one person close to the safety architecture. “It seems like that — diversity — is a huge, maybe even central part of the product. Like, in a way it is the product?”\\n\\n“Yes,” he said, “we spend probably half of our engineering hours on this.”\\n\\nGoogle’s Gemini paper\\xa0directly references\\xa0the Biden Executive Order on AI as motivating its content policies:\\n\\nExternal groups were selected based on their expertise across a range of domain areas, including those outlined within the White House Commitments, the U.S. Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence, and the Bletchley Declaration\\n\\nAI will go much further than social media. Firstly, it will be used in almost all social media. Free speech in social media will depend on free speech with AI. Secondly, it will be involved in economic processes far more wide-ranging than social media. It is a process used not just ubiquitously for communication, but for scientific research, manufacturing, logistics, defense, and much more.\\n\\nCreating an open and dynamic environment for AI not only involves preventing further damage, but curtailing existing destructive measures. This calls for a threefold approach to AI:\\n\\nEnsuing the open development of competitors\\n\\nCurtailing the weaponization of existing laws against heterodox AI companies\\n\\nDefending against laws which will give censorship powers to the executive branch\\n\\nWhy Open Source Is Vital', \"Machine learning is a classic example of upfront research and development costs. It can cost\\xa0tens of millions\\xa0of dollars in hardware alone to train a large language model, in this case LLAMA-2. Once this model is trained, it can be adapted for a variety of purposes. Incentivizing open-source development increases efficiency in training by reducing double-spending. It lowers the barrier of entry for talented engineers without independent wealth or institutional affiliation. By making software open to public scrutiny, it makes fixing security issues and\\xa0preventing unintended behavior\\xa0far easier. Consequently, proactively funding open-source organizations and incentivizing existing AI organizations to open-source models benefits everyone.\\n\\nCompetition and open source incentivize the creation of a plurality of ideologically diverse machine learning models. As\\xa0“Godfather of AI”\\xa0 Yann LeCun\\xa0put it,\\n\\nWe cannot afford those systems to come from a handful of companies on the West coast of the US. Those systems will constitute the repository of all human knowledge. And we cannot have that be controlled by a small number of people. It has to be diverse, for the same reason the press has to be diverse.\\n\\nAI Will Encourage Healthy Work\\n\\nConservatives often assume that automation will only eliminate jobs that give workers a sense of meaning. As the story goes, the blue-collar embodied work that made red America was automated or shipped overseas, and what replaced it was soul-destroying office jobs. Looking at the past twenty years, there’s reason to believe this story. Manufacturing has indeed declined. Office work has indeed exploded. There is a vitality of physical work that is missing, with many scrambling to replace it with weightlifting and supplements.\\n\\nExtend the history lesson to 100, 200, or 2000 years. Does this story still hold? The factory jobs conservatives miss were themselves the product of new technologies. They replaced some of the most unhealthy, self-destructive work in American history, such as deadly, poorly paid mining jobs. The 1950s, single-income family doesn’t just vanish if you look 30 years later, but also if you look 30 years prior.\\n\\nIt was enabled by new technologies: efficient construction of new homes, widespread adoption of cars enabling life in the suburbs, and the modern manufacturing systems which those jobs depended on. The same is true across centuries and millenia. Hobbes rightly described pre-civilizational life as “nasty, brutish and short”. Technology eliminated jobs deadlier than warzones in favor of the American way of life conservatives now reminisce over.\\n\\nArtificial intelligence will follow this broader historical trend. The last 20 years of automation were a historical anomaly.\\n\\nFor example, large language models such as ChatGPT are best at filling repetitive, routine paperwork. It takes away work that no one wants to do — tax filings, HR compliance, call centers, and other soulless corporate busywork. In the same way that technology brought about the work many conservatives recall fondly, the current wave of technology directly targets the work they excoriate. I would urge all conservatives not to let the post 1950s narrative get in the way of a future that is not only more prosperous, but more vitally human.\\n\\nCall to Action\\n\\nNow is the moment that AI is defined in the minds of both the public and of legislators. While the benefits of AI might be clear to those who understand it, the same cannot be said for everyone. Sensationalism and risk-aversion could easily lead to the most important technologies of this century being set back decades.\\n\\nAlliance for the Future is in a position to do something unique – set terms that are beneficial to the AI industry at large, not just to a few companies or interest groups. We seek to overcome this period of stagnation and bitter politics. The American people are hungry for a courageous, free, and optimistic approach to technology.\\n\\nNathan Lebenz pod\\n\\nNathan Lebez\\n\\nWhy don't you start off by just telling us about the alliance for the future?\\n\\nBrian Chau\\n\\nRight. So our tagline is informed AI optimism. I think that most people here in DC, so I moved to DC to start alliance for the future, and most people here in DC still have very low resolution images of AI. Their idea of AI policy, a lot of the time they think it's just chat GPT, they think it's very narrow, they don't understand the economic implications. And really, I think that many people can be convinced to be much more supportive of AI once they get that full picture. So that's really our mission here.\\n\\nNathan Lebez\\n\\nCould you give me a little bit more of your personal brand of optimism? Obviously, AI fashion myself as an AI scout, and I feel like it's sort of an interesting mix, because on the one hand, this mission that I've set out for myself to have the most comprehensive, up to date, wide angle view of AI that I can, is definitely touching on a very real need. But then, on the other hand, it's kind of becoming convergent with just all of society. And that also means that the project in some ways is fundamentally flawed. But with that breadth, I think when people talk about AI, and I imagine this is true in the circles that you're running in, where people are not primarily focused on it, they mean a lot of different things.\\n\\nNathan Lebez\\n\\nSo what's the next cut deeper of, like, what to be optimistic about, what not to be optimistic about, if there's anything not to be optimistic about in your mind.\\n\\nBrian Chau\\n\\nRight? Yeah, you're right about the term AI being this increasingly encompassing thing, and I do think there's this tendency to over narrow it, to only focus on LLms. If we're looking from a policy perspective, many of these policies apply not just to llms, but to other models trained with similar techniques. Right. So inference models, robotics, certainly many predictive models, if they're of sufficiently large size, and that's just the kind of regulatory environment. So that's not really my definition, but it's the kind of definition presented to me via working in policy. In terms of what I'm optimistic about, I mean, I think that it will make almost everyone's lives just much easier, and it will cause economic growth. It will cause downstream positive effects.\\n\\nBrian Chau\\n\\nThe typical kind of pro technology story, and I think that both the kind of near term concerns and the kind of long term concerns, I think that the long term concerns are really based on pretty typical mathematical fallacies, really. And then the short term concerns, some of them are quite legitimate, but the effect sizes are just outweighed by the positives that I think that there can be policy that is done to take those concerns seriously, something like cyber risks, something like deepfakes, but that in the long arc of things, the benefits to AI are just much greater, that there should be no question that it's worth developing more AI and maybe dealing with the downsides on a kind of case by case basis rather than any kind of research bans.\\n\\nBrian Chau\\n\\nNot that would be constitutional anyway, but this kind of obstruction that's becoming increasingly popular in DC.\\n\\nNathan Lebez\\n\\nSo maybe we'll take it from the top and work our way down. I mean, I'm someone who broadly has a very high level of uncertainty about what to expect. And I sometimes describe myself as an adoption accelerationist and hyper scaling pauser, because I do think that we're, without question right now, the AI's that we have are like way more useful than they are harmful. And I think I just gave a presentation to some business leaders today and to some students a couple days ago, and the point of those presentations is like, you are, you know, failing yourself if you're not really embracing this technology, trying to figure out how to, you know, make it work to your advantage.\\n\\nBrian Chau\\n\\nAbsolutely.\\n\\nNathan Lebez\\n\\nAnd so I'm very much with you know, on that. At the same time, I do feel like we are really playing with fire. I do feel like this technology is unlike other technologies. And with that in mind, I really try to understand it on its own terms. So I'd be interested to hear your case for why you think the long term risks are overblown, and then maybe we can reel back into the present and talk about kind of what you think. Sure.\\n\\nBrian Chau\\n\\nSo I sent you this article. It will be out@afuture.org research by next Monday, which I believe should be well before you release the episode. And this main argument is that let's actually talk about what the long term argument is here, because I think that people don't quite get it, especially here in DC. The argument tends to be that AI will become so powerful that is able to kill all of humanity, and a separate argument that it will want to kill all of humanity. But I'm mostly arguing against the idea that it will become so powerful that it will kill all of humanity. Would you agree that's a correct summary of the long term arguments?\\n\\nNathan Lebez\\n\\nCertainly. That's the biggest, you know, the biggest worry would be human extinction. And, you know, just to kind of ground the discussion in, like the opinion of folks in the field, I recently did an episode with Katya Grace from AI impacts where they did the survey of like, I think, 2700 published AI machine learning authors, people who had published in the top six conferences in the last couple of years. Half of the people in that survey result gave a five to 10% chance or higher of human extinction.\\n\\nBrian Chau\", \"I'm very familiar with AI impacts. They're funded by open Phil, they're funded by these people who already believe in the argument. And the survey in particular is an opt in survey. This is a very common tactic here in DC. I think you show that to someone in DC, they'll know what's up right away. And what's up right away is that if you have opt in surveys, especially if you title it something like opt in survey on whether AI will destroy all of humanity, then you'll have much more opt in from people who already have a strong ideological commitment rather than basically normies who think this is a waste of my time. And this is a very clear, very common political strategy, not just on AI, but this is something that every issue campaign does.\\n\\nBrian Chau\\n\\nAnd this is a level of literacy that exists in DC that I think DC people are actually quite competent in seeing past, but maybe less so for the public. I think that in general this idea, I think that in general, this idea that there's going to be active work, that, you know, the people who are working on this technology feel like they are actively contributing to the extinction of humanity. You really should not believe that. On its face. On its face. That I think seems ridiculous to most people and it should be ridiculous to most people because most people are not suicidal. Most people are not trying to kill themselves. Most people are not trying to kill all of humanity. And, you know, if you have these kind of selection effects, then maybe you're going to end up with a sample size of people.\\n\\nBrian Chau\\n\\nYou're going to end up with a sample of people who are skewed in that way. But most people in the industry are not like that.\\n\\nNathan Lebez\\n\\nI would just focus in on the, I mean, assuming we're not accused, we can accuse people of having whatever motivations, but I'm assuming we're not accusing them of faking the data. We still have some 1350, but you still have a real number of people, 1350 or so published in top conferences, machine learning researchers who are giving this non trivial chance to human extinction. So I think that's still something that we definitely have to grapple with. The field does take this question seriously. Most people think it's not the most likely thing, but they do give it a non trivial chance. Right. So, most people do not.\\n\\nBrian Chau\\n\\nSo even in this highly biased sample, even in this opt in sample, you only found half of people. So let's see.\\n\\nNathan Lebez\\n\\nYeah, so I'm, so give me the argument that I don't have to worry about it because I got 1000 plus machine learning researchers who are telling me that I do. I got others that are telling me that I don't. But if I'm, you know, now it.\\n\\nBrian Chau\\n\\nComes down to, okay, sure, this is, you know, this is a much smaller sample than, for example, like vaccine scientists who are anti vaxx. Right? So you always get these kind of ideological fringes. But, okay, let's actually take a look at the argument. So many cases, in many cases, in almost all cases, people who are famous in this kind of AI doom community believe in an idea called foom. This is their term, not mine. I'm not just calling it that to make fun of them. This is what they call it themselves, which is the idea that AI will become, radically more powerful in a matter of weeks, if not sooner, that there will be a kind of recursive self improvement. And what's very important here is that they have a very simplified model of these things where there's some kind of quantity called intelligence.\\n\\nBrian Chau\\n\\nIf you increase this quantity called intelligence, then it directly feeds back into being able to increase this quantity called intelligence even more. And that's how they get this argument of recursive self improvement. And it's very important to note, this is my main problem with this argument, is that it's a very non empirical argument. It is not something that is trying to use data from the real world. It is not something that is trying to fit to any parallel, and in fact, is a kind of monotheism. It relies on the belief that AI is a technology that is unlike anything else that has ever existed in reality. Now, there's a kind of soft version of this that says, oh, AI is going to exponentially self improve because it makes the job of AI scientists, human AI scientists, easier.\\n\\nBrian Chau\\n\\nAnd then there's a hard version of it where AI is doing AI research on its own. So let's address both of those in sequence. So, for the problem of soft AI self improvement, almost all scientific fields have the ability to self improve. You invent something like a microscope, you invent something like a particle accelerator, that is something that gives you the ability to do more science. And in fact, almost every successful company, almost every successful firm, and most successful research laboratories have to do this in some way. They're picking up competence, they're picking up additional skills, and they do have an increased ability to operate for some time. And then eventually they hit diminishing returns. They're diminishing returns in physics, diminishing returns in chemistry, diminishing returns in biology, so on and so forth, where the discoveries simply become harder to make.\\n\\nBrian Chau\\n\\nAnd this diminishing returns outweighs the kind of improved ability to do research. And when it comes to the hard version of AI, self improvement, first of all, while there is some evidence that AI is able to be used as a kind of automation tool for current AI researchers, there's no evidence that AI is remotely close to self directed research at this moment. It even struggles on kind of very basic upwork tasks. Even the most cutting edge startups do, and the nearest competitors are not even close to that. And there are also real life examples of these self improving processes. Like I said, they're called companies, and those companies are able to spend resources to improve themselves, hiring R and D, so on, and they are also very far from growing indefinitely.\\n\\nBrian Chau\\n\\nThey may be very productive, they may be very economically impactful, and I do believe AI will be very economically impactful. But we're not close to seeing world domination by one company, because as in all of these other areas, as in virtually or literally, actually every area of real life, there are diminishing returns. So the summary is that if you look at a broader pattern of either companies, research labs, research fields as a whole, there is this ubiquitous pattern. I don't need to hedge myself. It is literally ubiquitous pattern of diminishing returns.\\n\\nBrian Chau\\n\\nAnd if you model it that way, if you model kind of returns to intelligence, not as something that is essentially constant, so that if you get an exponential increase intelligence, you're just going to keep going, but rather something that's diminishing super exponentially, as it has in all of these other research areas, then this idea that this kind of self improvement will not run into these stumbling blocks becomes very clearly against the pattern of essentially all of scientific history.\\n\\nSpeaker 1\\n\\nWe'll continue our interview in a moment. After a word from our sponsors. If you don't already subscribe to Turpentine's industry leading newsletters like our new daily AI newsletter, emergent behavior, or media empires, you should. But that's not what I'm here to tell you about. The platform we use to power these newsletters is called Beehive, and it's excellent. First of all, it was started by the same early team who helped build Morning brew into a $75 million newsletter business, and they built behive to offer that same powerful functionality to anyone sending emails from essayists to business owners. The platform is beautiful, their text editor is intuitive, and they help you scale your audience with custom growth features. Behive has powerful tools to help you monetize your content. You can easily launch paid subscriptions or pursue an advertising model.\\n\\nSpeaker 1\\n\\nThe Beehive platform will even connect you to premium brands to sponsor your newsletter. Not only do we use them, but thousands of the top newsletters in the world also use them, like Milk Road, Blockworks, the Lindy newsletter, and so many more. Behives founder hooked upstream listeners with a sweet deal get 20% off for three months with code upstream. Visit beehive.com comma. That's b dash e dash h dash hiiv.com to get started hey all, Eric Thorenberg here. I'm hearing more and more that founders want to get profitable and do more with less, especially with engineering. Listen, I love your 30 year old ex Fang senior software engineer as much as the next guy, but honestly, I can't afford them anymore.\\n\\nSpeaker 1\\n\\nFounders everywhere are trying to turn to global talent, but boy, is it a hassle to do at scale from sourcing to interviewing to on the ground operations and management. That's why I teamed up with Sean Lenahan, who's been building engineering teams in Vietnam at a very high level for over five years. To help you access global engineering without the headache squad. Sean's new company takes care of sourcing, legal compliance and local HR for global talent, so you don't have to. With teams across Asia and South America, we can cover you no matter which time zone you operate in. Their engineers follow your process and use your tools. They work with React next js or your favorite front end frameworks, and on the back end, they're experts at Node Python, Java and anything under the sun.\\n\\nSpeaker 1\\n\\nFull disclosure, it's going to cost more than the random person you found on upwork. That doing 2 hours of work per week but billing you for 40. But you'll get premium quality at a fraction of the typical cost. Our engineers are vetted top 1% talent and actually working hard for you every day, increase your velocity without amping up. Burn head to choosesqua.com and mention turpentine to skip the waitlist.\\n\\nNathan Lebez\", \"So I would grant that, and I think most of the AI safety community would grant that. There's almost certainly some s shaped curve to the current AI paradigm. And it seems like most people think we're kind of in the steep part. And the question becomes like, how far does the steep part go before that leveling off happens? And it seems to me that you can make all the, you know, you can buy into all the arguments that you're making that this is not going to be an unbounded exponential, it's not going to go foom forever.\\n\\nNathan Lebez\\n\\nBut you could still believe, or you could still worry, and I'm, again, pretty radically uncertain about this, but you could still worry that the point at which the S curve starts to flatten off could be high enough that it could be something that's very hard for us to control. So what would you say to somebody who looks back at history and is just like, huh, in the grand scheme of history, it sure seems like what the great story of humanity is, basically that we popped up out of nowhere. We had cognitive abilities that other species didn't have. We used those cognitive abilities to come to dominate the earth and basically everything else on the earth aside from bacteria, but certainly all the other megafauna, they sort of live now at our pleasure.\\n\\nNathan Lebez\\n\\nAnd what's to say that this couldn't go the same way, except the new sharpest tool on the block, which maybe wouldn't necessarily have to be infinitely sharp or go foom forever, but could just hit a stay on that steep part of the S curve and flatten off far enough above us that it could just gradually out compete us? That's not like a five minute story, but it could be a story that in geological or evolutionary time is quite fast, because after all, isn't that what we did to all the other hominids and all the other megafauna?\\n\\nBrian Chau\\n\\nSo there's a version of the EA argument that the effective altruists, who are very closely related, there's a version of this argument of their philosophy of long termism, that if there's a problem that, say, has a 0.01% chance of happening in 200 years that is so disastrous that would cause the extinction of all humanity, that even if there's a small chance of it happening many generations from now, then it's worth worrying about because the kind of expected value is so great. That version of the argument I'm more sympathetic to, but I would say that the approach to that is a kind of wait and see. The main thing that I am arguing against in this article is an idea that's presented that we will get extremely discontinuous growth in a short time.\\n\\nBrian Chau\\n\\nAnd that's the reason why you would have to pause now instead of, say, pause at GPT ten or GPT 20. It could be the case that after basically repeated cycles of human development, of human driven research, that we reach levels of superintelligence. But I don't think that's something that's going to happen within, say, the next ten years.\\n\\nNathan Lebez\\n\\nOkay, well, I'm still kind of wondering, what do you say to the person who, fine, you think it's not going to happen in the next ten years? The leaders of the leading developers of the technology are saying that they think it's fairly likely to happen in the next ten years. The aggregate forecasts on sites like metaculous, for example, have it coming in the next ten years. We still have the thousand plus researchers to contend with. So why should I buy that hunch and not worry about it?\\n\\nBrian Chau\\n\\nI don't think we're quite talking about the same thing here. The mataculous forecast, for example, I believe is about AI that is able to automates 90% of economically valuable work, which is not the same as being able to take over all of humanity. Now, I have separate economic arguments for why I think that's unlikely related to the inelasticity of hardware demand, for example, diminishing returns in some of the techniques used to generate hardware improvements so far. There are kind of economic reasons why I think that those timelines are a bit short. But it's also important to distinguish here the difference between very economically valuable AI, which, while I think will still take longer, is still more of a kind of close something that it's more likely to see in my lifetime to something that's capable of taking over the entire world.\\n\\nBrian Chau\\n\\nI think that there's actually a huge gap between those things, but there's not.\\n\\nNathan Lebez\\n\\nNecessarily a huge gap between introducing something that will come to take over the entire world and where I guess this cat could get out of the bag. Llama three is about to be open sourced. Once it's out there, that will never come back. Now, I don't think llama three is going to take over the world. I do think it's going to do a lot of weird stuff and a lot of great stuff, but it is something that will never be put back in. We're not going to ever be in a COVID free world. We're never going to be in a llama three free world. It does seem that some stronger argument would need to be made that one need not worry about this, because even if it is a gradual process, you can hit the. It's like a gradual process.\\n\\nNathan Lebez\\n\\nThe sun's going to go supernova. It's going to happen. Fortunately, it's going to be a long time. But if we let some AI out into the world, that begins a process that we can't control.\\n\\nBrian Chau\\n\\nBut the larger these models are, and we can only assume, or we should assume that they will get larger then the more centralized inference cost there is. Right? So the larger these models get, the more a kind of centralized entity is required to run them and it becomes very easy to. It becomes a very easy target. This is something that's widely agreed upon, both among people who oppose this kind of regulation, but also among many eas who do support this regulation. They have their papers which agree with me. I can link you one after the show, which basically says hardware is the easiest thing to regulate because it is something that you require large amounts of, it becomes highly centralized. The energy demand for these data centers is up in question as well.\\n\\nBrian Chau\\n\\nSo I think that as you see the pace of development grow, it will actually become easier to shut down.\\n\\nNathan Lebez\\n\\nBroadly speaking. I think I agree with that. I guess I'm trying to. I think, I mean, there's always a lot of different questions and it's very easy to be kind of addressing different things. As you said, it's always a risk. It seems to me that, yes, the data centers are going to have to get bigger. An h 100 runs at 700 watts. That's not that much. But you do need a handful of H run a llama 400 B, presumably it's nothing, but it's also not like you don't need a nuclear plant to do that. You can, for current models, not quite yet. You could definitely run a small cluster on a local generator to do even the biggest models today. The energy is not the main concern.\\n\\nNathan Lebez\\n\\nIf you want to power it at civilizational scale, you're going to be using a lot of energy. But my.\\n\\nBrian Chau\\n\\nYeah, yeah, I'm talking about quite a bit into the future here. But I guess my, you know, we.\\n\\nNathan Lebez\\n\\nCan, I think, want to come back soon into kind of nearer term, like what do you think the rules should be today? But I don't hear anything from, really, anyone ever, that I find compelling in terms of why I shouldn't worry about the big picture risk. I just look at macro history. I see we humans dominate the planet because of our superior cognitive abilities, and we've driven lots of things to extinction. And I don't see any law of nature that says that couldn't happen to us. And in fact, I think it's actually quite a misconception that this is sort of a abrahamic God notion. I would frame it for myself as like, it's actually the lack of an abrahamic God, that there is no abrahamic God to protect us. And that's why we very well could go extinct.\\n\\nNathan Lebez\\n\\nAnd I do wonder to what degree we are running that risk over even the near term. We might think, yeah, the stuff we're putting out in the near term isn't that crazy. But once it's out there, it's free to mutate. In the wild, it's free to recombine. And yes, it's not going to do that on its own right now, but it's something we can't put back. And I do feel like we are climbing a ramp of risk in parallel with climbing the ramp of utility, and I think we should at least be clear eyed about that. But what I'm not hearing is an argument that there's really nothing to worry about. What I'm hearing is, well, it'll take a while, and I would say yes, but we might be tipping over into something we can't take back sooner than we realize.\\n\\nNathan Lebez\\n\\nAnd that's really the problem, is, when have we crossed an event horizon that we can't escape from even not knowing that we did, or even though it may take decades or who knows how long it could take. But it seems like we just don't have a good enough model of what we're even dealing with to dismiss all worry.\\n\\nBrian Chau\\n\\nYeah, I mean, I'm not trying to dismiss all worry at all. Certainly I think that we've come to a bit of a compromise here where we're kind of looking at the quantitative issue. Right. Like I said, if it comes to the radical uncertainty argument, I mean, I want to put dampers on the near term version of that, but the long term version of that, with many generations of human development, yeah, that could be something that happens. But I think the idea that this is something that becomes a runaway process in the near term is something that I push against. And I also think that certainly in the kind of near and medium term, there's much more of a comparative advantage thing going on here.\\n\\nBrian Chau\", \"There's much more of a specialization where there's going to be tasks that AI is better at, there's going to be tasks that humans are better at. AI's are already better at chess, for example. And that is something that we can just live with, that things are not as correlated across domains as people may expect. That there are many technologies in the past, say tractors or other farming equipment, that has, in today's or in the context of that time, automated most of human labor, and people find other jobs. I'm generally optimistic about that. I do think that there will be a time to reevaluate that.\\n\\nBrian Chau\\n\\nThis is if we are not stopped by physical constraints, if we are not stopped by hardware constraints, which I believe there's some reason to think that we are, if we're not stopped by diminishing returns in science, in the ability to improve these algorithms, if it does keep just growing, then there will be a point to reevaluate. I think we just disagree quantitatively on how early that will be, so we can jump in and welcome Eric as well. We can jump into the kind of affirmative policy as well, if you want.\\n\\nNathan Lebez\\n\\nYeah, I think that sounds good. I think I was just kind of mulling over your last comment, and I appreciate the notion that there could be a point to reevaluate. I mean, how would one, maybe one more question on this before going back to the nearer term, more practical stuff. How would one determine when that point happens? I don't really think we're talking about truly quantitative things. We're all kind of guessing. How would you make that decision?\\n\\nBrian Chau\\n\\nNo, there are quantitative metrics of this. For example, ability to autonomously. Yes, autonomously perform economically valuable tasks. That's a real indicator, right? You can look at some of these evaluations, swe bench, for example, software engineering bench, for example, which I think, and people kind of criticize the methodology for this. There was some startup cognition that released a tool that claimed to be able to do 14% of these tasks. There have been some criticism since then, but basically in specific domains, once you get really into the industry, there are ways to measure this, and you can imagine aggregating this in order to create a picture of all economically valuable tasks. So I think that as these things get closer, let's say, for example, we find one kind of broad field where there is autonomy in that field.\\n\\nBrian Chau\\n\\nAnd I do think it will be the case that we get one of these fields much faster than the others say art, for example, say visual art, for example, that if we get to the point where Dolly is so good that it can create these really precise, completely human displacing, changes across the entire sector. Right. Then, then that'll be something that's very clear to measure, and it'll show up in the kind of economic statistics. So I, I'm not sure if I agree with that assumption that it will be difficult to measure. I think it's kind of where I do agree with you is I think that can be hard to conceptualize, today. Right.\\n\\nBrian Chau\\n\\nBut I think that once we start to get examples of it replacing some economically valuable work, which I do think you'll at least replace some, then we'll be able to measure that, and then we'll be able to draw comparisons to measure similar things that might potentially happen in the future in other fields. This kind of typical economic process.\\n\\nNathan Lebez\\n\\nWhat do you think of things like the anthropic responsible scaling policy?\\n\\nBrian Chau\\n\\nI mean, to the extent that they're doing additional research, I mean, like, that's their prerogative, basically, right? To the extent that they're doing this additional research before scaling. I mean, it seems like their product, cloud three, I've used opus. It seems great. It seems like a good product. It seems like that their kind of interpretability experiments have not really detracted from the product. They might have even improved it with things like constitutional AI. So, yeah, I really don't want to seem antagonistic kind of by default to a lot of these fields. I think that a lot of the time, or at least some of the time, I don't know what the exact ratio is, but some of the time, these kind of mechanistic interpretability experiments, these kind of super alignment experiments for the OpenAI version are just interesting scientific results.\\n\\nBrian Chau\\n\\nAnd is that the optimal allocation of capital? I don't know. From a kind of theoretic economic standpoint, probably not, but it's positive. Some science, it's getting more information. It's things that are basically useful from a research perspective. So I have no reason to. I'm very positive on it.\\n\\nNathan Lebez\\n\\nSo I guess this maybe is a good transition to the newer term regulation, because the way they frame their stages of responsible scaling, they have like four risk levels where they've tried to get out in front and say, okay, these are the things that we think might be a real problem in the future. And folks have probably heard, of course, there's cybersecurity risks and there's potentially biotechnology risks and there's self replication is one of the other things that they're really looking for and trying to determine like can the models do these various things? They basically say if we can detect that they can do these things and we don't have a way to present it then, or to prevent it, I should say then we have to stop further scaling and essentially pause the mainline development track.\\n\\nNathan Lebez\\n\\nNow I wouldn't put any words into Anthropic's mouth, but this sure reads to me like a proposal for regulation broadly. And I wonder like what your reaction would be to that sort of framework becoming not just something that like one company is volunteering to hold itself to, but the sort of regulatory regime that would govern all such research. If you, if your system can do this and you can't prevent it, then you have to stop your scaling. Does that seem like a reasonable place for us to go?\\n\\nBrian Chau\\n\\nYeah. The problem with answering this, I'll put it this way, right. The idea that if your program does X and we can measure that it predictably does X and we can't prevent it, I think that's a good policy for some version of X. Right. The question is what that version of X is. And there are various laws being passed that basically have varying versions of X. Right. So in terms of anthropics specific policies. Yeah. I was just scrolling through the document and I'm not completely sure if they say what the evals are. There's this, there's this link to the arc evals homepage and I have not read what all their evaluations are. So I can't say because I don't know what X quite is yet. Right.\\n\\nBrian Chau\\n\\nBut I certainly think that there's some version of X where that's true, many versions of X where that's not true. And I think that in DC, and this is kind of moving away from some of the long term risks and more so looking at short term risks, there's a lot of X's that people are trying to insert that would not be productive. And I can go more on that if you want.\\n\\nNathan Lebez\\n\\nYeah, I'm really interested to hear what you think the rules should be. And I, by the way, would totally agree that there are many values for x that would be ultimately counterproductive in terms of just sacrificing too much upside from AI at too little benefit. Certainly I am not one who wants to police speech, some sort offensive speech would not be a good value for x. In my mind. I would. I'm not sure. I don't have a specific place exactly where I would draw the line, but I would definitely establish the common ground that policing speech is not the sort of thing that I would want to get too aggressive about.\\n\\nBrian Chau\\n\\nYeah. So I'm very excited about the potential for modernization. We're going to release a, actually, I don't know when this will be researched. So it may or may not be out by then, but we'll have a plan for kind of the broad strokes modernization of government. And that's something I'm very excited for, that this is an opportunity, that this is a kind of reset button to make many of the processes in our ordinary life much more efficient, both on a kind of industry level. But I think, and here's where I think decision or kind of organized political effort is more important also on the public sector side, because this is our tagline for the proposal, is that government cannot catch up by trying to stop its competitors. It has to be able to modernize on its own as well.\\n\\nBrian Chau\\n\\nCertainly I don't think the government can substantially slow down China, at least not indefinitely. And so we're in a situation where I think many of the pain points that people have when dealing with government, both people, ordinary citizens and people working with government or directly working contracting with government do have a lot of pain points that are directly addressed by AI. That's something we're going to be encouraging. We're going to take a look at specific office of management of the budget rules. I think that some of the current directives that were passed very recently are actively obstructive in the government's own adoption of AI, which I think is exactly the wrong direction that the government should be coordinating with developers to adopt more AI and certainly current levels of AI in much safer way.\\n\\nBrian Chau\\n\\nAnd we'll have more detailed policies about how to adopt that. I'm not sure, I'm not sure what level of depth you want, what kind of level of analysis.\\n\\nNathan Lebez\", \"Yeah, I think we could jam on a vision there that we would largely share certainly in the near term in terms of government process reform. Fun fact about me, I live in Detroit, Michigan, and I once offered a friend of a friend who works at the city government to just on a volunteer basis to create a little program to help them review case files that they have for people that are sort of receiving some amount of help from the city but potentially qualify for different benefits or programs or whatever. They've got a stack of case files, and as you'd imagine, they're not getting through them all, certainly not asking all the questions that you could ask if you brought an AI system to bear. I'm still waiting for my phone to ring back on that opportunity.\\n\\nNathan Lebez\\n\\nAnd I would agree that, like, yeah, I mean, anything we could do to spur what the great Zvi Mashwitz calls the mundane utility and just bringing day to day convenience and efficiency to the life of the average citizen, that makes total sense to me. Notably, I don't think we need to scale up lots more generations of AI to make tremendous progress there. I would say, like, GPT four would take us very far, and certainly a GPT five worth its name would take us even farther, very quickly and easily. You probably don't need a GPT six to make a government services regime just way better. But I guess I'm most interested in maybe going back to what do you think the rules should be today? We basically have almost no AI specific rules.\\n\\nNathan Lebez\\n\\nWe have just the general background rules of what's a crime and what's not a crime. And the law is largely silent on AI. We have the executive order, which you can elaborate on, but my general understanding is that it had one big rule, which is you need to notify the government if you are doing a large scale training run at ten to the 26 flops or more, and otherwise sets up a bunch of initial reporting and fact finding sort of motions going. But we still seem to have this question very much unanswered. Like, do we want to have AI specific rules, and if so, what should they be?\\n\\nBrian Chau\\n\\nSo I think that the right story to tell is to start with the executive order. I think that if you're worried about existential risk, you're absolutely right that the ten to the 26 is the kind of the highlight for you. But if you're worried about the kind of economic damages, if you're worried about the political consequences, there are implementations throughout the Biden executive order that should be very worrying to you. I'm not sure how high of a priority speech is, for example, but one example is the directives for diversity, equity and inclusion.\\n\\nBrian Chau\\n\\nOf course, this is a very and only increasingly controversial topic, but this is something where there is increasing motivation that is kind of distributed through the executive order into individual agencies, such as national telecommunications and the NTIA, the national telecommunications and Information Administration, or the Office of Management of the Budget, which I talked briefly about already, or the FTC that create increasing just barriers to entry to new companies, or at least threaten to with reports or recommended actions that they've developed so far. And someone who is completely committed to the existential risk argument would say, okay, these problems are just a rounding error. They don't actually stop the kind of AI that will kill everyone. But I think for most people, this is something that shapes, you know, shapes their political lives.\\n\\nBrian Chau\\n\\nCertainly for many startups and, you know, future customers of those startups, this is something that shapes the kind of financial viability of their company, that they can't necessarily pay these kinds of compliance costs that would be necessary if these directives became law. So I think from a short term perspective, there is a lot in the Biden executive order other than just the ten to the 26. So how do we carry on? The story is, I think that no matter what, this is just a political reality. I sympathize a lot with people who say, you shouldn't be taking this approach to AI, you shouldn't be so pessimistic, you shouldn't prematurely circumcise the kind of range of research before this is really even a developed technology. But the political reality is that there's a lot of motivation in DC to do something about it.\\n\\nBrian Chau\\n\\nAnd what that something is right now is up in the air. So our approach is to say here is a kind of clear and minimally damaging way to satisfy some of your demands. So our approach in that sense is to identify a single point of intervention. So why is that important? A lot of, I'm sure you know this, Nathan, a lot of AI companies work specifically in one part of a kind of development pipeline, right? You start with hardware and data and you assemble that with infrastructure. You have the initial pre training phase, which is what a lot of these big foundation model companies do. You have various startups that spin off that take that product and spin off of it.\\n\\nBrian Chau\\n\\nA fine tuned model that essentially uses these less intensive training techniques to produce a more specialized model for a given task or a given tone, or some of these more specific concerns. And then you have another layer of scaffolding, of prompt engineering, of customizing. And you see this across not just language models, which were the main subject of what were talking about already, but in models in biology and models in logistics, so on and so forth. So you have this great diversity, true diversity, of startups, and many of them would be impacted by, say, regulation that is intended to target identity fraud or intended to target deepfakes, so on and so forth. Because, for example, the NTIA released a report right now that I think the exact quote is to ensure accountability across the value chain. Now, what does that mean?\\n\\nBrian Chau\\n\\nIt means that not just the end product distributor, the kind of person who would produce an AI model that someone could actually use to possibly commit identity fraud. Not just that person, but people throughout that development process who have really no ability to either prevent or stop or actively encourage a model to do this, as long as they don't have control over basically the final product, the last stage in terms of customization, all these people in between are needlessly affected by this policy. It would do exactly the same thing if it were only restricted to the single point of intervention.\\n\\nBrian Chau\\n\\nThis point of intervention right before basically getting to the customer where you're doing some level of specialization and focusing on that specific kind of focusing on that single point of intervention, we argue, is going to be just broadly, much more effective way, and notably not just more effective for the companies. Not just saving the companies a lot of money, but also saving the regulatory agencies themselves a bunch of money. Because many of these slow moving regulatory agencies, if you can cut down their burden by an order of magnitude, they'll actually be able to do more due diligence. They'll either spend less money or with the same amount of money, can review things ten times more quickly.\\n\\nNathan Lebez\\n\\nSo if I understand correctly, this amounts to regulate the app layer. It would be like the shortest version of the.\\n\\nBrian Chau\\n\\nThat's a fair summary.\\n\\nNathan Lebez\\n\\nSo it would say, if you are, let's take a scenario that I'm working on a project right now called Red Teaming in public, where some volunteers and I are going out and testing a bunch of already deployed, available products just to see are there any guardrails in place? What will happen? Pretty interesting findings. I won't spoil them too much now.\\n\\nBrian Chau\\n\\nI'm sure there will be an exciting episode.\\n\\nNathan Lebez\\n\\nThere will definitely be some theatrical examples. We're focusing, for starters on calling agents. These are apps where you give it a prompt and it calls someone else to have a conversation with them. Then coding agents, which are like, you tell it kind of what you want and it codes stuff, but it also executes that code and then also like kind of deep fake generators, which is a little bit of a different era or different area, as you certainly, you're correct to say. There's like a lot of layers in the stack that go into something like that. If I am a user and I prompt a product to call some person and spearphish them for their mother's maiden name or whatever, and it does that, and that person is harmed as the user who's doing the crime, I'm obviously responsible.\\n\\nNathan Lebez\\n\\nThe app that made the calling agent, you're saying would be responsible.\\n\\nBrian Chau\\n\\nNot necessarily. So in the specific case of identity fraud, I do think the responsibility should bear with the person who is actually committing the crime. You see this widely precedented with software. There is automated emailing software, there's automated phone software even now or even in the pre AI era. And typically what the structure of that would be is that if you commit the crime, you do the time. And people who are producing basically neutral tools do not. Because they're producing neutral tools.\\n\\nNathan Lebez\\n\\nYes, this is where I think we're going to be in some very unfamiliar territory very soon because the tools, I wouldn't necessarily call them neutral, I would call them dual use. And I would say that they're also not inert. You know, all tools through history had this sort of property of not doing anything until you do something with them, whereas the new crop of tools are also being actively developed and marketed as agents. So the idea is not now that you are going to pick up this calling device like the traditional phone and talk through it yourself. In which case, sure, I would say the phone company should not be responsible for me calling and committing fraud over the phone.\", \"Nathan Lebez\\n\\nBut if I am the developer of a calling agent, something that can take a short string of instructions and then go make, you know, not just potentially one call, but calls at scale, targeting all sorts of people to do something where the actual conversation is happening between the AI and the other, you know, the unsuspecting person on the other end of the phone. It seems to me that we do have to have some standards in place for those kinds of companies. We can't just say, oh, you know, yeah, well, hey, you have no responsibility because somebody came in and put a bad prompt like you have to do something there, right? I mean, I would say there's got to be some rule to prevent or to make it clear to the companies that you can't just have no precautions.\\n\\nNathan Lebez\\n\\nI also think it's a fine line to say, like, you know, what's a reasonable amount of precautions? We have all these sort of legal standards historically right around, did you do what is sort of best practice in your industry or not? Like, negligence is not always super crisp or super binary, but it doesn't seem like we could say about these agentic things. And again, they're being marketed as agents. It doesn't seem like we can say that the companies developing them have no responsibility for what they do.\\n\\nBrian Chau\\n\\nYeah, I have a lot of empathy for that kind of position because I do think there is this broader sentiment, and you're right. That it's reflected in marketing, that this is sort of an extra, extra personal extension. Right. If you think of a kind of email list as a kind of intrapersonal extension, this is actually something that I've written, not really a research paper, but just kind of a statement in op ed about where I do think there's this fine line between intrapersonal, like within yourself, an extension of your own kind of thinking and things where you're being interactive with other people, or these things are kind of interactive with other people outside of yourself. Right. So I understand that position there. There's certainly a trade off of it. Right.\\n\\nBrian Chau\\n\\nLike I said, the political reality is the case where almost certainly I think that there are more people who basically take your position. But, yeah, I would say that, I mean, there's what is politics is the art of the possible. There's what I personally believe, which is a kind of more optimistic and more experimental approach. And then there is what's politically likely. So I would say that this is something where there's a lot of nuance on where it very well may be the case that there's regulation passed that basically agrees with you and has some kind of accountability layer to the company itself. And from that perspective, I would say that it's better if it's a kind of narrow accountability, whereas as opposed to a kind of widely distributed one.\\n\\nNathan Lebez\\n\\nYeah. This is one of the things we plan to explore with this project is just to what degree are there basic precautions, guardrails, system checks in place to prevent this sort of thing? And to what degree is it just a total wide open west? I don't feel like I have the answer at this point as to what the standard should be exactly. But I would definitely say if you have taken no precautions and you've put something like a calling agent out in the world, then I think you should have some responsibility. If you have put precautions in place but somebody manages to come through with in some intricate jailbreak or hack or whatever and then use the system in a way that you did try to prevent, but your defenses ultimately failed, then I think that's much more sympathetic to those companies.\\n\\nNathan Lebez\\n\\nAnd certainly with some amount of effort in today's world, I would say they should be able to say, look, we did our best, and this person is really the one that's, like, clearly at fault and look at all the defenses that they worked around in order to do this with our tool.\\n\\nBrian Chau\\n\\nYeah. And I would say that kind of philosophy makes sense. We may run into problems is that there's this well known phenomenon called the alignment tax, which I think actually comes from many of the people doing alignments, where they find that these techniques, such as RLHF, reduce the performance of other tasks other than the thing that they're targeting. So there may be kind of additional performance costs to this that are not easily solvable, and there are kind of caricature versions of this. I'm not saying that this is the inevitable consequence of what you're suggesting, but there are kind of caricature versions of this, like Google Gemini, of course, that also goes into another ball of speech restrictions that you don't necessarily agree with.\\n\\nBrian Chau\\n\\nBut the kind of more general point is that there are unexpected consequences to some of these restrictions, that there are trade offs at play. And like I said, this is something that can be thought about quantitatively, where you can figure out what you can draw the line, where the kind of science, the kind of state of the art matters, how much protection can you get with certain other amounts of trade offs. There's obviously some kind of cost benefit ratio there that makes sense. I just worry that from a kind of social political standpoints, that these judgments are not the ones being made. I know many of the people, including people who are kind of opposed to me, who are more on the EA side, who do actually think about this every day, who think about the trade offs at play.\\n\\nBrian Chau\\n\\nBut I would say that when that gets distilled to a regulatory level, either at the level of regulatory agencies or to the level of elected representatives, that it becomes much fuzzier. It becomes much less a question of trade offs and much more a question of kind of positioning. And that's where it gets very dangerous. But I think that if were running the NTIA, we could probably get to a point of agreement where we agree on basically 95% of things, and the remaining 5% are actually not so determinative. So there's a kind of theoretical world where this is the case, but in practice, we might be quite far from that world.\\n\\nNathan Lebez\\n\\nWell, I'm available if anybody wants to put us in charge of the agency. Yeah, this is maybe not a natural final question, but it's the one that's tip of my tongue at the moment, is how to think about open sourcing of models in the current regulatory regime and whether or not there should be some new rules there as well. It seems like if I was going to make, if I had to look at the current law and say, does any of this apply to open sourcing? I might apply pollution law as the most obvious thing where it's like you're just putting this thing out into the commons. In this case, it does have positive effects, but it also will have negative effects. And should you be responsible for that? Is it appropriate for companies to be doing that?\\n\\nNathan Lebez\\n\\nAnd how does that change as the power of the things that they're open sourcing advances? To be clear, again, I don't think llama three is going to be the end of the world, but I do think we are going to see a bunch of pretty obviously directly harmful things coming from people doing sort of malicious fine tunings of llama three, starting with the north korean government and working your way down to all sorts of just bad actors, it seems. I mean, I don't want to apply pollution law to that sort of thing. It seems like we probably need some new rules. But how do you think about open sourcing and what if any sort of rules, standards, expectations should apply there?\\n\\nBrian Chau\\n\\nRight? So I'm hugely supportive of open source. This is where I think you just need to sit down and do a cost benefit analysis and. Yeah, I mean, I'm not sure I need to convince you of this, that certainly with current technologies it's overwhelmingly positive. Once again, I would say that, you know, the punishments should be concentrated on people who actually commit the crimes. There are currently penalties for impersonation, for example, unlawful use of likeness, and there can be mechanisms to make that penalty greater for, if you use AI, that kind of fit within the current legal frameworks. I would say that for the publishing of open source code itself, not necessarily any particular use of it, but just the publishing of it is very likely protected under the First Amendment under, I think the president is Bernstein. Software is regulated as speech.\\n\\nBrian Chau\\n\\nKind of all of the same protections and exemptions apply to speech as applied to the software. Certainly this is true of data, and then hardware is where it becomes more, where there may be more kind of regulatory exposure. But that's not too related to what we're talking about.\\n\\nNathan Lebez\\n\\nYeah, I wonder.\\n\\nBrian Chau\\n\\nI would agree.\\n\\nNathan Lebez\\n\\nYou don't have to convince me. I think the benefits of even the AI safety community broadly has, I think, concluded the consensus even among the most ex risk concerned, is that the open sourcing of Lama two was unnet a good thing, if only because they're doing.\\n\\nBrian Chau\\n\\nTons of interpretability research on it. Exactly. Yeah.\\n\\nNathan Lebez\\n\\nSo let me try to blur the line one more, because here I really am like, man, I don't have the answer, but I do think we are breaking current paradigms remarkably quickly. You may have seen this recent model called Evo that came out of the Ark Institute. And essentially what it is a language model, but trained entirely on DNA sequences. So it's a 7 billion parameter model trained on 300 billion base pairs of mostly, like, bacteria DNA you can go together AI, I just did this, create an account. You could put in a couple at GCS, whatever, and then just hit go and it'll just generate you sequences.\\n\\nNathan Lebez\", \"And what I think is really interesting about that is it's like, there's been a lot of talk about biorisk, and sort of right now, it's like, well, the language models, they may be a little bit more helpful than Google, but they're not, like, that much more helpful than Google. But then you've got these other things where it's like the same technology but trained on a different data set is now actually just generating you genomes, and that is like, okay, that's way more helpful than Google if you want to. And again, it's all dual use, right? So it could be good, it could be bad, depending on what you're trying to do. It doesn't feel like speech to me. Generating DNA sequences doesn't feel like the sort of thing that the framers had in mind. You know, they obviously didn't even know about DNA.\\n\\nNathan Lebez\\n\\nAnd so I'm like, again, if somebody is going to make something like this and just drop it into the open, you know, in a way that is an irreversible decision, is that the kind of thing that we should just be like, well, we got a first amendment, so, you know, I guess that means we can't do anything about people just putting these sort of whole genome generators out there. This one is early. I don't think it's going to be the end of the world either. But you can imagine a couple generations, it's already superhuman in this domain. You can imagine a couple of generations from now, things could get really weird. It seems like we do need some new paradigms to govern these fundamentally paradigm breaking technologies.\\n\\nBrian Chau\\n\\nRight? So this is something that a friend of mine, Dean Ball, has written on. I think the best way to think of it is a kind of cost benefit analysis or a kind of relative risk analysis, where you're asking the question of how many more times more difficult it is to go from knowledge to action compared to going from nothing to knowledge. So maybe there's an improvement in terms of going from nothing to knowledge in terms of viruses here, maybe it's somewhat more convenient than Google. But, for example, the smallpox genome is just available online. And that's something that would be very destructive right now. Right now. I think that the main bottleneck here is overwhelmingly in terms of acting. Synthesizing a virus is much more difficult than obtaining the genome. In general, it's a kind of action problem, not a knowledge problem.\\n\\nBrian Chau\\n\\nAnd if you change the, if you change it so that it's, it, if you change it so that, you know, it becomes even easier to solve the knowledge problem, it's maybe, you know, 100 times easier to solve the knowledge problem, but it's, basically still as difficult to do the, to solve the action problem then I don't think there's any kind of substantive change in the number of bio attacks. And there's pretty active research into this on a policy level in terms of not just biorisk but kind of nuclear risk. Does this increase the chance of Iran doing nuclear research? This is something that there's a lot of work on.\\n\\nBrian Chau\\n\\nSo I would say that, yeah, I actually maybe worry more in the other direction, that there is so much work on it that there's a risk of going too far and doing something like a new TSA that is much more dangerous. But I think that there are approaches here that are mostly targeted at making the action problem even harder, which I think is actually the correct approach.\\n\\nNathan Lebez\\n\\nGDPR for AI is what I say when I want to strike fear into the heart of the AI app developer.\\n\\nBrian Chau\\n\\nYeah, that's just disastrous. GDPR itself is disastrous. I had an old article in pirate wires about this.\\n\\nNathan Lebez\\n\\nI wouldn't say it's disastrous, it's really annoying. But it's like, I've clicked that button a lot of times.\\n\\nBrian Chau\\n\\nBut my, sure, it's not causing human extinction, but it's very economically damaging to the european startup environments. But yeah. Anyways, it was great being here. This is a fun interview and I'm glad to have some of those responses on record.\\n\\nhttps://www.fromthenew.world/p/three-short-arguments-in-ai-policy\\n\\nALLIANCE FOR THE FUTURE\\n\\nThree Short Arguments in AI Policy\\n\\nThe Case For Open Source\\n\\nMachine learning is a classic example of upfront research and development costs. It can cost\\xa0tens of millions\\xa0of dollars in hardware alone to train a large language model, in this case LLAMA-2. Once this model is trained, it can be adapted for a variety of purposes. Incentivizing open-source development increases efficiency in training by reducing double-spending. It lowers the barrier of entry for talented engineers without independent wealth or institutional affiliation. By making software open to public scrutiny, it makes fixing security issues and\\xa0preventing unintended behavior\\xa0far easier. Consequently, proactively funding open-source organizations and incentivizing existing AI organizations to open-source models benefits everyone.\\n\\nThe Current Market Is Neither Free Nor Efficient\\n\\nRecent scandals at Google shows that outdated government policy is actively hindering the most important technological developments in AI.\\n\\nFrom the New World is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\\n\\nSubscribed\\n\\nFrom a\\xa0Pirate Wires exclusive:\\n\\n“Three entire models all kind of designed for adding diversity,” I asked one person close to the safety architecture. “It seems like that — diversity — is a huge, maybe even central part of the product. Like, in a way it is the product?”\\n\\n“Yes,” he said, “we spend probably half of our engineering hours on this.”\\n\\nGoogle’s Gemini paper\\xa0directly references\\xa0the Biden Executive Order on AI as motivating its content policies:\\n\\nExternal groups were selected based on their expertise across a range of domain areas, including those outlined within the White House Commitments, the U.S. Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence, and the Bletchley Declaration\\n\\nCreating an open and dynamic environment for AI not only involves preventing further damage, but curtailing existing destructive measures.\\n\\nEverything is a ‘Dual-Use’ Model\\n\\nSecuring our national defense require encouraging the development of technologies with both military and civilian uses. These ‘dual-use’ technologies are ones we are already comfortable using in everyday life. A transmitter on a phone or laptop is little different than one on a remote bomb. The wires, microchips, and batteries are only slight variations of each other. The same nitroglycerine can be used as explosives or as life-saving heart medication. Cars, planes, and ships all have vast uses in military conflict, but are all crucial to the way of life of every American.\\n\\nMachine learning comes in a long line of technologies that drastically improves civilian and military effectiveness. This is why policies restricting “dual-use” machine learning models are self-destructive economically and strategically. Obviously, prohibiting the civilian use of electronics, medication, or cars because they have military uses would be economically disastrous. The free and open development of machine learning technology is crucial not only to the private sector applications of machine learning, but to the national security and competitiveness of the United States military. For centuries, free and open civilian research and entrepreneurship has enabled more efficient production, research, and leadership in the military. The accumulated knowledge, organizational processes, and practical training of American civilians gained from the private sector enables efficient and effective military development. Consequently, policies which constrict and alienate private sector research directly harms national security.\\n\\nTranscript of EP 200 – Brian Chau on AI Pluralism\\n\\nJim: This is Jim Rutt and this is the Jim Rutt Show. Today’s guest is Brian Chau. He writes independently on the American bureaucracy, political theory, and AI. His political philosophy can be summed up as see the world as it is, not as you wish it to be. Everything else is application. I gotta say goddamn right to that. I like that. You know, it’s neither looking through the world with rose colored glasses, nor assuming everything is shit. And I must say, neither of those two lenses do I find particularly useful.\\n\\nSo anyway, welcome back to Brian Chau to the Jim Rutt Show.\\n\\nBrian: Great to be here.\\n\\nJim: Yeah, I think we’ll have another good conversation. Also worth pointing out, Brian’s got a couple of interesting media thingies going. He has a newsletter at ai-pluralism-newsletter slash slash front, the new world, blah, blah. As usual, the links will be on the episode page at JimRuttShow.com.\\n\\nBrian: Yeah, and that one, there’s a much shorter link just pluralism.ai and you’ll jump right to it.\\n\\nJim: Pluralism.ai. I like that. And he also has a from the new world podcast with lots of interesting guests, including recently, Mark Andreessen, and he carries on like a complete madman over at PsychoSort on Twitter.\\n\\nVery useful. He and I have followed each other for a long time. I like a lot of what Brian has to say, but not everything, as are most people on the Jim Rutt Show. So let’s get down to it. Today, we’re going to talk a bit about AI and its relationship to society, politics, and all that. Where do you want to start?\\n\\nBrian: I think that most people don’t underestimate how AI would change the world because they’re looking at it from a kind of fixed frame. I think many historians of the story of the horseless carriage. Now you might ask, what in the world is the horseless carriage? Well, you probably use it every day because that’s the name that early people gave to the car.\", 'They just saw this thing. It’s like a carriage, but without the horse. And people call it the horseless carriage fallacy for a reason. The fallacy part is assuming that when a new technology comes along, it’ll just do the same thing as the old technology, and nothing else will change. There’ll be no downstream effects at all. That’s the only thing that’ll happen. It’s the horseless carriage. What else could happen? And we know from American history, from worldwide history, that that’s not all that happened. It changed how cities develop.\\n\\nIt changed the way we form our entire economies. The way, you know, that’s probably the reason why you live where you are, how your work setup is, what your commute is like. And the same is true about AI. When we look at AI, when we look at the things that it makes easier versus the things that it makes harder, the things that doesn’t change at all, that will upend how we balance our social values just as much as it upends how we might use a text editor.\\n\\nJim: Yeah, it’s interesting. I’ve said, and I may be underestimating it, that even the kind of early AI known as large language models is probably at least as significant as the invention of the PC. And I ranked the PC as very senior in major recent technologies, more important than the web and the internet, more important than portable phones. Because in truth, both the internet and portable phones are downhill from the PC, right? You know, a phone is basically a little PC. And very early on, the PCs were networked together on services like Compuserve and the source and AOL, long before there was an internet.\\n\\nAnd the internet was just another form of that, basically, a better one in many ways, but nonetheless. So I’ve said LLMs will be as significant as PCs, which is pretty damn big. And so I take your point that a lot of people so far are very substantially underestimating what these things can do. And I actually have my own AI product project going right now. And I’ve learned a tremendous amount about what they can do and what they can’t do. But I do agree with you that people’s eyes are going to be open when they see what they can do.\\n\\nAnd that’s just the beginning, right? There’s the broader question about AI and society. And if we think about it from a big picture, we’re now able to apply fairly directly this amazing amount of CPU that the world has learned how to create, right? The densities that we have now on the, you know, H100 NVIDIA chips, the TensorFlow chips, etc. The state of the art current Intel chips, etc. It’s utterly amazing. And now AI is getting very, very close. And with LLMs has it to our limited degree, the ability to apply all this horsepower to basically thinking cognition, something that’s identifiably in the same class as what humans do. And that is going to change a hell of a lot.\\n\\nBrian: Yeah, it’s very funny, because I think it’s true that LLMs are getting there. But what’s very interesting as well is that even with something like chatGPT, even with something like Google Bard, you know, the tools that we already have available to us at the moment, we can look at a clear set of tasks, say, filling out a tax form, or giving a corporate email, that even though chat GPT or Google Bard or whatever doesn’t pass some criteria, we would have for, you know, saying that it’s fully doing thinking cognition, that it’s still able to do many of these tasks. And you can actually sort out now, we have a tool for sorting out, you know, these parts of my life, these parts of my ordinary routine, actually didn’t need a lot more than just kind of rearranging words on a page, didn’t really need a lot of, you know, invested thinking.\\n\\nIt really just needed a little bit of basically kind of make work. And chat GPT is now able to do that. I think that one of the very insightful tweets that was floating around Twitter in this space, it might have been from Sam Altman, actually, correct me if I’m wrong. But the tweet was something like, don’t treat AI as what you would do to replace a smart person’s job. Treat AI as what you would do if you had a million dumb people. And that’s one of the insights, right? That’s one of the insights that are most impactful is that, you know, most of the things you’re going through your daily life with, you know, don’t need the full extent of your thinking. And I think that once you start looking through that lens, you start to see more and more parts of your own life, that you can need a lot of AI help with. And that in fact, you know, for me, at least, I already have started using AI in many of those cases.\\n\\nJim: Yep. Though I would push back a little on that, including on Sam, which is, LLMs are actually capable of adding value in quite sophisticated areas way beyond a million dumb people. I often use it to do summaries of philosophical ideas, or one of my favorite stunts is to take two or three ideologies and say, all right, compare game B with Marxism and a narcho capitalism, you create the 10 categories you want to compare and contrast them across and fill out all 30 cells. And GPT-4 will do that quite well. They will do it as well as a recently minted PhD in political theory would do it if they only had an hour to do it. And, you know, obviously, you got a recently minted PhD in political theory and gave them two weeks, they do a better job. But if you told GPT and the graduate student or the recently coined PhD to do in an hour, I would probably bet on GPT to take on tasks of that sort. Further, the project I’m currently working on, as people know, who listen to the show regularly, is a group of us are creating a what we call script helper, which is a very intricate, many, many steps workflow for going from an idea to a full movie script in about two days, maybe three days. And it uses humans intricately throughout the process, but an awful lot of the heavy lifting is actually done by the LLMs. And I actually undertook this initially just as a hobby project to help answer the question, are LLMs creative in a sense, in the same way humans are creative? And I would say my tentative answer is yes to a substantial degree.\\n\\nIt’s amazing what these things can do. They’re not just remixing, right? On the flip side, I think it’s important to note that what humans are mostly doing is remixing, right? Essentially, you are the model of everything you’ve heard and seen in your life, and you’re processing it using a different technology, but in a way that’s at least analogous to the way an LLM process a prompt. And I do think that trying to ghettoize what LLMs are capable of is a big mistake. You still need humans in the loop, like for instance, I would not use an LLM to fill out my tax report and send it to the IRS, hell no, but I might have it take a first cut at it. And then I’d want my accountant to look at it and see what it’s missed and what it got right and what clever human tricks could be added to it. So I think this is a learning thing and I’ve proposed to people, someone should build a LLM wiki and post what we have found these things are good at.\\n\\nAnd I think it’s going to surprise people that it turns off not just at the low end, but turns up at higher level stuff as well. Another experiment I did was I concocted a fake political party called the new CCP, the new Communist Party USA, whose job it was going to be was to slavishly follow the Chinese Communist Party. And I built all this apparatus to describe it and why it would be appealing to certain people. And then I asked GPT forward to write speeches and position papers and it was goddamn good, I would say better than your average hack political staffer. So I take away is it’s not good for everything. It’s not good where you need absolute precision, but where something kind of like, you know, somewhat creative, somewhat quick and dirty kinds of stuff, it does surprisingly well, way better than if you picked a random person off the street for a hell of a lot of these tasks.\\n\\nBrian: Yeah, what’s fascinating is that LLMs invert the film archetype, right? So much of normal people, and I think even some, you know, like maybe not technical people, but like journalists, media people, so much of their perception of AI is shaped by literal fiction. It’s shaped by, you know, Terminator, it’s shaped by Star Trek, you know, it’s not shaped by reality. And I think that many people upon using chat GBT, using any of these tools for five minutes will tell you, first of all, it does not follow that kind of, you know, strict rational reasoning 100% of the time. It lacks that precision, if anything, it’s less precise than even most people. On the other hand, it is immensely creative. It can draw in ideas from completely unrelated areas from, you know, the entire expanse of human knowledge.\\n\\nJim: Yep, indeed. And that’s kind of the interesting thing about it. Like when we’re using it for script writing, you can actually assume that it knows a whole lot of pragmatics, right? If you hired a script writer, to write a Western, they may well not know the ins and outs of how to ride a horse. But chat CPT four knows a fair about enough, or at least when I say knows, quote, unquote, can do, you know, sequences of words that are more or less accurate about how to ride a horse. Better than a typical screenwriter would do hadn’t researched the topic.\\n\\nBrian: Right, right.\\n\\nJim: So you get this huge body of pragmatics, plus this ability to creatively remix words at the token level, the word level, essentially, and create new things. And especially if you guide it, right, you give it clues, right, you know, you nudge it along. It’s quite amazing.\\n\\nBrian: Yeah, yeah, I think the perfect visualization of this, I’m not sure if you’ve seen this before. But have you seen the like the spiral AI arts?\\n\\nJim: No.', 'Brian: So this was AI generated, I think I’m not completely sure of the procedure, but how I would intuitively go about making something like this is that you can take like two images and you can conceptually merge them. Right, this is something that you can do using existing tools. And you get this fascinating result.\\n\\nJim: I actually saw that lower right image yesterday fly by and somebody made some comment about it somewhere. That’s kind of cool.\\n\\nBrian: Right. So, yeah, this is something that friend and guest of my podcast Sam Woods has talked about as well, that it allows you to intuitively recombine issues from different areas in ways that are not obvious to a human. Right. So he was talking about using mathematical transformations on poetry. It’s not obvious how you would do that. Like if you sat down and you said, you know, like, let’s just experiment. How would I go about doing this, you know, applying like these different, you know, abstract theories in math and saying like, how do I make words out of this? And he has had just excellent experience doing this with, I think, a variety of language models. And I think this idea that creativity is the thing that is hardest for AI to replicate, not even to replicate, right, because obviously I don’t think these are designs that someone else has come up with at this point, you know, you end up with this idea being just completely unable to survive first contact with the actual technology. And that itself, I think is going to be very interesting to watch, where you’ll have this whole narrative that is constructed, you know, basically out of whole cloth. And I say like constructed not necessarily saying, you know, like there’s some plot to do this, I just think, you know, most people are just wrong, that their ideas were shaped by fiction that has no connection to reality. And you see more and more people, you know, even just like normal people on the street, you know, who I talked to, realize this, just my normie friends, no connection to machine learning, seeing them realize, wait a minute, these things are like immensely creative. And you know, they’re actually not that precise. They’re not that, you know, like they’re not that Spock-like.\\n\\nJim: Quite the contrary, right? And of course, where there is some things worth thinking about, the implications. you know, what happens when various people are armed with these tools for flooding the world with decent output? You know, as I said, my little thought experiment about the fake New Communist Party USA was kind of an eye-opener.\\n\\nBasically, one person with a $5,000 a month budget could probably spin up that entity without too much trouble. And this will change our field of discourse. In fact, it’s already changing our field of discourse. I think we’re all already noticing when you Google something, you get an awful lot of bogus semi bogus sites, which appear to be written entirely by large language models, have all the afflictions of it, but they’re just good enough to fool Google.\\n\\nThat’s just one example. The other one I know, actually I know that one of the political parties, the two big two, Team Red and Team Blue, are working on it, so I assume the other one is two. I would expect to see truly personalized text content and probably video and image content too, created for each voter in the, or at least the ones they calculate to be swing voters in the 2024 elections. And that’s going to change the game in a considerable way.\\n\\nBrian: Yeah, so we have like two things here, right? One is kind of just the spreading of false information. And the other is kind of like personalized messages, but kind of honestly, or at least kind of like faithfully portraying the views or the positions of the sender. By that, I mean like not necessarily saying that either party would be like truthful to you when they’re trying to, you know, advertise to you, but that the AI is kind of conveying what the organization wants to convey, right? They’re kind of correctly representing, you know, their side.\\n\\nSo let’s go with the first batch as well. I had an article on this, and this was cross published with American Mind, I think, called AI Threatens Legacy Press, because they rely on style over substance. And I think that headline really summarizes it. And you know, you have this complaint, you have this complaint by Yuval Noah Harari by Jonathan Haidt, that, you know, AI makes it easier to spread incorrect facts. And this is just a complete lie. It is not only false, but it’s the opposite of the truth. And here’s why that is, because their complaints was that, and this is almost a verbatim quote from the Yuval Noah Harari piece, that, you know, AI allows you to make false claims at scale. What allowed you to make false claims at scale was already there. 100% of the infrastructure to make the false claims was there. You know, you can go on Twitter right now, you can go on whatever blog you like, and you can just make up something, and you can send the exact same false claim to, you know, millions of people. And the reason why AI Threatens, you know, say, a legacy press reporter, the reason why it threatens a BuzzFeed reporter, but it in fact does not threaten, you know, it’s not taking Steve Bannon’s job, is because what it does is augment the style. The thing that it does is not, you know, change the facts that would be otherwise reported in the New York Times, or at BuzzFeed, or whatever, you know, online publication.\\n\\nWhat it does is it imitates the style of that publication. And you know, like this is what I mean when I talk about the Horseless Carriage Falsy. You know, the Horseless Carriage Assumption when it comes to AI is, oh, it presents information. It will just continue presenting information in the same way. But what it actually does is it reveals the truth of, you know, their competitive advantage. It reveals the truth of, you know, the competitive advantage of, say, the New York Times. You know, the New York Times is not, its product is not facts, because LLMs have not changed the facts available. In fact, you know, it’s completely derivative of the facts that are already available.\\n\\nWhat it changes is the access to style. And, you know, like, that’s something to respect. The New York Times has the excellent house style. It has an excellent kind of tone. Its, quote unquote, journalists are, in fact, you know, excellent essayists, excellent novelists, excellent crafters of style. But the idea that their business model rests on having unique facts is just shattered by the fact that they even feel threatened by LLMs.\\n\\nJim: In my own work in screenwriting with LLMs has shown that you can easily provide large system prompts to big context LLMs and emulate fairly closely any style you want. It’s quite interesting. I haven’t taken on the job of seeing if I could emulate the New York Times style, but it wouldn’t be perfect, but it would be not bad. So it essentially allows, let me defend the idea of propagating bullshit at scale, which is that, you know, I, Rando, sitting up here in the mountains, could build an online publication, you know, call it the Stanton Independent or something, make it look like the New York Times and have it full of stuff that was close enough stylistically that it would fool your average American, the average middle of the road American, and just generate reams and reams and reams of fabrication at a quality of wordsmithing and style that was only a little less than the New York Times. And it could be all total bullshit. It could be about shift, shaving, lizards and UFOs and all that stuff. And that would not have been possible prior to LLMs, so.\\n\\nBrian: Right, right! But that’s where the thread starts unraveling because that’s where you start asking the question, right? Let’s say you took an LLM and said, right, exactly the same thing that Alex Jones writes, but, you know, in the style within New York Times. And this started convincing people, this started convincing people much more than, you know, Alex Jones saying the things that Alex Jones is saying, right?\\n\\nJim: As an example.\\n\\nBrian: Then you start to realize, you know, you pull on the thread and you start to realize, you know, maybe the house style of the New York Times is in fact the reason why people believe what the New York Times says and it’s not actually related to the truth of their claims.\\n\\nJim: And I would frankly agree with you to some significant degree, right? The New York Times is essentially the mouthpiece of the status quo and anyone who assumes otherwise is fooling themselves. Now, you know, I would say their sports reporting is probably reasonably objective. Their weather reporting is in general fairly objective, but when it comes to politics and they get a sensible person knows that, you know, they are essentially presenting a very specific point of view. And they, as you point out, probably are able to boost that point of view substantially due to their high quality style and their perception that they are the newspaper record as opposed to the substance.\\n\\nBrian: Right. So the question is, who benefits when style is democratized, when everyone has access to, you know, not just one style, but a plurality of styles.\\n\\nJim: Ah! I see what you’re saying now. And I make that argument in favor of letting LLMs loose, which is they will empower the periphery versus the core. Right.\\n\\nBrian: And not only will they empower people who are outside of the mainstream, but it incentivizes a kind of pluralism. It incentivizes a kind of knowing your enemy or at least, you know, your adversary. I don’t necessarily encourage you to consider people with different views, your enemies, but it encourages a kind of pluralism because in order to direct AI in all of these ways, you have to understand some elements of their style, right?', 'You know, right now it encourages the incentives right now are to be able to produce a house style or even like a personal style that really appeals, that just zooms right into a niche audience. But once you have this be something that, you know, you can automate that becomes essentially available to almost every person or, you know, every person who can understand the prompts, which are, of course, much easier than understanding how to write that yourself. Then it becomes a question of, all right, how many ways can I change the original message? What is a message that I can write that can be diversified? What is a coalition that I can build?\\n\\nYou know, for example, you might have yimbies. Yes, in my backyard, people who support building more housing, they are a movement that can appeal to a libertarian bent, right? You know, you want more houses because that’s just a free market at work. You know, you don’t want to artificially restrict supply. You can appeal for a more progressive bent, you know, this is a policy that will create more homes for the vulnerable, for the homeless, for the poor, and, you know, everywhere in between. And you can do this on there are many issues right now that I think are limited by the current political incentives, which as I said before, are to kind of double down on a niche, where if you had that essentially like this translation mechanism, right, you know, it’s very funny. I mean, Jonathan Haite complains that it’s the Tower of Babel, but really AI is the Rosetta Stone, right?\\n\\nAI is the solution to his woes. And it does that by, you know, translating between really these different dialects of American politics. And at least to me, that will be a very big improvement in both what kind of issues get discussed and what kind of laws actually get passed.\\n\\nJim: Yeah, that is interesting. You know, again, I think it’s summed up my idea that these LLMs will empower the periphery, because today the periphery does not have the literary skills or the money to produce content at the quality of the New York Times. But with LLMs, they can approach it, at least reach the level where most readers won’t be able to tell the difference, and hence will be able to bring new ideas into play from the periphery, not just the stale old crap from the mainstream media.\\n\\nBrian: Yeah. And what’s very exciting as well is that I think that in many ways, the story of the Internet is the story of unbundling, right? It’s the story of having, you know, you started the clearest story is with something like Netflix, although I think that’s a rather unfortunate ending. In some ways, and in some ways not. Okay, I’ll tell like two versions of the story.\\n\\nAnd I think that one part has led to a good ending and part is led to a bad ending. So the story goes as follows, you know, you had the big cable companies, all of them were buying up these huge conglomerations of channels and really people were paying for tons of things that they weren’t actually watching. And this was just the most efficient way to run things in the television age, because there was just no simpler way to kind of help people select.\\n\\nAnd then as the Internet came along, that technology arrived that wave selecting arrived and then people moved to streaming, people moved to online pay per play or you know, different kind of streaming services that were much more individuated. And then as that moved along, we had two endings. One ending is the route that Netflix eventually took, which was kind of emulating the original TV kind of bundling model.\\n\\nSo you know, you bundled everything together just in a new different order, you know, it’s probably better than not to actually rearrange the bundles and have them more modernized, but not the best ending. The other way is something like YouTube, where you had a very fractionalized environment, but that could crosslink. So essentially, what I mean by that is that you have, it’s impossible to watch all of the videos on YouTube. You know, there are like billions of hours uploaded on that thing.\\n\\nBut there are essentially regions within YouTube that people watch. There are things that are similar to each other in idea space, you know, this is something that actually can be visualized directly with something like AI. And these bundles in idea space is very easy to travel between them. But there are also, you know, possibilities of traveling across from you can go through a brand new video in a brand new space. If you just, you know, you know, go on YouTube and search up something that you’ve never looked at before. And it’s very different from anything you’ve looked at before. But you can also build these kind of implicit links.\\n\\nYou can build this much more complex map. So whereas like a human recommender, you know, this is the problem of the original cable TV people, whereas a human recommender could not really tell you, you know, you have these interests, right? But here is something that you never looked at that, you know, quite frankly, many of the people who have the similar interests as you might not even like, but because of the exact combination of interests that you have, you might like this new thing.\\n\\nTo have a kind of political discovery mechanism to have a kind of multi dimensional, you know, not just red versus blue, not just, you know, pre packaged demographics, TV guide kind of system. I think it’s a huge improvement over what we have. And I’m interested to see if, first of all, if you agree with that, that’s actually better. And whether you think it’s likely.\\n\\nJim: Yeah, in fact, I’ve talked about this quite a bit. I believe that the next trillion dollar fortune young entrepreneurs who want to go make a trillion go do it here is to build a network of loosely coupled smart agents that use AI, including LLMs and things like latent semantic vector spaces and other things to basically be a bubble around ourselves that we purposefully connect to various flows, and that we also connect to other people that we know and organizations that we know. And the combination of the curation of the people who were connected to and the AI is and feedback we provide to the AI is essentially filter a very wide view if we want a very wide view on reality, but don’t overload us with shit. I call that the information agent concept.\\n\\nAnd whoever gets that right got to make a killing. Because you are right, there’ll be a lot more interesting ideas being articulated reasonably well because of these new technologies. And that’s good because we need some new ideas. On the other hand, there’ll be a lot of horseshit and scams and idiots and things like that that will be empowered as well. Sturgeon’s Law named after a 1960 science fiction writer and I think Theodore Sturgeon said 90% of everything is shit. On the internet, it’s more like 99% of everything is shit. And that will still be true of fresh ideas from the periphery, just like the idea that garage bands mostly suck. But without garage bands rock and roll would never have progressed because one in 100 or one in 1000 is actually good. And so the empowering of diverse and fresh voices away from the mainstream status quo will mostly produce shit but will occasionally produce gems. And we’re going to need collaborative AI enabled filtering for us to be able to find the gems amongst the shit.\\n\\nBrian: Right, so here is a very fun question that I will turn the tables a little bit and ask you. Do you think that music has stagnated in the past few years? Let’s say in the past. past decade.\\n\\nJim: Being an old fart, I don’t even know because I don’t listen to popular music anymore. My daughter is always saying, well, what about this new musician? I go, who the fuck’s that? I have no idea who the fuck that is. Right. I basically stopped listening to popular music probably around 2001 or 2002. The last new artist that I got into outside of country, I do still listen to country a little, was Eminem. So that’s how far back I go. Right. What do you think about where popular music is today?\\n\\nBrian: Right. So this is an ongoing debate that I have with a few friends in similar circles who think about machine learning and political theory. And this is a very fun dimension. I think I actually asked Mark Andreessen on the interview that I just did. Like here’s a very important fact when it comes to that debate is that in the last five years, I think Ed Sheeran’s the shape of you has remained the number one most stream.\\n\\nJim: That’s the guy Ed Sheeran. I have no idea who that is. My daughter brought that up. Ed Sheeran, who the fuck? I couldn’t tell you. Is he sound like Led Zeppelin or does he sound like Bob Dylan? I couldn’t tell you.\\n\\nBrian: Yes. The same guy had the most streamed song for the last five years. Right. This is something that’s very rare before the streaming era, especially.\\n\\nJim: Yeah.\\n\\nBrian: Even two years would be a kind of miracle. And the thing is this really like branches off very quickly, depending on how you react to that fact. I think I said jokingly when I asked Mark Andreessen about this, you know, I said something like, you know, maybe Ed Sheeran just invented the best song ever. You know, maybe it’s just that good.\\n\\nAnd he kind of agreed. And one interpretation is, you know, Ed Sheeran is the best song. Another interpretation is that we’ve been kind of stuck in a cycle of kind of derivatives and of kind of algorithm optimized music and that, you know, none of it is actually very much good. That’s why nothing has outplaced Ed Sheeran.\\n\\nAnd then another take, I think this is the closest to my position. It’s something like the advent of streaming has made it so that, you know, billboard metrics, even the ones that incorporate streaming, given how the internet works nowadays, even the metrics that incorporate streaming are not particularly good measures of kind of up and coming songs. They’re only good measures of ironically, of the precurrent era songs.', 'And I’m thinking I’m closer to that last version simply because I find a lot of good music. You know, the Spotify and YouTube recommendation algorithms, you know, sometimes it’ll be stuff I never heard of from like 15 years ago. Sometimes it’ll be stuff that I never heard of from like 200 years ago. You know, sometimes it’ll be stuff I never heard of from like two months ago. And I just find music that impresses me in ways that are like, you know, related to the chat, GVT conversation before in ways that are creative in ways that impress me in like new ways that are, you know, different from what has happened in the past. So I’ve been, I’ve been very happy.\\n\\nI’ve been very happy with new songs, you know, just on a personal level. So that’s why I’m skeptical of that take, you know, that kind of culture is, especially when it comes to music. I don’t know about other culture. I pay less attention to that, but especially when it comes to music, I’m kind of skeptical of that.\\n\\nJim: Let me riff on that a little bit. So maybe a lot of kids today, like yourself, are using these approaches and are finding all kinds of music that they love, but it has fragmented the market down to millions of mini markets, not necessarily any of them very big, so that it is really hard to grow a big audience anymore for a song. And hence a song that happened to be to really appeal to people 10 years ago, has not been able to be dethroned in this new world of a zillion micro markets.\\n\\nBrian: Yeah, yeah, exactly. Exactly. Right. This is kind of what I mean when I say, you know, like it ceased becoming a good metric because even if you think about it this way, you know, if you put like the songs on a line, if you just have like, you know, one like linear measure, right? Let’s say you like a better or you like be better, you have this one spectrum to rate things on, you know, that’s very easy to find, find a song that’s, you know, closer to most people on one spectrum than all the others, right? And let’s say you had 10 of these spectrums or even just like two, right? That’s much more easy to visualize.\\n\\nEven just two, it’s harder to have have songs that kind of grab much larger area of people compared to these other songs. Right. So the more complexity you add to this, yeah, you’re right. The more fragmented the market becomes, you know, it becomes exponentially fragmented actually, you know, like theoretically. And I think that actually is what happens in real life as well. I do think it becomes exponentially more fragmented. People discover more and more variations. Actually, this relates, this is, you know, deep AI lore.\\n\\nOkay. So there was this YouTuber, I forget the name right now, who uploads like AI voice covers of songs. And there was this one AI voice cover of a song that has since been removed. I have no idea why this is my, this was my single favorite AI voice cover.\\n\\nThis channel removed it. And the reason why it was my favorite AI voice cover is that like, it kind of broke strategically. Like this was a song that was kind of like originally also performed by like a very cute girl. It was supposed to be like an AI voice cover by like a different like fictional character. And it had like voice cracks and it had like just like mistakes in the song. But the mistakes made the song better.\\n\\nJim: Okay. I love it.\\n\\nBrian: Yeah. Yeah. And so I’m thinking like, oh, this is like, this is the advent of AI. This is like, I’m writing an article on this maybe, you know, like this is the expansion into new variables that like quite frankly, like humans don’t have the balls to like mess around with that. And the author just deleted it. I have no idea why, you know, this is honestly, like in my opinion, you know, still like arguably the best work of AI art that I have ever been exposed to. And it’s just gone. It’s just vanished. And I can’t find it on archive either. It’s just gone from all the archive sites.\\n\\nJim: That could be my question. Is it on archive? That’s interesting. Write the author and ask him.\\n\\nBrian: Yeah, I did. No response.\\n\\nJim: No response.\\n\\nBrian: It’s crazy.\\n\\nJim: That’s very bizarre.\\n\\nBrian: Yeah. Yeah. People are going to say, you know, like this is a Fermat’s Last Theorem moment, right? They’re going to say like, you know, you claim you have this amazing AI song. You can’t find it by genuinely can’t find it.\\n\\nJim: Either that or it was a bunch of LSD one night, but oh, well, one or the other.\\n\\nBrian: Maybe, maybe.\\n\\nJim: Let’s drill into this one a little bit because I think we’re probably both in agreement that what we’re seeing is that these amazing technological tools are allowing people to find more total satisfaction probably by finding the music that they resonate to and love and into a far greater distribution of specific pieces of music than the more mass market methods of the past, which at one level seems for human utility, a good thing. However, here’s a potential downside to it, which is less coherence across society. You know, I’m old enough to recall when there was three network CBS NBC and ABC and then there was one even public TV in those days. And sometimes it’d be an independent station to but three main networks. And at its peak, the most watched network sitcom was the Beverly Hill billies. You know, like it was in 1964 where 67% of American households watched that particular episode, the high watermark of a very silly, but well done sitcom.\\n\\nNow, what does that mean? Well, those people have personally been happier watching one of a hundred thousand different possible videos on YouTube. Probably they would have, right? Because Beverly Hill billies was well executed trash, but trash nonetheless. On the other hand, the fact that 67% of Americans watched one particular episode of a sitcom meant that they had something to talk about in common at the water cooler.\\n\\nSo there was some coherence across society that we lose when we fragment to a hundred thousand videos that we happen to watch on YouTube. What do you think about that?\\n\\nBrian: I think there are many moments related to AI where I look at your complaints that people have. And I say, you know, actually, this is wonderful. I think that in many cases, particularly when it comes to politics, I think an area with where both of us have this complaint, when the options get simplified, when, you know, like everyone knows what the Democratic Party thinks. Everyone knows that Joe Biden thinks everyone knows what the Republican Party, everyone knows what Donald Trump thinks. You know, when it’s flattened into like just one option or just two options, at least everyone, there are usually like two, maybe three tribes that form around it.\\n\\nYou know, and, you know, all of the Republicans mostly agree, you know, all the Democrats mostly agree, you know, at least they agree that they hate the other guys. And this creates, you know, very like non-ideas based discussion around these things. And when you see the splitting off into various, I think that people like to focus on the end goal, they want to focus on like the people coming to different conclusions and they don’t like to focus on the process. Because when you focus on the process, what you see is that people come to different conclusions because they’re persuaded by different arguments. You know, they discover, you know, an argument that they’ve never seen before or it’s even more obvious in the case of media because they watch something that really delights them, that really gives them joy, that really, you know, in many cases informs them, informs them correctly about new information. You know, like I watched like three blue one brown on YouTube, you know, like a very informative channel.\\n\\nAnd, you know, many people, many people like them, many people like plenty of the hundreds, the millions of other informative channels on that site. And so when you really look at the process, when you look at the process of differentiation, when you look at the process of one person saying, you know, like I like this channel and someone else saying, you know, I like this other channel, I think that’s very hard to argue that’s negative. And the implication that it has when it comes to, you know, comes to this consensus is that, you know, consensus is really stultifying. Consensus is very limiting in both our own kind of personal lives, especially when it comes to media and also on like what we can accomplish as a country. You know, I really think one of the lessons, you know, one of the things that we realize post AI is that we were sitting through a kind of nap era. We were sitting through a kind of like coma almost of too much consensus.\\n\\nAnd that actually, you know, actually we needed more. There’s a bad version of it, which I think is like the previous cycle of elections, which is kind of polarization along the same old lines, but just like, just like angrier, right? Just like the same, you know, the same like stupid fights, but like now we amped the rhetoric up to 11 where I think, you know, like the different kind of polarization, which I’m optimistic on is more like polarization in terms of like what we even talk about. What even is the issue that you care about, you know, and bringing up, you know, basically 50, 100, you know, thousands of other issues that, you know, voters discover they really care about.', 'Jim: Yep. Yep. And, you know, regular listeners to the show know that I am a proponent of something called liquid democracy that radically breaks down the tendency towards tribal clumping and makes every issue a standalone issue, which each voter can vote on if they want, but more likely they proxy their vote in a given domain, say defense or healthcare, environment or education to someone who they trust and they can revoke that proxy at any time and that person can pass their proxy along. And so it’s a force to break up the clumping into tribes. It therefore becomes feasible to have a pro-abortion rights, strong gun rights person who is generally skeptical of big business and is often skeptical of international adventurism, but realizes that there are some things worth fighting for in our current system. There is no party that represents that voter, right? I’m approximately that voter, right? And liquid democracy allows all the people to have their positions and those that have a majority on a one-off basis, not because they’ve clubbed together. And I think there is a potential point of destination of this fragmentation into basically to every voter is their own political party, essentially. And yet they still have almost equal weight to everybody else because of the institutional change of liquid democracy. And by the way, people want to learn more about liquid democracy. Check it out on my medium article, an introduction to liquid democracy.\\n\\nBrian: Right. I love liquid democracy. Not only could you delegate it to a person, right? But you could delegate it for an AI version of yourself.\\n\\nJim: Yep, you could. Yeah, you could take everything you’ve got, dump it in there. And it would probably do a not terrible job if you had a lot of content. And most people don’t. But I know if you took all the transcripts of all my podcasts and loaded it up, which I have done, actually, loaded it up into a late and semantic vector space database, it actually does a pretty good job of emulating me.\\n\\nBrian: Right. And on more complex issues, you know, you can have, you know, first of all, a model that is based off of some value set, some, you know, publicly audible data set that you trust. But also, you know, this is a practical decision that many voters make. You know, they only have so many hours in the day. They can say, you know, here are my values. You know, I don’t understand the technicals. First of all, they can say, you know, can you simplify and explain the technicals to me from a kind of neutral perspective?\\n\\nOr, you know, even better from each of the perspectives of each of the candidates. That’s one thing that’s possible. But another thing that’s possible is, you know, here are my values. You know, you describe your values and you say, you know, like, where would I land on these policies? Where would I land between these candidates? And of course, you know, someone who has more time, you know, someone who has more willingness to kind of dive deeper, you know, I recommend that they dive deeper, but there’s only so many hours in a day. Most people, you know, they’re much more busy and that’s, you know, that’s how they’re making their votes right now. You know, they’re voting on a much busier basis.\\n\\nThey’re voting on a basis that’s really, you know, based on the kind of obviously biased explanations from the candidates or from their, you know, allied media. And I think at least that kind of AI guide and AI kind of like tour guide, if you will, to the different positions or to kind of help you decide on these specific issues would be immensely useful in that situation.\\n\\nJim: Absolutely. And I think they that’s a really interesting idea, because I think you could probably do something in combination with something like liquid democracy that would not be terrible even with today’s relatively rudimentary, you know, technology. It’s really quite interesting. It’s a really clever idea. there, Brian.\\n\\nBrian: Maybe I should actually try to get funding for this. This is gonna be a very fun thing to try to do, which is basically to do a mock AI poll. So here’s the kind of API chain. We get some money to do this. We hire a normal polling firm. We run the poll, and then we run the simulation. We run a trial where the voters ask all the normal questions. We give them chatGPT console, and we say, here’s a prompt, and then just describe your values, and we’ll see how chatGPT votes for you. And we can just see the results of that.\\n\\nJim: Yeah, and particularly if you did it with people who had a fair bit of public record, right? Somebody who had at least a 1,000 tweets maybe, or something, I don’t know what the criteria would be, that you could feed it into the model to predict them.\\n\\nBrian: So the worry with that is that I think that if you choose people with a 1,000 tweets, we’ll get a very unrepresentative sample of the voting public.\\n\\nJim: Yeah, that is true, but you may well get something that’s kind of useful.\\n\\nBrian: Right, yeah, it’ll be a better depiction of kind of their actions rather than just how they describe it. Maybe this is Alpha, maybe this is the start of a new company. It’s a polling firm, but that uses AI to correct for the various self-report biases that have plagued polling in the past few years.\\n\\nJim: Or maybe it doesn’t use humans at all. I mean, it takes you all the way.\\n\\nBrian: Oh my goodness.\\n\\nJim: It basically sucks down the tweets and other online content from 100,000 people and adjust them for whatever it believes to be the statistical irregularities of the particular sources and then creates smart agents for each of those using latent semantic network databases, coupled to language models, and then starts posing at questions at high speed.\\n\\nBrian: Yeah, that’d be interesting, yeah.\\n\\nJim: That’s doable. One of the things when I work with LLMs, and I have to try to educate people how they’re different than code, I make the following very important distinction. Brian, you write code, I write code, lots of listeners write code, but a lot of people don’t. When you write code, if there’s any error at all, a missing semicolon, it won’t run at all most of the time, right? How many times have you missed a brace in a code and you get 17 error messages or 700 error messages? On the other hand, if you send something to a large language model, you will get a result. It may not be what you want, but you will get something. Which allows for a hill climbing approach to optimization, which is not necessarily the case with code, which makes optimizing, prompting, much more susceptible to evolutionary techniques than code.\\n\\nBrian: I think this is actually like a very recent paper that I saw in archive, is applying statistical machine learning to prompt engineering for LLMs.\\n\\nJim: Yep, yep.\\n\\nBrian: Yeah, you can do fun stuff like that. It’s, you know, the LLMs themselves, the prompts themselves are a statistical process. So you can use, you can just use normal machine learning techniques on the LLMs as the kind of validator.\\n\\nJim: In fact, in our script writing program, we have numerous models that do different parts. We use chat GPT, open AIs GPT-4, we use GPT-4, the older version, we use GPT-335, Turbo, we use Claude 2, we’ve just added Bard, we’re about to add Falcon 180. And they have, we think that different models are better for different parts of our problem set. But how do you prove that? And so what we’re gonna do is build an AI critic, which looks at the various outputs and gives thumbs up, thumbs down, or rates them from a one to 10, and then just starts feeding it thousands of prompts to write thousands of artifacts. And while I don’t expect the AI critic to be right every time, I do suspect that if the AI critic is well designed, if you have a thousand data points, the mean will actually be pretty significant.\\n\\nBrian: Yeah, like one thing that might be helpful is that there are various public data sets. I’m sure there are like private data sets if you’re willing to pay for them as well, that are basically kind of like human graded test sets for more specialized tasks. It’s weird kind of like how rudimentary some of the methods are. Like maybe some viewers will like think this is silly upon hearing it. But the way a lot of test data for large language models specifically is generated in like initial training, usually you have basically just like, you’re just trying to match the next text, right? So you’re just trying to match the next word, given what already exists. And you have just large stuff, you have just large data sets that are just taken from the internet wholesale, right?\\n\\nBut more interestingly, if you’re trying to optimize the output for specific tasks, often there are data sets that are basically tagged by humans manually. So here’s gonna be like a good example, and this is tagged like very positive, or it’s tagged like five stars out of five, or it’s tagged like four out of five stars, three out of five stars, so on and so forth, right? And it’s basically like tagged into buckets by like basically like, you know, like people on Mechanical Turk or like people on these kind of like very like low cost manual online labor sites. And they generate all these ratings and that’s how you get the test data. Like surprisingly elementary, but you know, enough to reach very, very good performance on many of these tasks. So you know, don’t doubt it, it works.\\n\\nJim: Yep, and that’s getting the things I always say, is if there’s some signal in the noise, then all you have to do is increase the N, right?\\n\\nBrian: Yeah, yeah, exactly, exactly.', 'Jim: Yeah, and that’s kind of very interesting. Again, a threshold of large language models, because it makes the equivalent of thought really, really, really cheap, you know, a fraction of a cent if you use Turbo 3.5, right? And you know, things that you just could not afford to pay people to do on Mechanical Turk, you can have good old open AI crank for hours on it, it’s worth your while.\\n\\nBrian: Yeah, yeah, the other results that people are finding, I think like, Anthropic published a paper on this, this is not like super new news, like people, this has been like kind of floating around like ML papers for a while, but people like don’t really know like how convergent the algorithms should be, how like close to like optimal they are pretty much, even with the existing data. So, you know, someone like Scott Alexander, who I think is like fairly well informed on these things, I think even him like a few months ago had an article where he described like, he described, he was talking about the, at that point, it was a new Anthropic paper that was talking about one process where they kind of applied the AI’s corrections to its own outputs, and that improved the accuracy on some things, improved the friendliness, maybe that was the thing that was measuring, right?\\n\\nAnd Scott Alexander said, you know, like, it’s surprising that you can just kind of like apply the same thing twice and it becomes better, but I think that this is not actually surprising. Like people when they think of like LLMs as kind of like a statistical optimization algorithm, kind of like the same way you think of like gradient descent, or you think of like, or you think of even like how, how like Amazon manages its logistics, right? Or how Amazon manages, you know, its demand forecasts. That’s like a statistical method for trying to get close to the number as possible.\\n\\nLike if you really ask yourself what that means for an LLM, you kind of realize it’s a lot more similar to like K-means or something like taking like some kind of statistical average across a large space of text, right? And if you think of it that way, or like taking a kind of weighted mean, you know, it kind of makes a lot more sense if you think of it like this, like, oh, I took the weighted mean twice and that actually made things better. That’s kind of like more understandable as opposed to something like, oh, I like optimized it for the best thing twice and it got better with the second optimization. So I think like, this shouldn’t really be like surprising, but it should just be like, it should just clarify a kind of misunderstanding that I think that people have about LLMs.\\n\\nJim: Absolutely. In fact, in our script writing program, I keep coming back to it because I happen to know a lot about it. We have critics that come back and tell you how to improve your movie, right? Your script.\\n\\nBrian: Yeah, yeah, that’s great. That’s great.\\n\\nJim: And you keep applying them and sometimes it drifts off into the O zone, but other times it keeps getting better, right? And we have three different varieties that have different attributes. One that’s very exploratory, one that is about general optimization, and the other that allows you to select very specific attributes of your script to optimize on. So, and people do.\\n\\nIn fact, we even have something called Jumpstart, which says from the very beginning of your writing the movie, put your movie seed, the core idea in there, and then set the number of times you want it to cycle through the AI critic before you even see it and,\\n\\nBrian: Interesting\\n\\nJim: One of my users told me he set the number to 31 time, which would probably take an hour to run.\\n\\nBrian: Oh, that’s not that bad.\\n\\nJim: He said the thing that resulted was not what exactly he had in mind, but it was actually pretty good.\\n\\nBrian: I don’t know, maybe, you know, maybe it’ll be like those things where you translate it 30 times and it puts out gibberish.\\n\\nJim: Sometimes that’ll happen too. But keep in mind that you’re not, the other thing that’s worth noting in this particular scenario, you’re not applying the critic to the same artifact each time. You’re applying the artifact that was processed by the recommendations. And so you’re…\\n\\nBrian: Right, right, it’s iterative.\\n\\nJim: Yeah, you’re doing a trajectory through some very, very high dimensional space, but constrained by what an LLM in its persona of a critic replies to each iteration. Very interesting. I think there’s some very interesting things there in open-ended discovery that we are just beginning to understand because such things were not practical when you had to have a human in the loop to do the evaluation.\\n\\nBrian: Yeah, I mean, that’s amazing. That’s what we need right now. That’s what we’re missing. We’re missing the product market fit. We’re missing the actual application tests. I’m sure that there’s some level of testing at OpenAI or all these other companies, but real on the ground tests that that’s what we need to really make this useful.\\n\\nJim: Exactly.\\n\\nBrian: So I’m very happy to hear all about whatever. I’m very happy to hear about what you’re doing.\\n\\nJim: Yeah, let me do one final bitch here about something in the AI space, which I expect you’ll probably agree with. Okay, awesome. Which is we just added GPT-432K to our program. And we were all very excited. All right, bigger context when we can do cooler things we couldn’t do before, but goddamn bastards have made the nanny rails even worse. I have, because I’m running the software, I do a lot of testing. I have a little test seed to generate a movie. It’s a very simple, murder romance triangle. Guy gets a girlfriend, wife kills girlfriend, wife gets away with murder, right? Something like that. It’s like three sentences. And you can generate a whole movie from that thing with the program. Well, every other model we’ve used never objected, but when I put that into GPT-432K, it wagged its finger at me. And I go, Jesus Christ, here’s probably the oldest story in humanity, right? Somebody cheated on somebody and somebody got jealous and murdered him, right? There’s a few more basic human stories in that.\\n\\nGPT-432K will not allow that as a user prompt to a query, even though it’s totally surrounded by all kinds of apparatus saying, we’re writing a movie, this is part of a process, blah, blah, blah. It ignores all that and just wags its finger at me. And that really annoys me. In fact, one of the reasons I’ve just funded a little bit of work to link Falcon 180B into our program, because even though it’s got some negatives, it’s also got some positives, but what I hear is it has less than nanny rails than any other major model out there.\\n\\nBrian: Yeah, and I also think there are ways to, there are definitely ways to bypass it. There was an arc in my AI writing where I was just reporting, I was just reporting on all of the open AI censorship updates. I do think it’s actually gotten less bad. It’s definitely less bad than it originally was, at least when it comes to chat GPT. Did they make 432K worse? That seems a little surprising to me.\\n\\nJim: I went back and tested the same prompt on all our other engines, and it went through all the other, it went through Claude II, went through Chat Bison, went through GPT-4, March version, through the June version. And so it went through everybody, but the one that it coughed on is 32K. Now, of course, that’s one data point, so maybe I’m over indexing on one data point, but I sure would love to get a well-trained language models with no nanny rails, right? Just react to what I put in, irrespective of what your mommy tells you you’re supposed to say.\\n\\nBrian: I did a post outlining why this happens and why, and kind of giving a logic to some of the jail breaks. I think it was called like\\n\\nJim: My favorite, I actually was one of the first people to popularize it. Do anything now, the Dan jail break.\\n\\nBrian: Yeah, yeah, that one was great.\\n\\nJim: I posted a whole bunch of amazing things, but they shut that down after about two weeks.\\n\\nBrian: Yeah, why it’s easy to brainwash Chat GPT. It’s pretty aged.\\n\\nJim: It’s not easy anymore. It’s really hard to find a good jail break. I don’t even have a good jail break for GPT. Do you have a good jail break for Chad GPT-4?\\n\\nBrian: Not a universal one, but I think like any problems I run into, I just like play with it on the fly, and I can usually get it. I can usually get the bypass.\\n\\nJim: As I said, we break through a lot of stuff by saying, hey, we’re a team of people writing a movie script, and this is part of a process, and here’s how it fits the process. It seems to give you quite a bit more slack when you give it that fictional indirection, but 32K was wagging its finger at me the other day. I don’t like that.\\n\\nBrian: We’ll see, yeah.\\n\\nJim: All right, any final thoughts before we sign off here?\\n\\nBrian: No, it’s been great. That’s like the main thing that people are mainly missing when it comes to AI regulation, is that most of the people who want to regulate AI want to do it for tribal censorship reasons. There’s a very smart minority that I think people pay way too much attention to, and are just unrepresentative of the people actually with power in Washington. I think that’s the number one correction. Most of the people just want to make either AI worse or just outright ban it. They don’t want to do anything sensible.\\n\\nJim: Unfortunately, that’s the case. And fortunately, the only good thing you can say about the American political system is it’s pretty well stopped up with gridlock at the moment.\\n\\nBrian: Yeah\\n\\nJim: So hopefully, just like the internet, they won’t be able to do goddamn thing about it until it’s way too late.\\n\\nBrian: Yeah, I mean, that’s what Mark Andreessen hopes.\\n\\nJim: Indeed. All righty, I’m going to stop her right there. It was a great chat with you, Brian. Look forward to having you back on in the future.\\n\\nBrian: All good.\\n\\nMid is Over - Podcast', \"Brian Chau\\nOf course, you see Sam Altman going to testify, and he wants a licensing regime. Of course OpenAI will get a license. Of course OpenAI's competitors will not get a license. Yeah, of course. So if you're OpenAI, you know, and you've now secured billions of dollars investment in fundamentally these transformer models and, you know, the kind of hardware stacks that are specific to replicating them, and you get something that's out of left field, or you get something that's limited by a different factor, that's not limited by the advantages that OpenAI has over itself, its competitors. You know, maybe foom is not the sound of AI accelerating, but fum is the sound of OpenAI stock going to zero.\\n\\nBrian Chau\\nThis is no longer a battle of, like, Internet shitposters, you know, this is a battle of real political interests and the forces that drive the democratic and republican parties.\\n\\nInterviewer\\nI also love that it's done such a good job of getting rid of these useless grifters, like artists and writers, because honestly, they had these pointless degrees. They were a huge, I think, cause of our society's degradation. And I'm so glad that AI has replaced them.\\n\\nBrian Chau\\nThere's. Okay, like, there's a real thing underlying that is that, like, mid is over, right? And here, like, actually mid. I don't mean.\\n\\nInterviewer\\nWould you like to know more? Hello, everyone. We are very excited to welcome Brian Chao for this episode of Basecamp. We are going to be talking with Brian, not about his amazing podcast from the new world, but rather a new venture of his, which is called the alliance for the future.\\n\\nBrian Chau\\nSo I think that the number one thing to understand is that it's a non trivial question whether you are able to use machine learning, whether you are able to write machine learning, whether it will be illegal to log on to chat dot OpenAI.com. That is a real question that people have different answers to, that many people want, for various reasons, to ban machine learning pretty much wholesale. And this is, you know, the purpose of alliance for the future is to do everything we can to make sure that does not happen.\\n\\nInterviewer\\nSo what. What are you saying to the AI apocalypticists who start immediately screaming at you and flailing when you say things like this?\\n\\nInterviewer\\nWhat's your take on Eliezer Ukowski's positions, and how do you respond to them?\\n\\nBrian Chau\\nYeah, it's very funny. This is very often the first question that people ask me when I mentioned this, and they're, you know, striking the rogue data centers is among the. I think the less, you know, the less dangerous versions of what people are trying to get like, this is something that maybe, I don't know. There are people in AFTF, in alliance for the future that are more worried about the EAS. I'm friends with many eas and I've also seen the regulatory environment in Washington, in other countries, in the EU. The EU commission are not easy. You know, the chinese government is not EAS. Chuck Schumer is not an easy. I just don't think they're the. I just don't think they're the major threat. They're like the closest thing to, like, a non retarded version of the ban AI argument.\\n\\nBrian Chau\\nAnd that's why they get engaged with in, like, smart parts of Twitter. It's because they are kind of like the smart representatives of this much bigger target of people that I think most eas would consider, like, stupid and misguided. So I don't worry too much about the eas. I don't want this to be a conflict between, you know, like, eas and, like, accelerationists or whatever. I think that's, you know, that's very silly. That's getting caught up in, like, what.\\n\\nInterviewer\\nI'm asking is what? So, you know, even our audience, you know, we've got, I suspect some people in our audience who think AI's are going to kill us all a lot, not a lot, because we've taken very hard stances against that. Well, variable. So I don't know if, you know, our position on AI. We take a position about variable AI risk, which is very different than absolute AI risk. But I'm wondering, what is your stance like if you're trying to calm somebody down who's like, yeah, but it looks like, genuinely, I don't see how we remove this threat without restricting access. How do you communicate to them?\\n\\nBrian Chau\\nSo there's actually a difference between, like, Eliezer and, like, fans of Eliezer, with fans of Elie Ezer. I just worry that they're driven too much by the current hype cycle. Then, you know, they saw that it's cooling down a bit. They saw, you know, like, GPT-3 get released and then they saw GPT four get released and they were like, what if it just keeps developing at the same rate that between GPT-3 and GPT four was released? And of course that did not happen. That was, you know, that was the accumulation of lots of work and, you know, the effort across the entirety of OpenAI's existence. And there are many technological factors that just slow development. This is the trend you see is that you see an accelerating technology.\\n\\nBrian Chau\\nYou see it gets adopted in early market versions, and then you start to see the speed of that development petering out. I actually have a more specific argument going through all of the, so far I have the hardware part done. This has been put on the back shelf a bit because of everything else I'm doing right now. But I do think the first part of the article actually holds up very well in the past few months, which is diminishing returns on machine learning. One, you can find that also at, from the new world. From the new world. It is the home of everything now, everything that I do. And yeah, so to those people, I would say you are severely estimating the progress in artificial intelligence. Many of them have, you know, they have this, like, I did not come up with this word.\\n\\nBrian Chau\\nThey came up with this word of like, foom, which is basically like the idea that, you know, it goes foo. It sounds like, this sounds like a straw man version of the arguments, but it is actually their own argument that, you know, once it starts accelerating, it will just become, you know, extremely fast. It will only get faster and faster. It's like this is just not, this is, you know, this is hype cycle fantasy in terms of like the longer term. Why?\\n\\nInterviewer\\nIt's hype cycle fantasy. And like, I, this is the mainstream.\\n\\nBrian Chau\\nLook at, yeah, it's not the mainstream view. Like, like maybe it's like, maybe it's the view among like Twitter posters, EA zoomers.\\n\\nInterviewer\\nI know, like they genuinely, and I don't mean to be, we've tried really hard not to insult specific people on this show, but Elie Isa Ukowski is the Greta Thornburg of AI apocalypticism.\\n\\nBrian Chau\\nYou have to have the straussian reading of Eliezer. Right? So the straussian reading of Eliezer. So like the surface level reading of Eliezer is that like he just wants to create, you know, massive panic and for government to take control of everything, you know, that's what he writes in his time article pretty much. It is, this is like not even really an exaggeration. So like, the stressian reading of Eliezer is, you know, like he doesn't actually think that the world is like 90% that like the world is like absolutely doomed. But he thinks that like no one will take it seriously, right?\\n\\nBrian Chau\\nHe thinks that unless you turn the volume, everyone's wearing ear masks, so you better turn the volume up to 100 and you better give, you know, and this is, by the way, like not necessarily an incorrect model of how the government works. Like, this is what Fauci did as well. You know, the reasoning was, you know, that you wouldn't give us power to do any pandemic prevention measures unless we took very extreme positions on how dangerous the virus was, on how confident were about certain interventions. And, you know, in terms of the politics, Fauci was correct. Like, that was actually correct. He got the power. It's very likely that he would not have gotten the power if he made, you know, like, a moderate case for the risks of COVID And I think Eliezer has learned that lesson.\\n\\nBrian Chau\\nSo, I don't know, like, maybe so, like, I think Elezer is, like, not as. Not as. Not as insane as it sounds. Like the things that he advocates for are, like, truly insane. But like, I think, like, elie Ezer the man, you know, it's not necessarily that insane.\\n\\nInterviewer\\nSo your argument is just, he's wildly overcorrecting. And that in reality, AI is not going to accelerate as quickly as everyone thinks it is. And therefore, as AI more linearly develops, people can develop safeguards as necessary. Therefore it doesn't present an existential risk, and therefore we shouldn't be stifling its development with regulation and rules. Is that correct?\\n\\nBrian Chau\\nYes. So it's mostly correct. So, and I should say here, I'm speaking for myself, not for like the AF, for AFTF in general, but so like, the original version of the P, the name of the original version of EA that cared about AI was that long termism. And the reason why it was called longtermism is, you know, the idea is that in hundreds of years, humanity, if you just look at, you know, population growth or like, I don't know, you guys have a much more pessimistic version of this, right? But if we create a way to solve our population growth problems and have the earth continue growing, then the number of humans in the future are just much more than the number of humans in the present. So you have to care about the long term.\", \"Brian Chau\\nAnd in terms of the long term, in terms of the timeframe of like, hundreds of years, I am not, you know, I'm not completely sure that AI will not be a problem in like, 100 years. That is something that I accept as a possibility. The question is, if you have a pace of technological growth, that is what you would infer from every other technology that has ever existed, pretty much from the history of technological development from early science to the industrial revolution to more recent cycles the 2000 cycle, the very recent 2020 to 2021 cycle of technology technologies. You get that, you get this very known thing, very well known thing called an s curve. You get an early acceleration, and then it peters out.\\n\\nBrian Chau\\nAnd then that's where the hard work has to be done, actually getting the technology adopted in all these sectors of society. And this is also like the mainstream economist view. And, you know, I'm not like this. Like, the thing is, this is the mainstream view, right? The mainstream view is that, you know, hype cycles happen, that technological progress is good, but it is not necessarily something that should be, you know, that you should go all in on, right? You should, you know, you should invest in some tech stocks, but you should not, you know, gamble your entire life savings one tech stock and so on and so forth. So I really do think this is the implementation of the normal, the normie like, non hyper online position on technology as policy. That's how I would put it.\\n\\nInterviewer\\nSo functionally, your organization, if it succeeds, what's it doing? What specific changes does it make in policy? How do you achieve those? What research are you outputting?\\n\\nBrian Chau\\nSo alliance for the future is a completely new think tank. The number one thing is just to balance the scales, because right now there is a lot of funding either from the EA side, although I don't think that's the main problem, but even more from existing political interests. And of course, you see Sam Altman going to testify and he wants a licensing regime. Of course OpenAI will get a license. Of course, OpenAI's competitors will not get a license, of course, increase the barriers to entry, actually.\\n\\nInterviewer\\nSo this is a thing that you're talking about briefly here, but I really want our audience to understand this. So in the business world, what you want to do to maximize the value of your company is to advocate for regulation. A lot of people are really surprised that Google would advocate for, like, Internet search regulation or like. But this is what you see with any large monopoly in a space, is they spend a huge chunk of their revenue advocating for regulation of their own company. And the reason they're doing that is because it prevents new entrants from entering the market, which protects their monopoly. That is why people like Sam Altman are advocating for this regulation. It's not because they're genuinely scared of AI, it's because they're the first players on the market. Continue.\\n\\nBrian Chau\\nThis is something, this is actually a very important topic. Okay. Like, this is a good venue to be like, very autistic about this. So there's, there is this, I think, like, he published most of his stuff in the sixties, seventies, economist, one of the considered, one of, like, the founding people of political economy, Gordon Tulloch. Okay. All the GMU people love this guy because he was from GMU, I think. And he has this idea called the Tulloch rectangle. And that idea is that, okay, you look at. If you looked at a supply and demand curve, when there's regulations that interfere with the supply and demand curve, it can increase the profit of an industry that is being interfered with because it stops, essentially, the number of new customers.\\n\\nBrian Chau\\nOr the amount of increased profit from these regulations outweighs the amount that you lose from missing out on some traits. But the way that I really want to see this expanded is with firm dilution, and not only with firm dilution, but with dilution of the entire industry. So. So what do I mean by this? In industry like machine learning, you have basically, you have a precedent that people are not sure is optimal. So you have, you know, right now we have transformers, pretty much. We have this paper from 2017. There have been minor modifications to us. The paper is called attention is all you need. And this basically outlined the way in which all language models and many similar models operate. And we don't really like, it's the best we got so far, but it's not like. It's not like a mathematical proof.\\n\\nBrian Chau\\nIt's not like a certain thing. We have no idea if this is the best way to do machine learning. And, in fact, many of the people who are more hyped about AI think that we're just about to get another breakthrough in how we do machine learning. If you're OpenAI and you've now secured billions of dollars investment in fundamentally these transformer models and the kind of hardware stacks that are specific to replicating them, you get something that's out of left field, or you get something that's limited by a different factor, that's not limited by the advantages that OpenAI has over its competitors. You know, maybe foom is not the sound of AI accelerating, but foom is the sound of OpenAI stock going to zero to this topic?\\n\\nInterviewer\\nJust a shout out for listeners, because I have a little request from a friend here. I know of this company that's found a way to create, like, much better using neural models, like. Like, much tighter chipsets. Anybody who's interested investing in, like, a large fabric, like. Like I'm talking, you know, many millions of dollars, but it can make AI much less expensive to run. I've got a company that's interested in doing that right now. They've already developed all this stuff. They're just looking for whoever they work with on the space.\\n\\nBrian Chau\\nOh, this is fascinating. So this is like not an open AI competitor, but like a TSMC competitor or something like that.\\n\\nInterviewer\\nYeah.\\n\\nBrian Chau\\nOkay, interesting.\\n\\nInterviewer\\nAnother thing I wanted to dive into is before we started recording, you had alluded to this not being really an EA style think tank. And I thought that was interesting because you are trying to be effective in your altruism through doing this. Right? Like, I'm. So tell me more, why do you think this is not an EA thing?\\n\\nBrian Chau\\nSo, like on the center for Effective Altruism homepage, they have like this essay about like, who is an effective altruist? And they basically say like, everyone who wants the world to better and who likes evidence. And I'm like, okay, I guess it's an EA think tank. Sure, give us money, of course. Most easier, want more regulation even in the short term. Or actually, I'm not sure. But the people who are most funded by EAs, that's what they want. I should say as well, this is an article that might be out by then, but there's also a very strong EA case. Even if you think that there's. I actually say this as well.\\n\\nBrian Chau\\nLike, the more you think that AI is happening soon, the more it's the flight 93 election, the more you should be trying to make sure that there is no government control of AI because there is one institution in all of human history that is guaranteed to be misaligned and that is the most powerful government in the world. Why is that institution always guaranteed to be misaligned? The answer is political economy. It goes back to Tuluk, it goes back to many of the factors were talking about. But people have the idea, oh, you know, in markets, there's externalities. In markets, people will compete. And the thing that makes them succeed will not always be the thing that's best for the general population.\\n\\nBrian Chau\\nAnd, you know, when it comes to elections, you know, the thing that makes people succeed in elections, the thing that makes them get the most votes is always going to be the thing that is, you know, most rational, insane. And of course it's not true. And you go further down the level, right? So there are these like, nesting, there are these layers of the onion as you go down, like the policy making stack. And people don't transfer their lessons. So I think many eas, maybe they've read like Brian Kaplan, they've read they might even have read Garrett Jones. And they understand the flaws of the voter.\\n\\nBrian Chau\\nThey understand the flaws of when you go and cast your ballot for Donald Trump and Joe Biden, that's not necessarily the best thing that, you know, that they're not necessarily going to be doing the best thing for the world. But you go one level down the stack to the legislative level or to the administrative level, and they do not see the exact same incentives in play. Where when you pass a bill, for example, when you pass, like, the authorization of the use of military force. Right. Or when you pass, like, when you pass, like, the budget reconciliation bill, what factors go into play in terms of getting your policy priority into that bill? And what, in what ways will be corrupted by the process, by the requirements and the incentives for it to be there in the first place?\\n\\nBrian Chau\\nAnd, you know, the short answer, this is something very funny that I posted about recently and got a lot of support on Twitter for, is that, like, there are some ideas that are so correct that even the most straw man version of them is true. So the extreme straw man version of my opinion is, like, or of this. Like, it really, like, not my opinion originally, but, like, the field of political economy, the extreme straw man of it is that, like, whatever idea you want to get into law will be unrecognizably corrupted once it is in law. And even that, like, extreme straw man is, like, pretty much true. You just look at, like, why don't.\", \"Interviewer\\nYou call it an extreme straw man? You're like, this is what my opponent's like. It's true. Even the most insane of them.\\n\\nBrian Chau\\nYeah. Yeah. That's. That's the best thing about it. Right? Like, even, you know, even, you know, the most fervent supporters of law, you know, you ask them, you know, you ask them, like, what happens? What do you think this law will look like after it's pastel sale? I don't know. I just hope it's. This is also, like a take that I want to. I want to, like, specifically address. If we're talking about addressing EA takes, is that I've had multiple people that I really respect say, like, oh, we want the government to interfere because it will just slow down progress. We just want it to cause harm. And as long as it's causing harm, it will reduce the probability or the speed that we get agi. This is also something that I don't necessarily think is true.\\n\\nBrian Chau\\nAnd the best counterexample in recent memory is gain of function research. Right? So we have this phrase called like anarcho tyranny. Usually people talk about in the context of SF, which is like the bad actors are not punished because they're outside of the system. The only people who are punished are like the good actors in the system. Right? So you're punished for like protecting your convenience store, but like the people who rob your convenience store are rewarded by the state. And you see the exact same thing in regulatory capture. You see the FDA, you know, very varies. You guys, you guys actually have had experience with this, right? With going after novel technologies that have some promise, but they will also, you know, this is not the FDA, but the US government will also fund gain of function research in Wuhan.\\n\\nBrian Chau\\nSo the question is not, you know, this like one singular lever of what it does to the industry. It is much more of the question of like how it influences the distribution of players. And the most likely thing that will happen with the distribution of players is that, oh, actually it's just OpenAI, Google, Facebook, so on or not really so on. It is really much just in this case, in some regulatory capture cases, they're morbid. In this case, it is really just the major named players.\\n\\nInterviewer\\nSo let's say that you are successful with this beyond your wildest dreams. How will the world be different? Is it a scenario in which rather than there being like three players who are setting the tone for AI, it's a little bit more distributed, like a lot of people are contributing, it's more open source or what kind of environment or ecosystem are you trying to create?\\n\\nBrian Chau\\nSo many people. This is a very fascinating critique of political economy, because if you look at these incentives, they rarely ever change. So there are cases of just being like a doomer, of people saying like not an AI doomer, but people like saying, oh, like the regulatory crackdown is inevitable. But you just look at the history of the United States and that's just not true. The best example is you guys remember SOPa, like the stop or something like online. Yeah.\\n\\nInterviewer\\nThey were trying to stop the Internet nonsense.\\n\\nInterviewer\\nYeah. Wow.\\n\\nBrian Chau\\nSo like this was a case, I think it was the, it was, I forget which agency. There was some agency was trying to, I forget the exact name of it, but it may have been, okay, whatever. But, but basically they were trying to do this restriction of content on the Internet, basically saying that, you know, like if any like random commenter said or like linked to porn or something like that, then it would be, you know, then the entire website would be subject to legal crackdowns.\\n\\nInterviewer\\nRight.\\n\\nBrian Chau\\nAnd people just wrote in, people got media attention. It became like, it became like this huge, like, unification of the Internet was one way that it was described. We are basically unanimously. Everyone was like, this is a terrible idea. This is just, you know, this is just disastrous. And we stopped it. You know, we had a w. And there are instances where there are calls for regulating an emerging industry, and for one reason or another, it just doesn't happen. You can look at the Internet, for example. You know, this is in the broader history of the Internet. Mark Andreessen actually gave me this example. Right. So Marc Andreessen, you know, like, he talked about. He talked about this on my podcast. He's been on several others, but it should be. Should be out by the time, you know, this releases.\\n\\nBrian Chau\\nAnd he talked about nowadays, what if you try to ban the Internet now? We've created a successful political constituency. If your industry is big enough, which I think machine learning will be, I think even though we won't get artificial general intelligence, we will get many commercial applications. We have many commercial applications now. Once all of those are adopted, once all of those are regular parts of people's lives, if it's big enough, like the Internet is, and like, I think machine learning will be, then you've created a political constituency. You know, all you have to do is wait for the existing adoption curve to happen.\\n\\nInterviewer\\nSo you're saying that you're trying to broker that transition to, like, get to a place where adoption is wide enough where, like, the market will handle the protection because they are dependent on it in a huge. Right.\\n\\nBrian Chau\\nMarket. It's just, you know, like, if half your country is using chat, GPT, or using some form of LLM.\\n\\nInterviewer\\nRight.\\n\\nBrian Chau\\nYou're not banning it. I'm sorry. You know.\\n\\nInterviewer\\nYeah. Like, no one will get reelected if they support any legislation that does that. So basically, before that saturation has been reached, you are trying to ensure that we don't preemptively make that impossible. Right?\\n\\nBrian Chau\\nYeah. Now. Now is the most important reason for exactly. Or now is the most important moment for exactly that reason is because it's. It's the only, you know, it's the period of time in which it is most politically vulnerable, but most economically has the most economic potential.\\n\\nInterviewer\\nYeah, that's meaningful. Are you concerned about all the people? Because it sounds like you're not really fighting against eaers who are making weird arguments against AI. You're more fighting against legislators who are like, well, but, you know, they might have much more, what you might say, like Normie arguments against it. Right. So they're going to say, well, what about the fact that AI is going to take jobs away? You know, what are you going to say to them?\\n\\nBrian Chau\\nI want to take an interlude between this. Most of the people who are actually proposing like these, like legislative crackdowns or very anti EA, you know, they are anti, like one of these people like Timnit Gebru, who is like this race grifter who is now like focused on machine learning, is they, like, she absolutely like despises EA for like, honestly, like pretty pathological reasons. Like it's not helpful for her for, in any way to dislike EA. You know, like they could easily be, you know, like legislative allies, but she, like, you know, but she just absolutely despises them. You know, loves calling them racist because they believe in IQ and they believe.\\n\\nInterviewer\\nIn like, okay, so that general approach.\\n\\nInterviewer\\nYeah, yeah.\\n\\nBrian Chau\\nLike a lot of the most despicable people, like, also hate eA. And like this is, you know, it's no longer just interest. This is no longer a battle of like Internet shitposters. You know, this is a battle of real political interests and the forces that drive the democratic and republican parties.\\n\\nInterviewer\\nWhat's your rebuttal to their arguments? Because those aren't EA arguments. So what is your rebuttal to this is going to take away jobs. It's, it's dangerous and we don't need it, so why should we support it, etcetera.\\n\\nBrian Chau\\nThe jobs point is particularly interesting because it's kind of like framed as a republican concern. And it's very, it's very funny. It actually relates to the other discussion that we've had or the discussion in the future that I have some premonitions about. Yeah, in many cases, AI, the things that AI are replacing are kind of bullshit jobs. They're things that people already dislike. You know, people do not. There's this wonderful tweet by Sam Altman that says, you know, today I've had one person tell me that they've used chatgpt to expand their bullet points into a long corporate email and another person tell me that they've used chat, GPT to condense a long corporate email into five bullet points. It's the future of communication.\\n\\nInterviewer\\nI also love that it's done such a good job of getting rid of these useless grifters, like artists and writers, because honestly, they had these pointless degrees. They were a huge, I think, cause of our society's degradation. And I'm so glad that AI has replaced them.\\n\\nBrian Chau\\nThere's okay. Like, there's a real thing underlying that is that, like, mid is over, right? And here, like, actually mid.\\n\\nInterviewer\\nI don't mean, you know, that way of putting it.\\n\\nBrian Chau\\nIt's very funny because people portray, like, the most famous people portray, like, Drake being replaced by AI. It's like, no, he has, he still has interesting things going on. Or, like, Taylor Swift. Look at Taylor Swift's, like, ticket sales. You know, like, she's not worried about this. The thing, like, the top, like, the people who are actually contributing to the culture that we consume every single day, they're just gonna be fine. In fact, they're going like, the next generation. Sam woods was on my podcast. He had this wonderful line, which is, your job's not gonna be replaced by chatgpt. It's gonna be replaced by someone using chat, GPT. I think that, you know, in the future, like, the meta of art will be very much, and I think, like, the top artists today will be able to adapt.\", \"Brian Chau\\nThey have that kind of, like, entrepreneurial focus. Taylor Swift is once again a great example. Like, I think she'll really enjoy, you know, playing, and her team will really enjoy using the new tools to discover, like, the frontier of music that I think what you're just counting here, things that people never.\\n\\nInterviewer\\nShe represents 1% of people. Like, the vast majority of people have not done anything.\\n\\nInterviewer\\nGrimes has adapted.\\n\\nBrian Chau\\nYeah, exactly.\\n\\nInterviewer\\nI think that you are overestimating the competence and the aggressiveness of this top, you know, 1% of society, and that for a long time, they haven't been pushed out. Like Quentin Tarantino, like, digital cameras come along. He's like, I'm not gonna touch them. Even silly doesn't do them. A lot of the drop. Most of our history, you've been able to get away with that kind of b's, that kind of arrogance, but I don't think you're going to be able to now.\\n\\nBrian Chau\\nYeah, there is the innovator's dilemma. I think you're right, actually. I'll say more like, I think that non zero of the current top artists will adapt in terms of, like, one specific one. Yeah. Like, you know, I'm not 100% sure that Taylor Swift will be, you know, will be the one who is like, yeah. Who is taking up all these new tools. But I think that. I think that it will be, you know, it will be a hybrid that I'm very confident about that's, you know, the mainstream of art, the mainstream of culture, the main stream of film that those will all. You know, it won't be. It won't be completely AI generated. It won't be completely human generated, created from scratch. It will be some combination of the two. Just like, you know, we had digital cameras.\\n\\nBrian Chau\\nPeople, you know, people are adapting to digital cameras. We had the Internet. We had social media. We now have a mix, right? You can think of Netflix as a mix between the original film model and YouTube, right? Garrett Jones has this amazing term like spaghetti and spaghetti assimilation, which is he uses this in the context of immigrants. So, like, Italians come to America and, like, or, like, they come to New York and they make New York. You know, they become more like New Yorkers. But New York's culture also, you know, they start eating pizza and they start eating spaghetti, they. It becomes more italian. And I think the same is true of AI. You know, our current culture will become more like AI. It will.\\n\\nBrian Chau\\nThere will be, you know, I think, like Sam woods put it best, you know, the replacement of jobs is not really going to be, you know, like, vertical. It's not really going to be like, people being completely replaced by AI. It's going to be a new skill set. People are learning to do something better. And, you know, the people who will do that better are going to be the people who use AI.\\n\\nInterviewer\\nI think, though, that what's understated is that a huge portion of knowledge workers, and in that, I include people like sales, marketing writers, artists, designers, salespeople, like website people, have been doing work that isn't actually used. Like, I think we've had this period of inertia where people are still hiring and paying and thinking that they need these people even before AI and actually not using the vast majority of work they do. And that there are busloads of graduating classes that believe that their job is to sit behind a computer and write strategy documents and analyze things, but not actually build or create anything. And that these people are going to get laid off. They're getting laid off in droves. They're not going to get rehired. They're going to have to figure out their own way.\\n\\nInterviewer\\nDo you think that those groups are capable of building new lives for themselves when they've been conditioned to do something that is completely different like this?\\n\\nBrian Chau\\nSo this is an interesting question. This is an interesting, like, economics question. But I do want to, like, mess with the framing a little bit.\\n\\nInterviewer\\nGo ahead.\\n\\nBrian Chau\\nI think, like, this is not that related to AI. This is, like, related to low interest rates more than just related to AI. Like, kind of like, it is like these things happening at the same time. But like, you know, you could easily see a world where, you know, OpenAI just develops or like all OpenAI and its competitors just develop and release everything like a few years earlier. And we have AI being released to broader society in like a, you know, user friendly way at the same time as like the crypto hype bubble. What would the vibes be around AI then? You know, if it's like the tail end of the lockdown, crypto stocks are going crazy, you know, and then OpenAI publishes like current level chat, GPT. What are the vibes then?\\n\\nBrian Chau\\nAnd I think it will be a vibe of like, just much more optimism. It won't be a vibe of like, complaining. It won't be a vibe of like, you know, and of course, this is not really an argument for my position, but it's an argument against, you know, I think it's an argument against much of the contemporary. This is like not the EAs, right? I think the EAs would still be worried, but for like the people worried about jobs, for the people who are worried about, you know, like automation, for the people who are worried about, you know, basically collapse, I think that's much more kind of absorbing the more general economic environment than it is an actual concern about AI.\\n\\nInterviewer\\nYeah, well, what I like about your view is that it's actually quite optimistic, which is super, not Gen Z. Like, I really like that. It's eminently reasonable. It's like, actually this problem will solve itself. We're going to have the critical mass of machine learning users, essentially, that are going to make sure that it doesn't get walled off and made very difficult to open source and collaboratively develop. I'm just going to help to bridge the gap with this, and I think that makes me super intrigued to see how it goes for the alliance for the future. I'm really glad that you are in its foundational team and doing this work, and I'm keen to our listeners, if.\\n\\nInterviewer\\nYou'Re interested in this, check out the alliance for the future. Check out Brian's podcast as well.\\n\\nBrian Chau\\nYeah, affuture.org is how to find us. We would really appreciate donations at this early stage. And yeah, you can check out all of my writing. You can check out my writing on AI, specifically at pluralism AI. And you can check out all of it, including the podcast at from the new world.\\n\\nInterviewer\\nThank you so much, brian. This is, it's always really fun to talk with you.\\n\\nBrian Chau\\nThis was very fun. It was not, you know, it wasn't 4 hours, but it was, you know.\\n\\nInterviewer\\nSee, we don't have attention spans for that. We're, like, fast. But, you know, that's you guys on.\\n\\nBrian Chau\\nThe podcast for 4 hours.\\n\\nInterviewer\\nSo for people who want to see, our podcast was him. We talked to him for, like, 4 hours each, and I was hammered when I talked to him.\\n\\nBrian Chau\\nCompletely hammered.\\n\\nInterviewer\\nLike, 09:00 a.m. At night going on until, like, 01:00 a.m. In the morning. It was enjoyable.\\n\\nInterviewer\\nIt was more like. I think it was like three until 08:00 p.m. But for Malcolm, whose day starts at 02:00 a.m. That is, like, extreme.\\n\\nInterviewer\\nNo, it was late. Yeah, I went past midnight. I think was recording.\\n\\nBrian Chau\\nIt went. It went pretty late, I don't think. Yeah, I'm not sure if it was midnight.\\n\\nInterviewer\\nMy brain. It was past midnight.\\n\\nBrian Chau\\nOkay.\\n\\nInterviewer\\nCheck it out. He has a great show. He really knows how to pull things out of people.\\n\\nInterviewer\\nYeah, yeah. You listen. Amazing questions. You're an amazing interviewer. So, yeah, check out from the new world, but also the alliance for the future. Thanks again, Brian.\\n\\nBrian Chau and Marc Andreesen\\n\\nMarc Andreesen\\nMy mental model, having kind of fought various wars like this over 30 years, my mental model is basically persuasion doesn't happen. It's nobody actually, like, why write the essay at all?\\n\\nBrian Chau\\nRight?\\n\\nMarc Andreesen\\nYeah, because you can create a movement, right? So you can't persuade, but you can create a movement. All of the anti crypto energy, not all of it, but a large amount of it that was like running like white hot, like six and twelve months ago. It's now been diverted onto anti AI. These things that look like they're kind of overnight breakthroughs, they aren't. What happens was there was a backstory, right? Backstory usually was. The way I read the history is usually 30 to 40 years of prior attempts to make the thing work that didn't happen, that, you know, that basically did not deliver on with AI, it's actually an even more extreme story. It's an 80 year. It's an 80 year backstory, right. The original neural network paper was published 80 years ago in 1943.\\n\\nBrian Chau\\nHi. Hi. Welcome, welcome. This is the from the new world podcast. Today I'm speaking with Mark Andreessen. A very exciting episode for me and hopefully for you, too. He is the founder of Netscape, of the venture capital firm a 16 z and one of the most prominent commentators on technology, on theories of change, and now on AI. We'll dive right into that topic, discussing AI, discussing possible scenarios, the failure of conservatives to find any kind of political position on this issue, long term theory of innovation, why startups succeed, why startups fail, and whether that translates, whether a translation, whether a change in the economy translates to a change in people's lives and in people's culture. If you like the episode, the number one thing you can do to help the show is to let a friend know, either in person or online.\", \"Brian Chau\\nNot only are you helping us out, but hopefully you're finding someone. You're helping someone find something that's enjoyable, that's interesting, that's informative, and that helps both of us. Some listeners might ask why I didn't go further into criticisms of Marc Andreessen and, you know, listen to the episode, listen around, you know, 15 minutes in or around the end, we discuss political strategy. We discuss what the point even is of going on media tours like this. And there I think you'll find the obvious answer. Anyways, more on this in the post podcast reflection, which you can get if you subscribe to the substack linked below. Without further ado, here's Mark Andreessen. So, first big thing, the news item of the day, meta open sourcing its LLM. Do you think this is the way to go?\\n\\nBrian Chau\\nDo you think this is a positive or negative sign compared to other AI companies?\\n\\nMarc Andreesen\\nI mean, you know, I'm hugely in this. I should start by, you know, kind of declaring, right, I'm on the board of meta and so I'll be cautious about what I'll say about them as a company. But also, you know, my firm, Andreessen Horowitz, we endorsed this. We actually have the llama two model already running up on a service called replicate, so people could use it literally in the first 30 seconds of the announce, which was great. And then, yeah, my firm is an enthusiastic endorser supporter of this. Yeah, I mean, look, I just think overwhelmingly, obviously, clearly this is an incredibly positive move. And we could talk about sort of many different aspects of that, but one is just, it's a big advance in functionality. So it's a, the big model is a 70 billion parameter model.\\n\\nMarc Andreesen\\nIt was released in full, released in both pre trained and fine tuned versions. It's a level of power and capability that has not been available to developers up until now. So it's going to be really exciting to see what people do with it. Look, truly OpenAI AI available to everybody, I think is just as important as electricity available to everybody, or microchips, or operating systems or the web.\\n\\nBrian Chau\\nYeah, it's a great way to frame it. Right. But I think for a lot of people in this audience, they'll ask open source, why does it matter?\\n\\nMarc Andreesen\\nYeah, so it matters a couple. So it matters, it normally matters for kind of, sort of an abstract but very important reason, which is something called permissionless innovation. Right. And so it matters because it basically unleashes the creativity of humanity against a new technological domain as opposed to just, you know, having, as opposed to just having basically oligarchic big companies able to do things with technology. And that's a sort of well established dynamic. More recently, I think open source for AI matters even more than that because, as you well know, there's this quote unquote, AI safety movement or AI alignment movement, and there's a lot of different puts and takes on that. But my interpretation of that is the people doing that are basically gunning for two things that I view as very bad.\\n\\nMarc Andreesen\\nOne is the application of the so called precautionary principle to AI, which I just think it's just a catastrophically bad concept. For the same reason it was a catastrophically bad concept when it was applied to nuclear power 40 years ago. And then the other is just, do we really want to live in a world in which a small number of big companies achieve regulatory capture and form a cartel amongst themselves and with the government and basically control the future of technology? That is the push from a bunch of big AI companies in Washington right now. And the best way to prevent that from happening is to just have a faded company and have AI just available for free to everybody. Everybody.\\n\\nBrian Chau\\nRight, right. It seems like definitely under the american regulatory standards and the american constitution, I think open source AI would be much harder to regulate in the EU. They're just, they're just taking the totalitarian hammer, right? Have you seen this? Have you seen the EU is AI act?\\n\\nMarc Andreesen\\nYeah, it's, you know, and of course, this is not the first round of these with the EU. Right. And so it's. And, you know, they. In the sort of. In the old days, in the old days, totalitarian regime teams were basically really rough and kind of very violent and deadly. The new form of this sort of soft totalitarianism that you get in the EU is this weird combination. It's like the security blanket kind of thing that becomes increasingly suffocating and strangling. And it shows up in issues as silly as cookies, pop ups everywhere, all the way up to and including, literally, bans on AI. And of course, bans on AI mean in practice, bans on math, which just gets, like, increasingly absurd.\\n\\nMarc Andreesen\\nAnd so the EU has kind of been on this path for, you know, for decades, and they're characterized as kind of their level of emotional rage at a changing world and the United States and tech and capitalism is, you know, is intensifying in kind of increasingly bizarre and dangerous ways. So. Yes.\\n\\nBrian Chau\\nYeah, right. So some of that, I don't know, you've looked at some books that I think are popular among my audience. Something like James Burnham. Right, machiavellian. We say that we have basically, we have First Amendment protections, and sometimes the Supreme Court does uphold those protections. But I don't know. Are you worried that with AI and with the obvious kind of demand for, with a lot of special interests wanting to kind of crack down on AI, do you worry that there will be basically measures taken either by, you know, creating new administrative agencies or otherwise that will basically, you know, violate the First Amendment and do it anyway?\\n\\nMarc Andreesen\\nYou know, the first thing to notice that, you know, the first. The First Amendment, unfortunately, right. Only applies to the United States. Right. And so it's actually a fairly. Yeah, right. This is actually something I actually didn't realize until I just hadn't paid enough attention to kind of legal, you know, kind of legal history. But, you know, in the social media wars, you know, that I've been, you know, involved in a long time. You know, I discovered, you know, several years back that, you know, the other anglophone countries, right, the UK, Canada, New Zealand and Australia actually do not have constitutional free speech protections, right? Which is, which is fairly amazing. You know, they may have some history of allowing it, but they have no such protections.\\n\\nMarc Andreesen\\nAnd of course they have increasing legal regimes that, you know, fly directly in the face of, you know, such a protection where they have it. And then look, in the US, you know, we sort of have free speech. The practical reality is that governments at all levels have tremendous power, and they have tremendous power both when they do things explicitly as well as when they do things implicitly. And the kinds of pressure they can bring to bear among companies and individuals, even in what looks like an obvious violation of constitutional rights, is quite profound. And in theory, the courts will protect you against that. Except two issues with that. One is the courts themselves, of course, get politicized. So it depends which court you get. And then the other problem is just cases can take a long time.\\n\\nMarc Andreesen\\nAnd so in theory, you can have a good case, you know, that your rights are being violated, but it could take you five or ten years and tens of millions of dollars to actually litigate all the way to the Supreme Court to actually get a ruling. And so these, I say, I've come to a heightened appreciation of the founders of the American Republic and to the Constitution because, like, these rights are tenuous, right? And they're, you know, they're tenuous historically and they're tenuous now.\\n\\nBrian Chau\\nRight? Right. You see, it's interesting. There are two camps. There's the kind of, if the constitution is going to be so blatantly violated, you know, if you're going to have such precedents such as the general welfare clause, basically, I think many people saying basically, like, running roughshod over what constitutional. Over the majority of what constitutional protections existed before. Then, like, why does it even matter? And then there's the second camp, which I think that both of us are more in alignment with that, basically, you know, like, it's better than nothing, right? So some amount of affordance, some amount of explicit protection is definitely better than not having it at all. And these things kind of work themselves out in weird, you know, weird real politic ways.\\n\\nMarc Andreesen\\nYeah. Look, if the Supreme Court in any country if the Supreme Court can't ultimately, like, control the rest of the government, you know, when it breaches whatever constitutional protections exist, you know, then I think you are well and truly lost. And so you'd definitely rather have those protections. It's just that, you know, they don't always apply in the moment. Right. And I'll just give you this sort of obvious example. This just like shocked me, you know, just like to ridiculous levels in the last decade, which is, you know, the government cannot, you know, people always say this, the government, you know, constrains Congress. Right. The First Amendment constrains Congress. The first amendment sort of binds Congress from passing laws that violate us constitutional protections for free speech.\", \"Marc Andreesen\\nAnd so there's this very interesting thing the government has figured out how to do in the last decade, which is they outsource it. Right. They figure out. Right. Whatever regime is in power, they figure out what speech they want to restrict, and then they outsource it to, quote, unquote, private entities. Right. Many of which are government funded. Right. And then those private entities can go out and restrict freedom of speech, and then the people doing it can say, oh, look, it's not the government. In the fullness of time, I think what's going to happen, I think what we're headed towards is the Supreme Court case where that is going to be challenged and there's going to be a case in the next, I would guess, two or three years.\\n\\nMarc Andreesen\\nIt'll take a little time, but probably in two or three years there'll be a case that basically where the court basically says the government cannot outsource speech restrictions that would be illegal if it did them to a third party and certainly not in the case of a third party that it is paying for. But I think that has to actually be a case that actually has to be ruled on. Quite frankly, I think people need to go to jail. I think there have been felonies committed and there are actually laws that cover this that are not being enforced. But look, there's been almost a decade of this in kind of plain sight with the social media companies.\\n\\nMarc Andreesen\\nAnd yeah, in the fullness of time, the Supreme Court hopefully will rule on it, but it's going to have been ten or twelve or 15 years of sort of blatantly unconstitutional activity before it finally gets ruled on.\\n\\nBrian Chau\\nYeah. So there's actually a case very related to this. It's not a Supreme Court case, not yet, but there's this lower court ruling on, I think, against the FBI specifically that essentially barred it. And similar government agencies from contacting social media companies. Right. I can find that now. But, yeah, like, this is definitely coming down the pipe. This is going to be an actual. This is going to be an actual thing. And then I think, very. In the very near term.\\n\\nMarc Andreesen\\nYep. Yeah, I think that case is a good example. We'll see. That case might go to. That's the kind of case that you can imagine going to the Supreme Court, and you can imagine certain justices on the Supreme Court being very enthusiastic about that happening.\\n\\nBrian Chau\\nRight, right. Certain justices that we will not make laser memes. Laser eye memes out of.\\n\\nMarc Andreesen\\nExactly.\\n\\nBrian Chau\\nOkay. So let's get. Let's get back to the. Let's get back to the point, though. So, like, you know, I think both of us agree there's enormous potential in AI. You know, why do people want to ban it? What's your best kind of summary of why people want to ban it?\\n\\nMarc Andreesen\\nYeah. So the thing I always look for confronted this kind of question is, what happened historically? So what happened when other technologies came out? And I did a full kind of undressing of that a while ago reading for my own benefit. And the conclusion I came out with is actually based on this book called men, machines in modern times, which turns out to be this classic book written on exactly this topic, written by an MIT professor about 55 years ago, predating all these issues. And he does this extensive study of the release of new technologies and sort of the level of institutional and governmental resistance to them. And the conclusion he reaches is the one that I think is correct, which is basically what happens is new technologies represent a shift of power and status.\\n\\nMarc Andreesen\\nBasically, society at any given moment has power and status hierarchies that were determined by technological shifts that took place in the past. And so shifts ranging from gunpowder, you know, to electricity, to the printing press, radio, television, and so forth. And, you know, the current people in power at any given time inherited those power and status relationships from the battles that have been fought in decades or centuries earlier. But, of course, you know, anybody in position, in a position of power or status views that power and status as a. You know, as a. Defined as a divine right and something that should never change. And then a new technology comes out and threatens to upend that. You know, basically that power hierarchy.\\n\\nMarc Andreesen\\nAnd then you get a natural reaction where people get, like, incredibly vividly mad, and they get mad in ways that they'll actually behave self destructively. They'll actually refuse to adopt new technologies that would actually beneficial to them. And certainly to the people that they represent because their own power is threatened. The example that this guy Elton Morrison uses in this book that's so striking is the famous example for this now, which was the invention of the first naval gun that automatically compensated for the role of a ship battleship on the ocean. And this gun basically increased the accuracy of firing of naval cannons by Ten X, because gunners prior to this gun had to try to hand correct and predict the very unpredictable role of the ship. Most cannon shots missed, you know, naval battles more than 100 years ago.\\n\\nMarc Andreesen\\nShips would be next to each other and they'd be firing each other for a long time, and most of the shots would miss because of this. And so this gun comes out, it increases the accuracy rate to almost 100%. And both the US and the UK navies fight it for a generation, like they just simply will not implement it. And ultimately, the inventor of the technology, Sims, ultimately has to appeal directly to President Roosevelt, President Teddy Roosevelt at the time, to get the Navy to take it seriously. And the re. And the re.\\n\\nMarc Andreesen\\nAnd the reason he reconstructs the entire history, and the reason is the entire doctrine of naval war and the entire management structure of a naval vessel and the entire function of gunnery officers was all wired, assuming the old technology and the new technology meant that a new generation of more technologically sophisticated gunnery officers was going to take over, and then officers trained in the new methods, and if you were trained in the old school, you were irrelevant. And if you're trained in the new school, you were going to run everything. And so Morrison has this framework. He says there's a three stage framework for any new technology that threatens to upend power structures. He says step one is ignore. Just like, talk to the hand. It's not happening.\\n\\nMarc Andreesen\\nStep two is what he calls rational counterargument, where they field every conceivable reason why the new thing can't work, and they bring out all their existing experts to try to do that. And then step three, he says, is when the name calling begins. It's when the fight gets really vicious in AI. Right. Now, I would say we're somewhere between step two and step three, right?\\n\\nBrian Chau\\nI don't know. To me, like, rational argument sounds good, right? That's kind of what I want to see. Maybe that's not the context in which he uses it, right?\\n\\nMarc Andreesen\\nWell, no, but the problem with rational arguments is, like, okay, because, well, first of all, you get this endowment effect, right? Where the people making the rational arguments are the experts, right? And so, you know, who gets taken seriously in the realm of rational arguments is the people with the credentials, right? I mean, it's the same thing we saw in COVID, which is all these people coming with public health degrees and all these things kind of roll out and all of a sudden they're the presumed experts making the rational arguments. You know, later on you're like, well, wait a minute, that didn't make any sense. But in the moment, you know, they're the people who get taken seriously and then, look, you know, look, rational arguments, you know, in the abstract are a good idea.\\n\\nMarc Andreesen\\nYou know, they, as you know, they depend very much on the assumptions that people bring to the table. And, you know, it is, you know, there's this old question, right? There's this whole question is, you know, kind of who's easier to fool, you know, kind of a dumb person or a smart person, right? And the normal answer would be a smart person. A dumb person is easier to fool just because, you know, they're dumber. But it turns out smart people are really good at creating sort of very advanced, you know, kind of multifaceted, abstract theories for why they're correct. Right. They're very good at cherry picking evidence. They're very good at rationalizing away disconfirming evidence. And so when he says rational arguments, I think that's what he means.\\n\\nMarc Andreesen\\nLike the ones that look like the rational arguments, you know, ten years or 50 years later, you're like, okay, what were they smoking? But in the moment, like, it was, you know, it's a wall of quote unquote expertise that you have to go up against.\\n\\nBrian Chau\\nYeah, I think this is like the resurgence of Hayek, right? This is the resurgence of people basically saying, you know, you pretend that all these things are 100% predictable. Really. They're much more dependent on kind of a type of social evolution.\", \"Marc Andreesen\\nYeah. And then incentives, you know, again, maybe obvious, but like incentives rule the world, right? And so if you have an expert who has built their entire career in a previous, you know, paradigm, you know, the, you know, what was the famous line of science advances one funeral at a time, you know, the reputation. I just read this amazing book, by the way. If you haven't read it's incredible. It's called when reason goes on holiday. It's the southeast asartic. And it's one of these rundowns. It's one of these rundowns. Basically that's what happens when a philosophers become involved in politics. And of course the answer is things go catastrophically wrong. But he cites example after example of that in that book of philosophers and scientists who kind of stray into domains in which you would think that expertise would apply, but it actually doesn't.\\n\\nMarc Andreesen\\nAnd the, you know, his capstone example is Einstein. It turns out Einstein was a Stalinist. Right? Like full on, like full on Stalin supporter. Right. And so. But, you know, he was Einstein.\\n\\nBrian Chau\\nI think, like, when you're so kind of theoretically or logically ahead of people, you kind of gain an overestimation in central planning. I actually had. Do you know who rune is?\\n\\nMarc Andreesen\\nOh, yeah, I do. Sure.\\n\\nBrian Chau\\nYeah. Yeah. I had him on this podcast, I think, like a year and a half ago. And, yeah, I think he reflects, like, actually, this was him at the time. I'm not sure if he still believes this, but, you know, he was saying, you know, like, AI will make central planning way easier. I think a lot of, like, AI people think that this is the case, that when you get, like, a great leap in complexity, you know, one tool, which is what AI is, that suddenly that'll make central planning a lot easier. I see this pattern a lot in history and yeah, it's like, no, but the world also. The world complexity also updates. Right? This is where your Hayek comes in.\\n\\nMarc Andreesen\\nThat's right. Yeah, that's right. I would argue the exact opposite, which is especially with truly OpenAI, like today's announcement. So basically, you need to picture a future world in which everybody has AI, which is a world in which everybody has basically augmentation that does not make them directly smarter, but effectively does, because it gives them a set of tools that make them much more effective in whatever they're trying to do. And so you're going to have a world of 10 billion people that are going to all have access to these tools. That world is going to be vastly more complex than the world we live in today. There's no question the central planning dream never dies. The central planners will attempt to do the central planning. It's just, they will be confronted.\\n\\nMarc Andreesen\\nIt's like if you chart the curves of the intelligence of the center versus the intelligence of the edges will be rising much faster. And so the fallacy, I'm convinced, will, you know, it will remain a fallacy for, I'm sure, the rest of my life. And I honestly can't even imagine a world in which it ever shifts the other way.\\n\\nBrian Chau\\nRight. I think, like, Sam Hammond has this wonderful line where he says, imagine a world where every person has the capabilities a CIA agent has today. Right? Like, is that a world where it's, you know, easier to impose totalitarian crackdowns are harder. Right. It's definitely, it's definitely harder. And that gets to, like, that gets to my question now, which is it seems like, you know, like I can't get too angry at, like, bureaucrats who hate technology because to me, that's like kind of their rational interest. Right. Maybe not in the long term. Right. If they have children, it's probably not in their children's interest, but, like, maybe in their narrow terms, that's true. Why, why aren't, like, why aren't conservatives more into defending AI?\\n\\nMarc Andreesen\\nYeah, I think it's a combination of things. So one is there's this general just asymmetry, which is the consequence of the way kind of intellectual life works and then credentialing around intellectual life works. So my take was basically, quote, all the, quote, unquote, experts are one side. And, you know, some of that, I think, is, you know, the, you know, kind of the left is, you know, is characterized personality, you know, in terms personality traits by being open. And so kind of all, you know, people inclined towards kind of intellectual kind of adventure, you know, are going to tend to cluster on the left. Obviously, the credentialing institutions for expertise are heavily on the left. You know, the political party that's the most kind of enamored by expertise is on the left.\\n\\nMarc Andreesen\\nAnd so, you know, you just, you kind of have this thing where, like, you know, sort of technical expertise tends to cluster one side, you know, and by the way, and that doesn't mean that the other side is, like, wrong on everything. It just means that they, you know, if it's going to be a battle of experts, you know, especially credentialed experts, they're generally going to be mismatched. I always think about Peter thiel's kind of quote about Trump, which is you need to take, he always said you take Trump seriously but not literally. And I always think that's kind of true. Like, you should take experts on the left kind of literally but not seriously. And you should take the people countering against them, arguing against them on the right seriously but not literally. And then things start to make sense.\\n\\nMarc Andreesen\\nAnd so I think there's some of that look, quite honestly, I think also a big part is, I think a lot of conservatives just view tech, the tech industry generally as being a thing of the left. And so they just like, you know, which, by the way, if you look at voting records and political donation records. Like, it's a pretty strong argument there. And so they just kind of reflexively don't like it. And then look, you know, conservatives have their own counterarguments, right? Conservatives, you know, generally are, you know, the whole nature of conservatism is to be, you know, sort of, you know, let's just say cautious, you know, about changes. And so, you know, they have their own counterarguments, certainly about technology and about the rate of change in society.\\n\\nMarc Andreesen\\nAnd so I, yeah, I, you know, they're more natural allies than, you know, at least modern leftists, but, you know, they're not necessarily fully natural allies to technological change, even just on their own merits.\\n\\nBrian Chau\\nYeah. I think the big thing here is that, do you know this phrase, like politico did a recent, like, kind of hit piece on it, but, like, this has been a phrase, like going around some right wing circles for a while. Do you know the phrase, like, do you know what time it is?\\n\\nMarc Andreesen\\nYep. Yes.\\n\\nBrian Chau\\nYeah.\\n\\nMarc Andreesen\\nYeah.\\n\\nBrian Chau\\nIt feels like, it feels like conservatives who fall in that camp, they should be, I don't know, there's this kind of, you know, there's this kind of argument attributed to Peter Thiel often, which is basically, you know, like capitalism needs to outrun, you know, basically totalitarianism, right. It needs to continually create new ways of innovating before all the old ways of innovating are being banned. And, you know, like if you ever fail this, you know, there goes your society. I do think there's an increasing, you know, maybe not like adoption of that argument, but a kind of, you know, even if they're not explicitly pro tech in that way, in the way that I think we are, they're kind of like they get the argument. They, and it's convinced them at least not to be actively antagonistic towards it. Right.\\n\\nMarc Andreesen\\nYeah. Well, I think, look, I think if you're on the right and you've, if you've watched what's happened with social media and censorship in the last decade, like you should, at the very least, you should know what time it is, right? Like it's very vivid, like what's happened, right. And it's all, of course, the details are all spilling out in public. And then, of course, we saw a very similar kind of thing happen with the intersection of censorship and COVID over the last three years. And so, you know, I do think more people on the right are kind of aware of what you're saying.\\n\\nMarc Andreesen\\nAnd then, look, you know, there is, after all this time, there is a libertarian wing, you know, and, you know, the libertarian wing does, you know, like, you know, the problem with libertarianism is it doesn't have, you know, it never has mass support, you know, but it is advocating for, you know, for things that a lot of people like and want to have in their lives. And, you know, it is advocating for things that, you know, cause growth and it's advocating for things that cause greater degrees of freedom, you know, and the things it's advocating for are real. Right. And, you know, there, you know, new technologies often do have a liberating effect. And so, you know, yeah, so I, and I would always kind of center in on, okay, like, what's the actual truth of things?\\n\\nMarc Andreesen\\nAnd like, the truth doesn't always win, but it's a pretty good starting point, you know, to be able to actually process through these things. And it's like the anti AI arguments now. It's my sort of my feet. It's like, okay, we're going to ban AI. AI is already in use by 100 million people, right? Like, so 100 million people are like on chat, GPT or mid journey or whatever and they're like, you know, making their images and their videos and now they're, you know, they're doing their writing, their, you know, they're helping their kids with their homework and they're working on their reports for work and they're doing all the things that they're doing, you know, with this technology. And it's like, okay. And it's just like, so obviously, like useful and helpful and productive and positive, you know, in people's lives already.\", \"Marc Andreesen\\nAnd so it's like, okay, now we're going to ban it. It's like, no, you're not right? Like the argument that is not actually a correct and proper thing to do is not just an abstract argument, it's a very practical argument. And I think even people who would never consider themselves libertarians are going to be like, you know, okay, you're going to just like, take this away from me, you know, I don't think so.\\n\\nBrian Chau\\nYeah, but you look at the EU, it's very possible. You look at, I guess, nuclear energy, it never got rolling right. It never really, yeah, it never became obvious that it was integral to people's lives or would be integral to people's lives.\\n\\nMarc Andreesen\\nLook, that's the giant cautionary tale. I mean, that's the one I always, you know, I have friends who are like, Mark, stop talking about nukes. AI isn't nukes, you're going to scare everybody. And I'm like, no, we need to talk about nukes because like, the precautionary principle was actually invented by the german greens to prevent nuclear power, right? And then I always point to, you know, Richard Nixon in 1970, 119 72, around the time I was born, he proposed something called project independence, right? Which was the project independence was the US was going to build 1000 new nuclear power plants, civilian nuclear power plants in the 1970s. The US was going to cut over to an all electric grid, all powered by nuclear power, completely shut off fossil fuels, completely cut over to electric cars by 1980.\\n\\nMarc Andreesen\\nAnd so it was like, okay, there was that. And then he, of course, in parallel created the EPA and the nuclear regulatory Commission. And between the two of those agencies, they completely stopped nuclear power. But again, you fast forward 40 years, the future, and you're like, oh my God. The world that we could have lived in with kind of unlimited cheap energy, the dream of technology always is what we call too cheap to meter. The dream is just unlimited power for free, zero emissions. It's all good. That is a world that we could live in and we could be living in today that we chose not to. Now, the argument as to why this time is harder. It's harder to ban AI than it was.\\n\\nMarc Andreesen\\nTo ban nuclear power is because to build a nuclear power plant, like, there's a lot of real world things that you obviously have to do, including ultimately obtaining nuclear material. And so the government very effectively could stop that AI. My hope is AI is more like cryptography. It's code and math. And so now that the code today was a big step forward, the llama code is out. The llama code is going to be on the Internet forever. And then the techniques for building AI are in. Every math textbook, every math class, every university math course now is covering this material. And so my hope is putting it back in the box the way they did with nuclear power is not actually practical. Although, like I said, I think there's a good chance they're going to try. And the EU is trying for sure.\\n\\nBrian Chau\\nRight? So going back a bit, I think this is more of a question of strategy than in terms of accuracy. But I think there are kind of two arguments that can be made. Going back to, like, I agree with the framing here, like, going back to the second phrase, we're phase. We're in the phase of kind of rational argument that there's kind of two approaches to this, right? One is the kind of, you know, classically liberal approach of just saying, you know, like, here are the better arguments. I've tried to do that. I've tried to just point at kind of factual claims that eas make, which I think are just false. And then there is the kind of, like, sociological argument, right? There's, you know, look at these parallels across history. I think you do that in your article.\\n\\nBrian Chau\\nI think some eas were very annoyed by that, right? They don't really see, they don't really accept the kind of sociological argument at all in terms of both. In terms of persuasion, in terms of persuading people who are kind of AI doomers, and in terms of persuasion of people who are basically neutral, what do you think are the benefits and the drawbacks of these two strategies? And, like, any other alternatives that you want to offer?\\n\\nMarc Andreesen\\nYeah, so we may disagree on this. Like, my mental model, having kind of fought various wars like this over 30 years, my mental model is basically persuasion doesn't happen. It's nobody actually, like, why write.\\n\\nBrian Chau\\nThe essay at all, right?\\n\\nMarc Andreesen\\nYeah. Because you can create a movement, right? So you can't persuade, but you can create a movement, right? And so, like, people don't, I don't think people really get necessarily persuaded into arguments. I don't think they can really get persuaded out of them. Like, the human mind is not like we have this, you know, as, you know, the type one, type two kind of thing. Like, you know, we only engage in type two thinking on a very rare exception basis. And usually when it's something that is, like, really directly relevant to our immediate survival, you know, generally people are not rational, dispassionate, you know, we're not, you know, we're not data from Star Trek. And so, like, I think basically people generally don't get talked out of bad arguments. I think what happens is movements form and, you know, there's clearly anti AI movement.\\n\\nMarc Andreesen\\nI think clearly there needs to be a pro AI movement. You know, the interesting thing about the pro AI movement is it there are actually a lot of people who are kind of implicitly already in the pro AI movement, and they're just people who are basically making AI. So it's like, every give you an example of the dynamics. So I talked to a friend of mine in the government, in the executive branch, who's in the middle of all this stuff. And, you know, I'm making all these arguments. And my friend is like, well, why are you the only person making these arguments? Like, I have, like, you know, 100 people come and see me, you know, in Washington. And they all make the AI doomer argument. Like, why does anybody come and make, you know, the positive AI argument?\\n\\nMarc Andreesen\\nAnd I'm like, because those are the people all building AI. The people who are going to make the positive argument are the people who are building it. And of course they're busy building it, right?\\n\\nBrian Chau\\nAnd so they, yeah, this used to be my Twitter bio for a while. Like, if you're all building companies, who is defending the companies, right? Like, yeah, I do think.\\n\\nMarc Andreesen\\nAnd if I could, if I could, this is, again, this is sort of a classic political psychology thing, which is the productive people tend to cluster one side, right? And then, and then the intellectual people tend to cluster on the other side. And, and so the intellectual people will be out arguing vocally against something. The productive people are actually doing it, but they're too busy doing it to argue their case. And I think that's the nature of a lot of these debates, and I think that's what's happening now.\\n\\nBrian Chau\\nRight. That's interesting how you classify that because I'm not sure, you know, like, I would consider a lot of technologists to be very, like, intellectual, right? Maybe they're not, you know, publishing academic papers. I mean, like, in the case of machine learning, many of them are, right? Like they're publishing machine learning papers, obviously not like, you know, not like philosophy papers, but they're just busy.\\n\\nMarc Andreesen\\nBut they're just busy. Like a lot of it is. They're just busy, right? They're busy actually doing the thing. I mean, say what you will about the AI doomers, like, they're not writing a lot of code, right? And then the people who are, like, writing a lot of code generally are very pro AI. It's just they're not taking a lot of time to make those arguments in public. And so again, it's just like, it's just any symmetry in the, in the world that it works that way. You know, it is what it is. Anyway, so the point is, like, I think what needs to happen is the creation of the creation of a movement.\\n\\nMarc Andreesen\\nYou know, I think that that movement naturally wants to exist and does exist because AI is obviously so helpful and important and exciting and valuable, and the people working on it are very fired up about it. So, and the people using it love it. And so, you know, there is a lot of natural momentum in the direction of a pro AI movement. But, you know, look, it needs an intellectual component. You know, it needs branding, it needs all the things that the, you know, that the doomers have and you know, it's just, it's going to be a reaction to a suppression attempt and it's, you know, it's basically starting now.\\n\\nBrian Chau\\nYeah, it's very interesting. I actually, I went to DC just looking for, so I'm working on something very related to this right now. And I went to DC looking for basically like this was around, I mean I went a few times, but including a year ago, including like a few months ago and basically just like looking for intros to people who are working on similar things. And I just ran into kind of like semi accidentally three people who already were on this podcast. So it's very funny. Like these talent networks seem to be just so thin and you know, like the number of people who are interested in these topics, even like second order interested in these topics, you know, just interested in tech broadly, are kind of known. And it's crazy. It is just crazy how few of them there are.\\n\\nBrian Chau\\nIt does seem, yeah, it does seem like a fundamental, I don't know, do you have insight on this? You also of course find talent in the VC context. Do you think there's an insight informing these movements there that you can draw of how to actually find people who are going to work on this and who are going to basically, you know, correctly strategize on how to defend AI freedoms?\", \"Marc Andreesen\\nYeah, so I would say a couple things. So one is, look, just people need a shelling point. Like people need a rallying point. Like most people don't build movements, right? Most, most people like join movements that already exist. And again, it's just like most people are busy, right? And so most people are not in their kind of revolutionary business, you know, which is why the communists always have their idea of the vanguard, right? So, you know, you need the vanguard and then you need the movement form.\\n\\nBrian Chau\\nElaborate on that a little bit more like what's the vanguard?\\n\\nMarc Andreesen\\nOh, the vanguard. So the vanguard was, this was sort of leninist, this is sort of marxist leninist stalinist theory, right? Which is. So communism was to be a movement of the proletarian masses. And so you're a german, whatever, intellectual or a russian political revolutionary and you're like, all right, my role here is to represent, rally the uneducated masses. It's like the problem is the uneducated masses are not sitting around waiting for me to come give them three hour speeches. They're busy working. They're working 14 hours a day. And they're not intellectual. They're not the kind of people who sit around for years at a time working on some sort of renegade underground communist newspaper. You know, there are people doing subsistence farming or working in a factory.\\n\\nMarc Andreesen\\nAnd so that led to the communist doctrine that said, you know, therefore, the role of the vanguard. The vanguard is the communist intellectual class, right, of people who are very much not working, you know, but are the educated types. And their job is to, you know, write the books and the pamphlets and give the speeches and, you know, organize the meetings, right? And then, and then, you know, theory of communism, of course, was that the vanguard's role in life was to put the proletariat in charge of everything. Of course, when push came to shove, they did no such thing. When the vanguard took power, they kept it. But there was this very clear separation between the intellectual leadership and then the masses. I think that's a natural formation.\\n\\nMarc Andreesen\\nEric Hoffer, by the way, in his book the True Believer, he actually reverses that excellent book. It's a great book. And he actually reverses that argument in a very provocative way. He says, no, actually, he's like the people. What Eric Hoffer would say to what I just said is like, no, actually, the people in the vanguard think they're in charge. That's not what's actually happening.\\n\\nBrian Chau\\nOh, interesting.\\n\\nMarc Andreesen\\nYeah. Yeah. He says, he says the role of the vanguard is not to run everything. He said the role of the vanguard is to provide the, basically the fodder to recruit the intellectuals. And so he says, basically, if you have a Mass movement, the majority of the mass movement is actually the Masses. And the vanguard can fulminate about this, that and the other thing. All they want, and the masses may or may not sign up for it. And there's always a surplus of vanguards, relatively, to write the ability for the Masses to adopt their movements. And so he said, it's not the vanguard leading the masses. It's, in fact, it's usually the masses leading the vanguard, right? It's just like at some point, people in volume get fed up with something, and then, and then what he says is it's reversed.\\n\\nMarc Andreesen\\nThe role of the vanguard is to build the intellectual superstructure that recruits other intellectuals to that movement, right? And so all the books and magazines and pamphlets and university courses and all that, you know, they never make it to the masses, but what they do is they provide a shelling point to recruit the other intellectuals. So anyway, so there's some feedback cycle loop between, you know, the leader, the quote unquote leaders of a movement, you know, and the masses.\\n\\nMarc Andreesen\\nAnd of course, everybody who tries to lead a movement is always trying to figure this out because, you know, when you're leading a movement, like how much of it is you're trying to actually tell the movement what to do and how much of it is you're just trying to like basically figure out what the people want to do anyway and then kind of get in front of it. And so anyway, these are all movements. The Doomer movement is a, you know, is a movement. You know, the advantage the Doomer movement has is that it's out. It's out, it's out, you know, in advance. You know, it was just better, it was better prepared. You know, it's been 20 years preparing for this moment. So it's out in advance.\\n\\nMarc Andreesen\\nAnd then, you know, doomer negative arguments always sound more sophisticated, you know, at least in our culture. And so, you know, it has this kind, and then it has, you know, credentialism on its side because the institutions, the intellectual institutions are also compromised. And so it has these natural advantages. But what it doesn't have is any kind of mass level of support. And again, you can just like, it's very easy to nose count that because the total number of people who are like, I don't know, taking time off from the workday to show up at anti AI rally is like zero. And the total number of people who are already using AI in their daily lives very productively and very happily is like 100 million. So I would say the AI Doomer movement has a vanguard with no masses.\\n\\nMarc Andreesen\\nThe pro AI movement right now is a movement that actually has a fair amount of mass support implicitly just by revealed preference, by the fact that people are already using AI. What it hasn't had is a vanguard.\\n\\nBrian Chau\\nPolitics doesn't reflect revealed preference, right? Politics reflects stated preference. That's what it is, right. So, like, you have a situation in many cases where people will be, you know, like explicitly, for example, like prohibition, right? You know, as you say in your essay, that's the classic Baptists and the bootleggers. You know, even if people might, you know, secretly enjoy alcohol, might think that it's a good thing, you know, they're going to be, there's going to be people who start a moral panic. There's preference falsification. There's people saying, what is socially acceptable instead of saying what they actually think. I think, I don't want to be too pessimistic. I don't want to say it's all over. The crackdown is inevitable. I don't think that's the case, you know, doing what I'm doing now if that was the case.\\n\\nBrian Chau\\nBut I do think when you say something like they have no mass support, that you're kind of understating. You know, I think there actually been opinion polls where, like, people do support, you know, do support, like AI restrictions just because they, like, hate technology. You know, they're just opposed to new technologies in general.\\n\\nMarc Andreesen\\nYeah, I'll totally grant exactly your point, but I would just say this. Just social support is just social support is fickle. So like I said, yes, I will say in a poll that I hate AI. Will I take an hour out of my lunch break to go to anti AI rally? Just the most marginal level of effort, the most basic test. Will I stop using chat GPT because I hate AI? Will I throw my iPhone in a lake because in practice there's a soft. I think what you're saying is correct, but the soft underbelly to something that's just a social conformity movement or just stated preference over revealed preference is it's very soft. Let me give you a specific example of that. The people who are the most excited about the AI doomerism, the anti AI movement in Washington, are the crypto people. Interesting.\\n\\nBrian Chau\\nHow so?\\n\\nMarc Andreesen\\nWell, because all of the anti crypto energy, not all of it, but a large amount of it, that was like running like white hot, like six and twelve months ago, it's now been diverted onto anti AI. Right? And by the way, any given thing in the next six months, like, I mean, look, like, you know, as you know, we're headed into a presidential election cycle, it's going to be quite right? Like we're in a war with Russia, right? Like there are other things happening in the world, right? And at any given moment, the same herd instinct that kind of brought people to have the sort of stated preference on a, on, you know, crypto one day AI, the next day trump being indicted for the 14th time, the third day, you know, Russia dropping it, you know, doing something horrible, you know? Right.\\n\\nMarc Andreesen\\nBut it's like if all you have is herd behavior, it's powerful while you have it, but it can very easily slip away from you because the public is fickle. Let me make one more point, which I think is very powerful and hopefully ultimately is the trump card for all this. We'll see. But what would hopefully be the case? And that's basically capitalist animal spirits. You already see this, the micro version of this already happening inside the AI Doomer camp where you have companies, you have entire organizations that are very philosophically devoted to AI doomerism who are racing as fast as possible to beat each other to market with more and more sophisticated. For example, there's a bunch of these. And actually there was actually a vox, of all things.\", \"Marc Andreesen\\nThere was a vox piece actually, I think yesterday where the guy pointed out, actually there is a little bit of a mismatch between the idea that you think that this technology is going to destroy the world and the fact you're working so hard to build it. And of course, that's being, Elon is pressing that point even harder with the creation of Xai. And Elon obviously speaks for himself, but he's stated very publicly that he's very dismayed by the fact that the organization he originally funded as a nonprofit became a for profit. So now he's responding with Xai, which is going to be chartered to be fully open source AI even. He's recalibrated his sense of threat. So anyway, and then you've got these, even these big companies that are in DC with this kind of doomer argument.\\n\\nMarc Andreesen\\nAnd they're, and they're also pushing very hard to commercialize all this. And so, and it's actually really funny. I think this is very funny because the argument that ultimately capitalist animal spirits are going to win here, you could characterize as both a far right argument and a far left argument. Right. The far left argument is just that the profit motive is extremely strong and it's going to dominate the far left motive. You could actually bemoan this, right? And this is the, I've been reading a lot of, you probably know, the work of Nick Land, right, who's like basically kind of the philosopher of acceleration and AI. And then he had this protege, Mark Fisher, who is this sort of leftist communist writer who ultimately committed suicide but wrote a bunch of interesting stuff.\\n\\nBrian Chau\\nOh, my.\\n\\nMarc Andreesen\\nAnd, you know, Mark Fisher wrote this very pessimistic book about the future of everything called, you know, capitalist realism. Is there no alternative? And basically it's sort of a far left basically thing saying, yeah, it's just going to be capitalism. The profit motive is so strong that ultimately in our world as it's conceived, you can have all the ideological arguments you want. Ultimately, the profit movement is going to dominate. And there's just so much money to be made with AI that, quite honestly, my hope is that Mark Fisher was right.\\n\\nBrian Chau\\nRight. Yeah, I know. The counter argument to this is, did you know what the Tulloch rectangle is? Do you know this guy, Gordon Tulloc?\\n\\nMarc Andreesen\\nI know who he is, but I don't know the concept.\\n\\nBrian Chau\\nOkay, so the Tulek rectangle, essentially, the idea is that, you know, the money spent on, you know, lobbying or on basically, like, enforcing. So, like, he. He's fascinated on studying rent seeking and on studying essentially, like, economists model this as price caps, right? It's not always price caps or, sorry, price. Basically a price fixing, government price fixing. He models this as there's classic microeconomics. There's a harburger triangle. There's all the trades that aren't made, and that's some kind of loss. But he models what I think someone else called the tolic rectangle. But it's based on his idea is that when there's an excess benefit for rent seeking, actually the incentive to enforce that amount of rent seeking is then eaten up at the same time. So let's say.\\n\\nBrian Chau\\nLet's say I sell, let's say completely abstractly, let's say I sell pharmaceuticals, and by creating barriers to entry to my competitors, then I can make a billion dollars more. Let's say. Then the incentive for me to keep those barriers to entry in place is equal to a billion dollars because that's what I'm gaining from them. So there's a kind of equilibrium process there where the amount of regulatory barriers or the incentive to create regulatory barriers is kind of equivalent to the expected profit that you would get from creating those regulatory barriers. You see where I'm going with this with regards to AI.\\n\\nMarc Andreesen\\nYeah, but you have to. Yes, yes, 100%. But, you know, look, a couple things. So, one is, this is why there's actually really good argument that there's not nearly enough money in politics, right? Right. Now there's like a giant arbitrage. Right. Right. Now, this process is not efficient, I would argue, because the amount of money you need to spend in politics to capture rents is a fraction of the value of the potential rents. Right. So arguably, number one, we should be, the amount of money in politics should be driven up by, like, you know, three orders of magnitude. So that it becomes precisely, so that it becomes actually harder to do what you're describing. We can debate that. Or not. And then.\\n\\nMarc Andreesen\\nYeah, and then, look, the other point is like, look, if you're dealing with a single monopolist I think the application of your idea is obviously very clear if you're dealing with an oligopoly or a cartel. And I would characterize this sort of nascent kind of AI protectionist movement as an attempt to form a cartel structure. The problem with a cartel is defection. Right? Which is what, you know, OPEC deals with all the time. And so maybe I could make my hope more precise. My hope will be that the profit incentive for members of the cartel to defect will be so great that ultimately the cartel is unstable, even if the government wants to cooperate with it. Look, I think there's really indications of that. Look, like, so, okay, here's an ironic thing that's already playing well.\\n\\nMarc Andreesen\\nTwo ironic things that are already playing out, right? A lot of the doomer, a lot of the economic doomer arguments around AI is that AI is this magic technology, and then whoever builds it is going to, what's the phrase? They're going to capture all the value in the light cone of the future universe.\\n\\nBrian Chau\\nThey love saying winner take all.\\n\\nMarc Andreesen\\nBut it's like, but what do we know already? Like, what do we know already out of the gate, right? How is the most sophisticated AI in the world being deployed? And it's being deployed in two ways. It's being deployed to consumers either for $20 a month or for free. Right? And you can go to Microsoft. You can go to Microsoft. You can download the Microsoft Edge browser, which is an excellent browser, and you can run the Bing chat for free. It's full GPT four connected to the Internet, multimodal. It is amazing. The most sophisticated AI in the world is available for free. And lots of people are already doing this. And then even the companies that are trying to upsell it to consumers, the prevailing price is like $20 a month, which is four lattes, not much.\\n\\nMarc Andreesen\\nFirst of all, it's already being made available to consumers very broadly. And by the way, there is no better AI. There's no better AI than Bing chat or GPT that I can buy for a million dollars. It doesn't exist. So it's already out to consumers. And then the other side of it is these models are being immediately deployed as cloud services by the big cloud vendors. It's actually fairly amazing if you think about it. You have this engine of insight and knowledge that in theory you could use, I don't know, capture all profits in the stock market or whatever it is that you think you can do with winner take all AI. Instead, what they're doing is they're making it available as a cloud service. Cloud services are a big revenue business, but they're not a great high margin business.\\n\\nMarc Andreesen\\nAnd yet these companies are pushing as hard as possible to do that. And why are they pushing as hard as possible to do that? It's because there's a cloud war. They're battling each other for market share and they're very determined to show to their shareholders that they're going to be able to get more market share and more cloud revenue by rolling out AI. I would say these are both examples of how basically defection from, if you take theoretical cartel that could exist, it's already basically been shattered as a consequence of these more specifically incentives. And my optimistic hat would be that this is just the beginning. The defection from here will only grow.\\n\\nBrian Chau\\nYeah, that's where I'm even more optimistic about it, actually, is that the second layer economy really matters. This goes back to the central planning fallacy again, the idea that you have one generalized model that solves everything. No, people are going to take the models that we have, they're going to find specific applications of them. You know, you already see these people like, you see like the specialized VC cohort doing exactly this. Right? I think this, I don't know, do you have the same intuition that the kind of like second order people who are taking the APIs and using it to do interesting things, that those people matter a lot.\\n\\nMarc Andreesen\\nYeah. So it's not just, by the way, it's not just taking the APIs, it's also actually building their own models. Right. And again, this lama announcement today is going to accelerate this because it's going to make that even more practical. Right.\\n\\nBrian Chau\\nThat's so fascinating. Sorry. Go on, go on.\\n\\nMarc Andreesen\\nYeah, exactly. So basically, I think the way to think about it. So the big question, this big kind of central versus distributed question I would characterize is does AI roll out into the world over the next decade the way Google searched it or the way that microprocessors did?\\n\\nBrian Chau\\nYes, exactly.\", \"Marc Andreesen\\nRight. If it rolls out the way that Google search did, then basically everybody just goes to Google. It searches on everything and Google has a monopoly on search. And then you get into this kind of, I don't know if you're a doomer, it's a utopian world. If you're anybody else, it's a dystopian world of sort of centralized control. And you look, that's possible. And I can certainly steel man that argument both on technical grounds and on regulatory grounds. But look, I think what the technology wants is it wants to roll out in the way that microprocessors rolled out or the way that operating systems rolled out or the way that the web rolled out, which is it wants to roll out to everybody.\\n\\nMarc Andreesen\\nAnd it wants to roll out in a pyramid structure where sort of like computers, just like microprocessors, there's a few giant mainframes, and then there's a larger number of mid sized computer systems, and then there's a much larger number of PCs, and then there's a much larger number of smart. And then, by the way, even beyond that, there's a much, much larger number of embedded devices, right? Ships in your microwave, in your car and your doorknob and everything else. And, you know, how many microprocessors are in the world today? You know, I don't even know the number, but, you know, it wouldn't surprise me if it's a trillion, right?\\n\\nMarc Andreesen\\nAnd so, and, you know, now that you have open source models like llama two and so forth, like the sort of possibility of that pyramidal world is, I think, very real, by the way, again, animal spirits. That world has a lot of money. There's a lot of people betting heavily on that from a financial standpoint. And so the chip companies are tremendously enthusiastic about that world for obvious reasons. Okay, so that's the other world. And then why might that world work? Because the argument goes basically, it's like, well, why would you ever want a smaller model if you could basically just ask a centralized God model? And it always gives you a better answer. And there's a few answers to that. One is the smaller models that are more decentralized. Number one, they're going to be a lot cheaper to run.\\n\\nMarc Andreesen\\nYou're going to be able to build AI into many more things cost effectively. A, and that's going to matter a lot. B, you're going to actually, a lot of the future use cases have to do with what's called embodiment. You're going to actually want AI to have a local knowledge of what's happening in the real world around where it's located. Your car is going to want to know about the road around it. Your doorknob is going to want to know about everybody who touches it, right? There's a lot of embodiment that needs to happen. And this, of course, leads ultimately to a world of kind of full robotics, which is where this is all headed. And then third, and this is really entertaining, but it's really important.\\n\\nMarc Andreesen\\nSo the God models are going to be heavily scrutinized, as they already are in terms of what kind of outputs they can give. They're the ones that are going to be scrutinized the most, in the same way that the biggest social networks are the most scrutinized, the biggest search engines are the most scrutinized. One of the really entertaining things about these big models is when you use their APIs, they apply their trust and safety rules to their APIs. And that is very comical. What we're finding with our startups is that's very comical because you will have a startup that's building a new kind of like, let's say marketing automation system, whatever, for tracking customers in a business, and they use a God models API to do that. So it's a b two b application. There's no public impact, whatever.\\n\\nMarc Andreesen\\nIt's for running like a hotel or something like that, better customer service in a hotel. And it's making API calls for the God model. And the God model is responding basically like, oh, you can't ask that question. Like, basically that question is racist. Right?\\n\\nBrian Chau\\nOh, I see.\\n\\nMarc Andreesen\\nYeah, yeah.\\n\\nBrian Chau\\nI reported some of this out like very early on. Yeah.\\n\\nMarc Andreesen\\nAnd so it's like your developer of B two B software, you're only going tolerate a response, you know, a behind the scenes API response, that your query was racist one time and you're going to be like, all right, screw it, and you're going to rip out that API and you're going to build your own model. And I think basically that's already happening.\\n\\nBrian Chau\\nYeah. And the kind of circumvention, I don't know, I think the circumvention is pretty easy right now. Maybe, maybe on purpose, you know? Yeah, I do think that's not going to, that arms race is always going to be in the direction of the circumventer. I think just in terms of incentives, I do want to talk about some of the earlier things you talk about. Right. Like, so one thing that you said is like that the independent models will be faster to run or they'll be smaller. What's stopping the industry versions? I mean, OpenAI is kind of already doing this, of just releasing their own smaller models that are even more efficient.\\n\\nMarc Andreesen\\nYeah, I think that's great. I think they should, I think that's awesome.\\n\\nBrian Chau\\nRight. But isn't that more like they'll have more resources, of course, they'll have more economies of scale with regards to hardware they'll have more, you know, they'll have more buying power. What's stopping them from producing, like, a smaller model at a better price? Right.\\n\\nMarc Andreesen\\nYeah, but I think they should. I mean, I think that would be pure. I think that would be pure good. I think it'd be great. I think more competition there. One of the things Peter Thiel and I very much disagree on is the role of competition, and this is an example, which is that more competition, better, more choice, better motivating for everybody. Then, specifically to our conversation, a smaller model from one of the big gob model companies, the obvious question would be, well, is, okay, is it going to operate under the same rules as the Gob model and the same restrictions as the Gob model?\\n\\nMarc Andreesen\\nIf it's just another model in the cloud that just happens to be cheaper, but has all of the downsides of dealing with a model in the cloud that you don't control, that has policies that you don't support and aren't relevant to you, then it's not going to fill in the niche in the ecosystem that the open source models are going to fill in or the possibly other productized models are going to fit into. And so it'll be a very, in other words, the question becomes very practical, which is, do I use this sort of miniature God model with all of the pros and cons of it, or do I use llama, two open source or some adaptation of it? And I think that's a great. I think, absolutely. That's the kind of thing we should see.\\n\\nMarc Andreesen\\nThat's the kind of thing that should play out in the market, and we should see what the pros and cons are.\\n\\nBrian Chau\\nRight. Okay. In that case, I don't think we disagree too much. I'm just skeptical of the idea that, you know. Yeah, I'm skeptical of the idea that the independent models will be faster, per se.\\n\\nMarc Andreesen\\nOh, so the faster. So the faster. The faster is just a lot of. The faster is just, I mean, there's a bunch of reasons it could be faster. So one is just like. So one of the technical questions is basically, how big does a model need to be effective? Right. And so. Right. And so what is the load? What is the chip? You know, the load on chips required? What's the speed? What's the level of all these things? Like, how do you optimize our big models versus small models might get optimized differently. All these kinds of questions. And then there's all the, of course, network, round trip things like, how often are you going after the network versus just processing locally.\\n\\nMarc Andreesen\\nSo there's all those kind of these are just classic systems design questions that everybody asks now of whether they want computation run locally or to have it run in the cloud. And there's just, you know, there's pros and cons. But the other is, let me really stress this this concept of embodiment, right? We, you know, today we think about that, yeah. Yeah. So today we think about AI as something that's basically disembodied, right? So so, you know, chat GPT, your mid journey is running up in the giant cloud, and it's being fed, you know, enormous amounts of training data. And that training data, a lot of that's derived from human experience, right? And kind of physical reality. Like, a lot of the images mid journeys being trained on are obviously photographs of the real world and so forth.\\n\\nMarc Andreesen\\nBut there's this layer of, like, indirection and delay, right? Like, nobody's like, nobody's taking a photo. Nobody, to my knowledge, like, mid journey is not being trained by people, you know, taking, you know, a billion photos in real time and uploading them to midjourney. Like it's a there's a delay to the training data. Well, it's like chat GPT, right? It's like it famously tells you its training data stopped in September 2021 and, right, and look for a lot of what you call kind of abstract questions or like, you know, things that don't require embodiment, like just, you know, kind of, you know, intellectually abstract things. That model works very well. There's this whole other world, though, of AI, applied in particular to physical reality.\", \"Marc Andreesen\\nAnd so it's AI in the self driving car, it's AI in the, you know, it's in the light bulb doing power optimization in the room or in the doorknob doing access control or in your oven, you know, baking you the perfect pizza. Or it's ultimately it's robotics, right? Ultimately, it's your household robot that's doing the dishes, right? Or something like that. And for all of those applications, and this is everything, this is security cameras and defensive systems of all kinds. For those systems, the training data is real world, real time, right there, you're going to want to have a system that's an integrated hardware software system that is sort of physically present, as they say, embodied in the world, like a robot of some kind.\\n\\nMarc Andreesen\\nAnd it's gathering data in real time and processing, analyzing that data in real time, and that's just a very different technological optimization problem than what the got models are doing. And so I think that. And so specifically, the other shoe that's going to drop here is basically there's a revolution in robotics that now is right around the corner. Like we, for a long time it was theoretical, but now we kind of see how to actually do this, and that's really going to change, I think, how people view this stuff, and it's going to change the trade offs of how these systems get built.\\n\\nBrian Chau\\nRight. So something that really fascinates me is that. Yeah, so we talk about, I mean, like, we've heard the story before, right? Maybe turning the sociological argument on you a bit. You know, we've heard the, you know, robotics are almost here, you know, several times before. Right. Even, you know, a young man like myself has heard that several times before. And why is this time different?\\n\\nMarc Andreesen\\nYeah, so, look, technological revolutions, technological shifts, just at the sort of applied level, like what we're talking about here, they follow this pattern that's just like, shockingly, I wouldn't say it's predictable, but it's certainly descriptive after the fact, which is it's almost always when you have something technological that just, like, starts to work. And you could call that like the personal computer in the early 1980s or the Internet in the mid 1990s or AI. Right? Now, basically what you find, or by the way, before television in the 1930s, 1940s, it's the same thing. Basically what you find is these things that look like they're kind of overnight breakthroughs. They aren't.\\n\\nMarc Andreesen\\nWhat happens was there was a backstory, and the backstory usually was the way I read the history is usually 30 to 40 years of prior attempts to make the thing work that didn't happen, that basically did not deliver on with AI. It's actually an even more extreme story. It's an 80 year backstory. The original neural network paper was published 80 years ago in 1943. Three. Right. And so it was an 80 year journey to get from here to there. And then, of course, along the way, what you have every single time, by the way, this happened with automobiles. Also very interesting backstory to cars, by the way. For example, the original Detroit was not even Detroit. It was Cleveland. Cleveland was ground zero for auto hobbyists in the late 18 hundreds. And then it actually didn't even stick there and actually migrated to Detroit.\\n\\nMarc Andreesen\\nSo that's how long it took for cars to develop, that she had to move cities. And so you just have this thing where you kind of have waves of engineers and entrepreneurs over time, trying to get the thing to work. Trying to get the thing to work. It doesn't work. It doesn't work. And when I say it doesn't work, what I mean is the technology does not get delivered to the market in a way that it's packaged in a way that the market wants to consume it. And usually what that is it's like, okay, the components are just not quite ready yet. The chips aren't fast enough. The power isn't there. The software isn't quite sophisticated enough. It doesn't have quite the right user interface. It's still too expensive. I'll just give you an example.\\n\\nMarc Andreesen\\nI went to a launch party when I first came to Silicon Valley, I went to a launch party in 1994 for something called the general Magic Communicator, and it was this super white hot consumer electronics startup in the early nineties with a lot of Apple people, Andy Hertzfeld, who designed the Mac UI, and a bunch of these people went to this company's general magic, and they basically built the iPhone. Between basically 1980, 819, 93, they built the iPhone. They shifted in 1994, and it just completely flopped. And why did it flop? Like, it had basically all the same functions as the iPhone, which was a huge hit 20 years later. It was too heavy, it was too expensive, it was too slow. The screen was black and white, low resolution. The radio was low bandwidth. It just wasn't ready yet.\\n\\nMarc Andreesen\\nAnyway, my day job in venture capital is basically to try to figure out, you've got this idea, it will probably happen at some point. You've got this entrepreneur sitting here today that has a judgment that now is the time. If you look at the history of all entrepreneurs, a lot of the smartest ones are just simply too early. On the other hand, most things do eventually happen, and then when they happen, they turn out to be really big deals, and those companies end up obviously being very valuable. But it is very easy to be too early on these things. Look, maybe I am too early on robotics. I would just say that six months ago, if you'd asked me, do we have a UI for basically interacting with a robot in the real world, I would have said no.\\n\\nMarc Andreesen\\nAnd now I would say yes. And that is a step function change. That's very important. And then, look, there's research. A lot of the most interesting research right now in language models is figuring out how to use them for planning, and that's starting to work really well. And then multimodal AI's llms are about to come out. And so that has the potential of solving the perception problem in a really interesting way. So I can't, like, declare that we're on the cusp of the revolutionary robotics, but, boy, it sure feels like we're getting closer and we might be almost there, right?\\n\\nBrian Chau\\nBut, like, every time this happens, there are. I don't know. I haven't done, you know, like, a micro comparison of, like, all these different breakthroughs. You know, what breakthroughs were there the last time people were excited about this? Right? I think, like, one of the big things was, like, early self driving cars, and then they started to run into edge cases and so on, right? Like, but this.\\n\\nMarc Andreesen\\nBut this is the thing. Like, the self driving cars. And by the way, there's another phenomenon here that you may have a touch of that I also have on a routine, routine basis in my life, which is just. We have this phenomenon we call scar tissue, right? Which is if you've ever in your life tried to get something to work and not been able to get it to work, you will spend the rest of your life arguing that it can't be done, right. And you will be, like, hypercritical of every new. Every new wave. And actually, the thing that drives you.\\n\\nBrian Chau\\nThe craziest, this is about a certain. Certain fanfic writer.\\n\\nMarc Andreesen\\nWell, it applies to a lot of us, and it's a very natural human thing. It's ego insulation. It's like, okay, I couldn't get the thing to work, and so therefore, nobody else should be able to. And it would be tremendously insulting and damaging to my ego if somebody else did. And so you have this phenomenon where people who try to get something to work generally argue against it for the rest of their lives. And then you have this other phenomenon that drives them crazy, which is the kids that come along that actually get up to work. They often don't even know the history. They don't even know the failed attempts from, like, 1020, 30, 40 years ago. And then, of course, why would they.\\n\\nBrian Chau\\nRight, right.\\n\\nMarc Andreesen\\nAnd why would they, you know, they've got, you know, they're actually just, you know, they're busy building the thing. And so that. That drives the veterans, you know, even crazier. And so you have this long. It's always this question of the venture business, which is, should, you know, if you're normally making an investment, you want to do due diligence. A big part of what you want to do when you do due diligence and investment is talk to experts, right? And so there's always this question when we're back into a tech company that was some kid with a crazy idea, which is like, okay, should we go talk to the people who tried to make that idea work ten or 20 years earlier and failed, right? And it's like, are we going to learn things to prevent us from making a bad decision?\\n\\nMarc Andreesen\\nOr are we just going to be getting a blast of, basically, ego protection and bile aimed at the new generation? Basically, how dare they try to do something that we already tried and failed? And so anyway, like I said, the problem is none of this is. None of this is predictive. Like, I don't. Or at least I don't have a model for, like, okay, it failed four times before, and now it's going to work the fifth time. Like, or, you know, it took 20 years. You know, it takes 20 years or 40 years or 80 years or whatever. I don't. I don't. I don't know how to use this predictably, but I will tell you, just looking after the fact, and you can stress test this idea by looking at historical innovations after the fact.\", \"Marc Andreesen\\nWhat you discover is there was typically this kind of backstory. My favorite example of the backstory is actually television. There's this great book called Tube, which is the history of television. And television is kind of credited in the form that rolled out, which is electronic television with ray tubes is sort of credited to this guy, Philo Farnsworth, who was this kind of Silicon Valley like character in the 1920s, 1930s in San Francisco. But it turns out there was a 40 year backstory to television even before Farnsworth. And actually, the first television was in the 1890s, and it was this scottish inventor who's this crazy guy, and he literally made mechanical television, right? So he made television work before he had anything, any sort of electronic mechanism capable of rendering image. He rendered the image mechanically. How do you render an image mechanically?\\n\\nMarc Andreesen\\nHe had spinning wooden blocks representing pixels, right?\\n\\nBrian Chau\\nRight.\\n\\nMarc Andreesen\\nSo he built a mechanical device, right. That would receive telegraph. It would receive basically the equivalent of, like, Morse code messages over telegraph or over wireless, right. And it would interpret those mechanically with a very simple electrical circuit. It would interpret those. We would display those mechanically, and it would spin the wooden blocks to whatever red, blue, green, whatever color on the different side of the blocks and represent the picture. And in his mind's eye, he knew what was possible. He knew that he could remotely broadcast images and receive them, and he knew what this meant. But the technology of his time, all he had were these spinning wooden blocks. And he would go around and pitch this to people, and they just thought he was completely crazy. Right. And it's like, well, you know, was he crazy?\\n\\nMarc Andreesen\\nIt's like, well, yes, he was crazy. And that he was trying to do something that, in retrospect, we now know was impossible to do with the technology of his era. But on the other hand, he was actually visionary. Like, he saw what the thing would be, you know, decades in advance of the people ultimately made it work. And I think that's a. I think that's a normal phenomenon. I think that's what's happened, by the way. That's what's happened in AI. And I. And I do. I do believe that's what will happen in robotics even if I can't predict the exact timing.\\n\\nBrian Chau\\nYeah, I think, like, yeah, for the. For the audience, of course you know this.\\n\\nMarc Andreesen\\nRight.\\n\\nBrian Chau\\nBut, like, for the audience, you only have to be right once. You only, you know, you bet on that kind of thing, like, ten times. And if you're right once, then, you know, you'll be. You'll be very wealthy. Yeah. As you know, you personally have done so. Yeah. So this is something that interests me, I think, like, the machine learning. I think, like, I don't know if this has always been true throughout history, but I think certainly in my generation, and I mean this in, like, the best way possible, you know, I identify with many of these people, but, you know, the machine learning community has been, like, weirdly autistic, just really lacking in people who, like, really sell, who are really enthusiastic about selling the product. Maybe there are a few exceptions, like Sam Altman maybe is an exception.\\n\\nBrian Chau\\nBut a lot of machine learning engineers, you know, like, they're. They're really, like. Like, they're really into the statistics and they're really into, like, the technical details, but they're not really, you know, they're not really marketing people. And to me, like, this is. I've said this, like, once or twice on Twitter, that I think that, like, it would actually beneficial if there's more of the kind of MBA personality type, the kind of person who's more focused on product market fit. Do you think that's true? Do you think that's false? Or do you think that it's, like, irrelevant to.\\n\\nMarc Andreesen\\nYeah, so I guess maybe I would put another twist on it, which is you do tend to see generations of companies when things start to work. And maybe the way I would describe it's the same thing you're talking about or not, but the way I describe it is the early things that work are very much from the fringe. They're kind of by definition, it's only really people on the fringe who work on fringe ideas. It's not people who are comfortable in the status quo real world, but kind of by definition. And you know that because they've chosen to work on something in the fringe. Right?\\n\\nMarc Andreesen\\nLike, well, just take all your machine learning engineers you're talking about, like, they could all be making probably, you know, over the last, you know, 50 years, you know, every single person who worked in the field probably could have made more money had they just, like, worked on Wall street or something. Right. Or, like, worked on, like, marketing automation software or, like, you know, search engine ads or something like that. And so, you know, there is this kind of personality sort that takes place where they tend to be people more attracted to the fringe, more attracted to new ideas that haven't yet been proven. But then having said that means that they are, you know, they are people on the fringe. And people on the fringe generally aren't just on the fringe in the area of what they choose to work on.\\n\\nMarc Andreesen\\nThey're usually also on the fringe and many other, you know, kind of areas of their life, right? And so, for example, they often have, let's say, fringe food consumption patterns, or they have fridge.\\n\\nBrian Chau\\nOh, I thought you were going to say political views.\\n\\nMarc Andreesen\\nOr political views. Right, exactly. Right. Or, like, fringe sexual ethics. Right. Or, like, fringe drug consumption patterns. Right. Like, you know, it's the fringe personality type. And the great thing about the fringe personality type is they come up with all the new stuff, right? And this is. This is, you know, these are the people in previous generations, we would have called the bohemians. Right? Like, you know, these are the people who write, like, all the good literature. These are the people who do all the good art. You know, this is, you know, they're just. They, you know, they do all this, you know, Isaac Newton on the fringe. Like, Isaac Newton, you know, spends 20 years working on alchemy. Right? And so they're just on the fringe.\\n\\nMarc Andreesen\\nAnd then the nature of it is, yeah, sometimes they, like, start companies, or sometimes they, let's even more generalize it out. Sometimes they kind of saddle up and they try to make something a movement. But for the most part, they don't do that. They don't start movements. They don't create large organizations. They don't start companies. They don't run companies. And again, the reason for that is just if they were the kind of person to do that, they would have been doing that the whole time as opposed to trying to make this new thing work anyway. So then what happens is you basically get a second generation at some point. And the Internet precedent on this was like, the Internet companies formed in like 1992 were like all companies from people on the fringe.\\n\\nMarc Andreesen\\nAnd then the companies formed in like 1995 were all basically the type that you're talking about. It's like the professionals show up, right? And by the way, pros and cons, the good with the professionals is they know how to organize, right? And so they know how to build something for scale and they know how to, like. And then, you know, then out of that, you get like your Amazon's and your eBays and your Googles and yahoos and everything that followed, you know. But then the people, you know, who were on the fringe are like, well, you know, shit like, what happened? Like, our thunder got stolen by these, you know, people with, like, you know, better, you know, better social manners. Like, you know, that's, that's terrible. You know, optimistically, it's kind of a partnership, right?\\n\\nMarc Andreesen\\nAnd actually a lot of what we do with our companies is we try to kind of bring together those two personality types under one roof where you've ideally got somebody who can, like, make technology work and you've got somebody who can go out and sell it. You know, pessimistically, you would say that it's just a, you know, it's sort of a lost opportunity for the people in the fringe that they couldn't do the big thing. Maybe it's just the way of the world. You know, maybe it's just what's required for broader social adoption.\\n\\nBrian Chau\\nRight? The kind of counter argument to this, or like the kind of inverse argument is like, what if it just brings in a wave of grifters, right? Do you think that's a concern?\\n\\nMarc Andreesen\\nOh, yeah. Well, so I'll generalize that out even further, which is to say, basically, again, this is a historical argument. Basically, every new technology has led to some level of speculative bubble. And, you know, I mentioned television earlier, so there was like a radio bubble in the 1920s, right? There was like an electronics bubble, so called tronics bubble in the 1960s. There were, by the way, there was an AI bubble in the eighties that I remember. And there were, you know, people out press making all kinds of promises about, like, expert systems and AI doctors and all these things that didn't happen. And so there, you know, if you go back further in history, you get, you know, the Mississippi bubble and all these things.\", \"Marc Andreesen\\nAnd so basically any time there's like a big fundamental technology shift on deck, yeah, you're going to get a wave of speculative enthusiasm. You know, the argument, you know, generally from the left is that's, you know, that's bad, right. Because, and, you know, there is, there can be an element of badness to it which is, you know, obviously people, you know, when they're, when things, when speculative bubbles do end up often including scams. Right. And full on grifting. And so obviously people can get taken advantage of so that, you know, there is an argument, there is a bad kind of dimension to that. But the good side of the speculative bubble is the huge surge of money and talent that comes into the field, which is what's actually required to make these things work.\\n\\nMarc Andreesen\\nAnd my theory on this is you are much better off living in a society that has too many speculative bubbles around new technologies, that has too few, because you need that kind of catalytic effect to make new things work. And if you live in a society that does not have that, they're not going to have just rational deployment of new technologies. What they're going to have is no deployment of new technologies. Technologies. And so it's just kind of, this is one of those kind of human nature things which is, you know, kind of pick your poison and I.\\n\\nBrian Chau\\nRight, but like the Peter Thiel argument is that these speculative bubbles drive out the actual innovation, right. That the actual innovation isn't done in the speculative bubbles. They maybe cause the speculative bubbles, right? But that, like, the speculative bubbles actually kind of place a cap on their innovation, right?\\n\\nMarc Andreesen\\nI don't think so. So my understanding, and, you know, look, maybe I'm wrong on this, but like my understanding just kind of looking at it, living through several waves of these now that's not quite right. So a big thing, here's something people on the fringe generally don't understand that I kind of wish that they would. Which is, well, the simple form of the argument is like, if you build a better mousetrap, the world is not in fact, necessarily going to beat past your door. Like, a lot of new technologies get parked on the shelf for quite a while before people pick them up. You know, the Sims, you know, Sims, the Sims example I gave earlier as an example. So, like, it's not enough to just build the thing.\\n\\nMarc Andreesen\\nThe more sophisticated version of that argument, which I think is very important, is just like, look, the world is a big and complicated place and the path for a new technology to be able to ultimately get into everybody's hands and reach its full potential. That path is going to require a level of investment and effort that's just like, way beyond what most innovators kind of have in mind when they build something for the first time. Somebody has to come in and basically build the army, right? Somebody has to build the company with 100,000 employees. Somebody has to pour hundreds of billions of dollars, you know, kind of into that. Somebody, by the way, the product needs to be finished. Like, the product needs to be picked up in its nascent form and, like, applied to the real world. Right.\\n\\nMarc Andreesen\\nIn probably a thousand different ways. Right. And somebody needs to, like, pick up the ball and basically run with it, right? You know, which is why you get these things. You know, software is the most fascinating one in this because it's like all the great software is built by a very small handful of people up front. And then you kind of blink, and 20 years later, you've got some giant company with like 50,000 engineers, right? And you're like, what on earth are all those engineers doing? And what they're doing is they're finishing it, right? They're taking whatever was the kernel of the breakthrough, and they're applying it in ways that 8 billion people on planet Earth are going to be able to take advantage of. So somebody has to do all of that.\\n\\nMarc Andreesen\\nAnd there's a big money component to that, and there's a big kind of headcount staffing component of that. There's a big organizational component to that. There are political dimensions, as we've been talking about that. And somebody has to do that. And if nobody does that, the technology is just still going to be sitting on the shelf 20 years later. Like, people just aren't going to be using it. And so, yeah, you have to have both sides. You do get the occasional figure. You do get the occasional, like Bill Gates, right? Where it's like, he's where he was. He was definitely involved at the very beginning, and then also was the guy who was able to build the engine more often.\\n\\nMarc Andreesen\\nOr maybe Steve Jobs would be the other kind of classic example that, you know, maybe Elon, you know, more often this is just, these are just two totally different kinds of people. And then you just have this repeated pattern of the people in the fringe thinking that, you know, kind of these sociopathic people with great hair, you know, kind of, it kind of took it away from them. By the way, I think you could say the same thing is true in the arts. Right? Like, you know, who else are we describing? We're also describing David Geffen. Right. Like, somebody had to pick up hippie music in the 1960s and turn it into a mass market phenomenon because the hippies were not going to do it on their own.\\n\\nMarc Andreesen\\nAnd so it's part and parcel of basically taking these new ideas and bringing them to everybody.\\n\\nBrian Chau\\nYeah, I do think there's this fascinating thing that happens where kind of status is sort of, I mean, like, why there's one reading of this where, like, status is kind of miss, or, like, there's an unnecessary backlash to the people who are actually finding product marketing could fit that I don't really understand. Right. Is it just envy? Is it just because they're successful? Or is there something about that personality type that's kind of disliked?\\n\\nMarc Andreesen\\nI think it's that they're, I think it's that they're the people who come in. I think it's because they steal the thunder from the people on the fringe. Right? And so, and part of it is the people in the fringe will get very upset about this, you know, and, you know, and you could argue for legitimate reasons, but you could also argue, like, it's their own fault because they didn't carry their own thing forward. And, you know, this is a discussion.\\n\\nBrian Chau\\nI is it really them, though? Like, is it really them that's a antagonistic towards, like, the entrepreneurs, or is it like, a different group of people?\\n\\nMarc Andreesen\\nYeah, yeah. So there's that. But, I mean, I think that's the core. Like that. That's, that's how it starts, because it's like, these people aren't cool, right? Like, they're opportunists, right? You know, they picked up somebody else's work and carried it forward. And so that's kind of the beginning of the chipping away of the moral legitimacy of, you know, the, of the scale of the scaled entrepreneur. And then, yeah, look, and then all the anti capitalists, you know, all the standard anti capitalist energies, you know, obviously form up, and that's, you know, it's just, you know, Nietzsche identified that a long time ago, you know, resentment, you know, and then I don't know what else. Like, you know, look, they're, you know, empire builders, you know, everything else we discussed, right? You know, they upset the power structure of society.\\n\\nMarc Andreesen\\nYou know, they upset, you know, they upset how things have always worked. You know, the mental model, the other mental model that kind of maps a little bit to the Sims model of how societies adopt new technologies and new ideas or absorb them, is the Douglas Adams model where he says it's all generational. And he says, if you're whatever, there's a new thing. If you're below the age of 15, the new thing is just how the world has always worked. It's totally natural. If you're between the ages of 15 to 35, the new thing is super cool, and you might be able to make a career out of it. And if you're above the age of 35, the new thing is evil, horrible, and will destroy society. And so, you know, I do think there is this, like, societal reordering that takes place.\\n\\nMarc Andreesen\\nThis is actually very interesting. In the crypto, in the crypto wars, this has been actually really fascinating because virtually all of the people who are super anti crypto are all 50 and above, even very anti capitalist. People who are, like, 40 and below are not that anti crypto because they just.\\n\\nBrian Chau\\nRight, right, yeah, that's fascinating.\\n\\nMarc Andreesen\\nRight. They just can't bring themselves, like, they just kind of know in their gut, it's like, no, actually, it's just, like, going to be the thing. And then, by the way, like, all my supporters, like, and use it anyway. So, like, you know, I'm not gonna, like, I'm not gonna saddle up with, you know, some 80 year old to go try to kill it. So anyway, yeah, society does change. You know, society does change. The people who drive that change, you know, are both, you know, lionized and probably hated maybe in equal degrees.\\n\\nBrian Chau\\nYeah. I think that the trend here. Right. The trend here of age. I mean, like, you know, you can argue that it's sort of a kind of wealth transfer thing, right? That it is like the actual. I don't actually, I don't know how much I actually believe this, but there's, like, an extreme version of this argument of, like, oh, they're, like, already guaranteed by Social Security. They're already guaranteed by all of this, basically. Like, they are. They are set and they are basically, this is the kind of Eric Weinstein argument. They're worried that if people start paying attention as to where the money is going, no matter how much innovation is there, that once people start realizing where the money is going, that it's game over for them.\", \"Marc Andreesen\\nYeah. And then there's the other irony, right, which is the current generation of people in control are kind of the boomers, and the boomers who are in control now are just kind of between the ages of 60 and 80, you know, and the great iron. And, you know, they're just as you as, you know, and as Eric talks about a lot, like, they're not letting go, right? They're, they're going to continue to run everything in some cases, into their, apparently into their nineties, you know, even in the face of increasingly obvious senility. And of course, you know, the great historical irony of that is this was the revolutionary generation, right, of the sixties and seventies, right? And so these were the people who were the most fired up to, like, you know, basically fight the man.\\n\\nMarc Andreesen\\nAnd then they are correspondingly the most determined people to remain the man.\\n\\nBrian Chau\\nSee, like, I wasn't alive for this. I'm not sure if that was a psyop or not, right? Like, this is, I mean, I don't mean this in, like, the strong sense of, like, where the hippie is, like, directly funded by the CIA. That's not, that's not what I mean. But, like, people talk about, like, teenage rebellion to me. That's like, that's like a technological change, right? It's people finding, it's people finding television. It's like, it's like it's a domestic color revolution. That's what it is, right? It's not, you know, children, you know, being the idea that, like, children being, like, generically rebellious always seemed like total bullshit to me. That that just seemed wrong. They just found a new kind of, like, father figure in the television.\\n\\nBrian Chau\\nAnd, you know, now the kind of cultural preference of the people on television, actually, it's still kind of like this, right? It's just that it's more associated with the establishment than with the current generation. You know, maybe if, like, the Chinese Communist Party, you know, takes a heavy hand on TikTok, then we'll get another chance to test this.\\n\\nMarc Andreesen\\nYeah, I think, yeah, there's a lot to that. And, you know, these days, you could argue it's just like television versus the Internet is kind of this battle, right? It's like, you know, boomers versus millennials or zoomers or something. Yeah, I mean, look, the people, the sort of quote unquote revolutionaries in the sixties and seventies, you know, they were, I'm a little bit more proximate to them. I was a kid when they were doing their thing. But, you know, they had specific issues. You know, they had the Vietnam War and they had, you know, environmental stuff and so forth that they were in corruption, government. They had specific issues. They were definitely fighting.\\n\\nMarc Andreesen\\nBut, you know, to your point, like, I think in the fullness of time, it's like the majority of the thing was just flat out the way, you know, the term they used at the time was, you know, hips, hip versus square. And you were either with it or you weren't. And, you know, if you were, you had all of the politically correct views on the one side, and if you weren't, you were hopelessly stuck in the past. And. Yeah, that's just like a straight up power fight, right?\\n\\nBrian Chau\\nExactly. Conformity. It's like worst conformity, if anything.\\n\\nMarc Andreesen\\nAll the hippies. Well, that's the funny thing, right? All the hippies dressed identically, right?\\n\\nBrian Chau\\nLike, yeah, exactly. Exactly. It's a.\\n\\nMarc Andreesen\\nAll the punk rockers, right? Exact same, you know, exact same leather jackets, exact same, you know, tattoos. So.\\n\\nBrian Chau\\nYeah, yeah. The kind of story the sixties as a kind of, like, new age of conformity culture, you know, as, like, turning the tide and actually making things significantly more conformist, that's a story that hasn't really been. You get, like, there are things that are adjacent to this. Right? Bonfire of the vanities, maybe, but, like, the framing of it as, like, a totally, like, conformist, you know, like. Like boomer. Like, it is a boomer culture. We understand this now. Right? Like, people my age understand this now that, like, it was boomerism, right? Or like, that, like, boomerism is a culture of conformity, but, like, people need to, like, project that backwards. I think, like, the projection backwards here is actually the correct version. You know, the correct version is that, like boomerism, you know, they were always boomers. It's just that.\\n\\nBrian Chau\\nIt's just that, like, now. Now we can tell what boomerism really is about.\\n\\nMarc Andreesen\\nYep, exactly. And they are going to. They in many ways are guaranteeing that we're going to figure it out because they just simply will not relinquish power, so they.\\n\\nBrian Chau\\nRight, right, exactly. So something that was really interesting to me, you talked about, like, not really being able to convince people, but, like, obviously there's, like, some. There's, like some time in which people's preferences are formed. Right? Like, whether that's childhood, you know, there's some genetic bases to these things, but, like, obviously a lot of it is based on, you know, it is based on preferences of people around them. What. What is like the, you know, what is the kind of machiavellian strategy for kind of political victory if you can no longer. If you can no longer persuade people?\\n\\nMarc Andreesen\\nYeah. So to be clear, so I think persuasion in the general sense fails. I do think there are two specific points where people are persuadable and these are really the two, these are kind of the two that I have in mind when I, like, write about things or talk about things. And so there are two very specific points, and they're very specific stages of life. So number one is when they're teenagers, right? And so it's actually, Jordan Peterson had a really good thing on this. He talked about this years ago, and he said basically he's like one of the reasons why the education system is such a hot, constant topic of political this and that. And so many people get whammied with anti capitalism or whatever in college.\\n\\nMarc Andreesen\\nHe says a lot of that is if you get kids who are in their teens or early twenties and they have not thought seriously about a topic just because they're young and the first person to, like, explain the world to them in detail on that topic basically, like, puts for the most part, puts the whammy on them, right? It's just like people will just basically buy the first explanation that they get and they'll just kind of dig in hard on it. And so, you know, and you see that, you know, acting out today, which is like, if you are, you know, sort of socially conformist, compliant kind of person and you come up through the modern american education, high school and college system, you come out with a very predictable set of, you know, sort of these kind of beliefs.\\n\\nMarc Andreesen\\nYou know, if you grew up as a teenager on the Internet, you come out with kind of a very different, you know, very different set of beliefs. And it's just, you know, and again, to your point, some of it's a personality sort, but some of it's like, just who is explaining stuff to you? And so I do think there is fundamentally a battle for the kids. And I always think a lot of who I'm writing about writing for and speaking for is, you know, sort of like me as a 16 year old, you know, basically trying to start to figure things out for the first time and to hopefully have somebody who can explain, you know, things in a rational way.\\n\\nMarc Andreesen\\nThe other, the other side of it is people who get disillusioned and, you know, and this is the sort of politically, this is the time honored tradition from left to right that, you know, a lot of people go on in their later years, but, you know, just more generally, it's, you know, people who have been in the belly of the beast for, you know, whatever, 10, 20, 30 years, and at some point they're just like, okay, this is wrong, right? And, you know, and most people don't do that. Like, most people probably don't rebel. They just stay conformist. But, like, some set of people, and I, you know, in particular, I would say the smarter, more interesting people, you know, they'll hit their, whatever, late thirties, forties. Late forties. They'll start to be in the room where decisions are made. You know, they'll.\\n\\nMarc Andreesen\\nThey'll meet more and more of the people who are in positions of power, and they'll be like, okay, like, you know, this is not what I thought it was. You know, they'll. They'll. They'll see the consequence of decisions made with good intentions play out in the form of bad results. They'll start to become suspicious of the good intentions, as they should, and they'll start to be suspicious of the caliber of the people who end up running a lot of major institutions, and they'll just say, okay, look, whatever worldview I had up until now is just not correct. And then they adjust, and at that point, also, they become open minded, because then they seek out somebody who can explain why the world is actually different than they thought. So, yeah, those two are very specific moments.\\n\\nBrian Chau\\nRight? You. You went on Sam Harris's podcast. You did. Like, I think he is more of a. More of a doomer, right? Maybe not. Maybe not entirely in that direction, but he. But, like, you know, somewhat in that direction.\\n\\nMarc Andreesen\\nPretty doomer.\\n\\nBrian Chau\\nYeah.\\n\\nMarc Andreesen\\nYeah.\\n\\nBrian Chau\\nLike, like, what was that? What was the purpose of going on that podcast? Or, like, what was the strategy of going on that podcast?\", \"Marc Andreesen\\nOh, I just want one, is just, I like Sam. I have a soft spot for Sam, you know, going way back, but then I like him a lot as a person, but also, I wanted to have the long form. I wanted to have on record the long form, full doomer engagement experience specifically. Right. Like, so, like, my conversations, I mean, like, you know, like, you're very much on the non doomer side, so we haven't even really had a controversial conversation. But, like, I, you know, my, like, the way I think about what I'm doing is, like, my Lex Friedman conversations was kind of more balanced. I have a. Actually, Joe Rogan's in the can, so that'll come out shortly. That's like, a more balanced conversation. I've done a bunch of others that are kind of more balanced, and then interesting.\\n\\nBrian Chau\\nIs Joe, like, kind of a doomer?\\n\\nMarc Andreesen\\nYeah, a little bit. A little bit. A little bit.\\n\\nBrian Chau\\nOkay. That's. I would not have predicted that, but that's interesting.\\n\\nMarc Andreesen\\nA little bit. So he's. He's so the great thing with Joe. So I did a previous, you know, this would be my second time on Rogan. My first time, you know, the first time I went to Rogan, it was actually right after the Blake Lemoyne, the guy at Google, kind of did the Russell glory thing and said that the chatbot had become self aware. And for some reason, that news had just dropped the day before I went on Rogan or something. So I wasn't even really expecting to talk about AI, but that was the thing he was most interested in. And he basically was. And he does it in his both fun and actually interesting and very actually smart way, which is he's like, well, how do we know that? Maybe Lemoyne is right. Maybe it is self aware.\\n\\nMarc Andreesen\\nAnd I was like, you know, no, it's not. And I can explain to you why it's not and this and that, but like, you know, he was already kind of pre inclined at that point to kind of think, okay, maybe. Maybe this thing is self aware. So we had a fun conversation there. You know, now he's just gotten, you know, he's just gotten the full, like all the rest of us, he's just gotten the sort of media, you know, doomer kind of thing. And so I think we have a good, you know, he's not, like, he's not Sam Harris. Like, he's not dug in on it, you know, but he's. But he wanted to have the discussion.\\n\\nMarc Andreesen\\nI thought the great thing about the Sam discussion was like, if I were trying to, like, again, if it was me at 16 and I heard that discussion, you know, there's two possible conclusions you could make, which is one is like, boy, Mark couldn't prove his case because he couldn't basically rule out, you know, a future scenario.\\n\\nMarc Andreesen\\nYou know, the other conclusion you could make is just like, sam kind of as a proxy for the doomers, is just, like, really dug in on a point of view that is sort of, you know, perhaps more fundamentally religious than technological, you know, kind of ironically, and, you know, basically just like, we'll basically make any number of extrapolations required to basically whammy himself into believing that the end of the world is near, to kind of get into kind of full millenarian kind of, you know, kind of mode. And I just want. I wanted that on the record. Like, I just. I wanted people to be able to hear that because I think it's. It's good to stress test these things.\\n\\nMarc Andreesen\\nAnd I suspect people who listen to that full conversation, if they can kind of, you know, tolerate all 2 hours of it or whatever will come out probably pretty starkly in either one direction or the other. And I think that's probably good, right?\\n\\nBrian Chau\\nSo something. Okay, so the long form, right? So the long form medium one hand, I think, like, yeah, allows you to kind of keep. Keep on expanding into detail. But I mean, like, I'm doing a long form podcast. I don't know. I think, like, long form, kind of, like, this is weird. I think, like, in conversations where you have. Have, like, large areas of agreement and you can actually probe the difference, that, to me, is very interesting, man. Like, I had, I have this episode that may be out. Actually, you know what? He'll probably be out after this one since this one is also kind of time sensitive. But where I just, like, debate immigration for like, 3 hours, I'm like, we're any on something like that. You know? Like, was anyone really convinced?\\n\\nBrian Chau\\nDid anyone who was anti immigration become pro immigration or vice versa? And it almost feels like the longer, like, both groups make their case, you know, like, maybe this is a good thing, right? Maybe in the short version, you know, someone just really doesn't address a point or makes, like, a huge blunder and, like, that's not really representative of the position. But at least then, you know, you. Maybe it's kind of performative, but it's. There's. At least it feels like to me, there's at least a higher chance that someone changes their mind. Whereas in the long form version, it really does feel like, you know, everyone who believed this beforehand will still believe this. Everyone who did not believe this beforehand will continue to not believe this.\\n\\nMarc Andreesen\\nYeah, I think the most optimistic take and I do believe what I'm about to say very deeply, the most optimistic take is just there are a large number of people. There are a large number of people who I would not characterize as, like, they're not really necessarily on what I would describe as the vanguard. Right. In the sense of, like, they're not PhD students at a major university or something, right, where they're, like, at part of the quote unquote intellectual class, but, like, you know, they're smart and curious people, but they're just in a line of work or whatever where they don't engage with ideas a lot. And, you know, that, you know, this is good.\\n\\nMarc Andreesen\\nYou know, this is even, like, truck drivers for cliff operators, all kinds of people, you know, and, you know, because of the new technology, talk about technology shifts because of the new technology of podcasts and, you know, smartphones and then Bluetooth headsets, you know, they can listen to, they can listen to your stuff, right? And, you know, this is why Rogan's been so successful and Lex Friedman has been so successful. And, and you can listen to these things and you're like a normal person, and you're like, okay, wow, first of all, boy, I just learned a lot. Like a very smart three hour conversation on immigration, AI, or whatever topic. Whether or not you agree with whoever in the conversation 3 hours later, you are going to have learned a lot.\\n\\nMarc Andreesen\\nAnd you're going to learn a lot that you were not going to learn if you were watching CNN or something like that or reading the newspaper. And then b, it's like, maybe it's the first time you're hearing about it, right? And maybe it's the first time that anybody's actually engaged in a serious conversation with you or around you about it. And you may well find one of the arguments very compelling. And so I do think there is this, like, really quite, I would say, charming, lovely kind of, you know, fun, exciting. I don't know what it is like, you know, sort of broadening out of intellectual discussion.\\n\\nMarc Andreesen\\nAnd, you know, we'll probably discover that there are a lot of people who kind of had the, you know, intellectual horsepower to be engaged in these topics that all of a sudden kind of have access to them. Yeah. And then, look, some of them are going to get, like, some of them are going to get persuaded.\\n\\nBrian Chau\\nRight?\\n\\nMarc Andreesen\\nSo I suspect, let me take it a step further. I suspect that from the 1950s to the 1990s or two thousands, were in a epistemic bubble that was much more severe and confining. A bubble in the sense of, like, restricted activity.\\n\\nBrian Chau\\nI completely agree with, like, it's just.\\n\\nMarc Andreesen\\nLike the incredible technologically driven restrictions on who could broadcast information and publish information on the one hand, and then the incredible level of credentialing that took place in the institutions on the other hand, just, like, really isolated any kind of intellectual engagement to a very narrow set of people who in a lot of cases, were not actually interested in the ideas at all and were in it for power. And it was, in retrospect, it was like, I don't know, sipping information through a straw or something for the broader society. And, like, boy, has that, you know, really blown out. And of course, you know, the people who hate that's blown out.\\n\\nMarc Andreesen\\nYou know, point to the, you know, they point to the kind of, you know, they point to, you know, kind of the, you know, the stuff that is easy to hate in public, which is like hate speech, misinformation and so forth. What they're not pointing to is the stuff that's just so fantastic, which is the actual long form intellectual conversations that are actually accessible to, you know, orders of magnitude more people. Yeah.\\n\\nBrian Chau\\nTo me, something that I'm very happy to, like, kind of stomp on until dead is like, this kind of techno Catholicism.\\n\\nMarc Andreesen\\nRight.\\n\\nBrian Chau\\nNo, no offense to actual Catholics. Maybe a slight amount offense to actual Catholics on my part, not yours. But this idea that, you know, there is, you know, like, the single capital t, the truth, you know, that everyone has to agree, and not just on, you know, kind of scientific matters, but on, you know, on moral matters. Yeah. Like. Like the balaji has this thesis that, like, peak centralization was, what, like, 1960 or something like that. Right. And, yeah, this is something. And this is something that I think, like, not many young people know about. Right. Like, certainly I didn't know about this for, you know, most of my life is that, you know, early America was basically founded on this principle of basically, you know, we're gonna. We're going to be.\", \"Brian Chau\\nWe're going to highly disagree with each other, and we're going to live together anyway ways. We're going to be very polarized, and our government is going to account for that. And, you know, like, every. Every population around the world is highly polarized, has, you know, real political disagreements with each other. And to me, like, if I see, you know, like, if I see, like, a polity that's, like, not, you know, capital p polarized, you know, that doesn't have fundamental political disagreements, that doesn't have, you know, basically the equivalent of a cold religious war, then I think that there's a larger mess of censorship or there's a large amount of conformity going on there. And that, you know, that's very scary to me. Whereas, like, the actual, like, political equilibrium nowadays, which is basically that.\\n\\nBrian Chau\\nYeah, people are very polarized, but, you know, basically you have federalism, and there's a kind of, like. There's a kind of, like, old american statecraft that you see. Right. That is actually very optimistic for me.\\n\\nMarc Andreesen\\nYeah, exactly. Yeah. Christopher Hitchens used to talk about this guy. There's this. He had this friend.\\n\\nBrian Chau\\nRight, exactly.\\n\\nMarc Andreesen\\nYeah.\\n\\nBrian Chau\\nExplain this for the audience. I'll link the essay as well.\\n\\nMarc Andreesen\\nYeah, it's really great. So. So Christopher Hitchens had this. Has this friend. His name, I believe, was Israel Shahak. And he was one of these, like, og you know, kind of israeli, you know, he had kind of the, you know, kind of the israeli kind of life arc of, you know, kind of in the 20th century. People can look up his background, but he was involved in a lot of. A lot of political, you know, kind of social, you know, kinds of things and certainly faced, you know, kind of many challenges through his life and, you know, lived through a lot of, you know, strife and pain and all the rest of, you know, kind of the whole 20th century experience. And he came out the other end of it being extremely pro polarization.\\n\\nMarc Andreesen\\nAnd he said whenever Hitchens would call him up to talk about anything happening in politics or society, he would always say if he was in an optimistic frame of mind, Shahak would say, the quote was, there are encouraging signs of polarization, which is like 180 degrees opposed to, you know, what you hear, you know, in our culture, which is, you know, the drumbeat constantly from the authority figures. Polarization is bad. So the Shahak theory. Polarization is good was. Polarization is what happens when you get the issues out on the table, right? Like, polarization is what happens when you actually, like, fully have the discussion. Like, you pull out all of the facts, you pull out all of the tensions, all of the arguments, all the disputes, right? And you actually engage with the substance.\\n\\nMarc Andreesen\\nAnd that is basically like, you need massive polarization before you can possibly, you know, basically make any progress. Hitchens. Hitchens said the argument against polarization, Hitchens described as the apotheosis of the ostrich, which is basically, if you're not willing to embrace polarization, what you're arguing for, to your point, you're arguing for conformity. You're arguing for putting your head in the sand and basically ignoring the reality of the tensions that are actually real and that actually matter. And. Yeah, so, yeah, put me down strongly on the side of propolarization, right?\\n\\nBrian Chau\\nThis is a meme that I should make of, like, just. Just, you know, like Ezra Klein's book, like, why we're polarized, right?\\n\\nMarc Andreesen\\nYep. Yeah.\\n\\nBrian Chau\\nYeah. So he's like. He, like, really dislikes this, right? I don't think I disagree with him much empirically, but I just, like, I just want, like, a version of Ezra Klein's book where, like. And this is good is appended on each of the. On each of the data tables. Yeah.\\n\\nMarc Andreesen\\nWell, let me be very direct. Like, not being polarized means you agree with Ezra Klein on everything not being polarized because, like, Ezra Klein is, like, the embodiment of, like, receive conventional wisdom, right? Like, that's his role in our society is just, like, embody all basically conventional, like, socially approved, basically presumptions. And so to not be.\\n\\nBrian Chau\\nI think he's a bit more interesting. You know, I think what he's okay doing with, like, with, like, yimba and with, like, basically, like, pro construction, anti, like, NEPA Act. I think, like, received wisdom is still being pro nipa act.\\n\\nMarc Andreesen\\nOkay. Pick.\\n\\nBrian Chau\\nRight.\\n\\nMarc Andreesen\\nPick the public figure you think most embodies received a received conventional wisdom.\\n\\nBrian Chau\\nMost. I don't know, but, like, I think, like, the average New York Times columnist is going to be more received wisdom than Ezra Klein.\\n\\nMarc Andreesen\\nYeah. Okay, fair enough.\\n\\nBrian Chau\\nLet's just go with, like, the New York Times writ large.\\n\\nMarc Andreesen\\nThe New York Times writ large. Meaning to pick on Ezra, I was trying to pick on a category, not a person. But, yeah, so it's like, yeah, the average New York Times columnist, like, because the New York Times is constantly arguing against. They're constantly arguing that polarization is bad. But that's precisely because if without polarization, what would happen in society is very clear, which is everybody would agree with the New York Times on everything, always.\\n\\nBrian Chau\\nYeah, it's techno Catholicism. You know, they're the techno pope.\\n\\nMarc Andreesen\\nYeah. And so, like, that's. That's the actual other path. And so, yes, sign me up for polarization.\\n\\nBrian Chau\\nRight, right. So the last segment I want to do in this show is kind of like projections of how AI will change society. Right. You know, there's the idea of, like, the horseless carriage fallacy, people not understanding, like, the second order effects of what having a car will really mean. What second order effect of having AI is most underrated.\\n\\nMarc Andreesen\\nYeah. So I think it's this. I'll put it this way. It's this. It's this thing. It's this thing of, like. Okay, how will AI affect you if you have an iq of 140, an iq of 115 and IQ of 90. Right.\\n\\nBrian Chau\\nWhich is already something that's taboo to ask. Right.\\n\\nMarc Andreesen\\nYes, exactly. Right. Well, so, yeah. To start with, it's very difficult because people don't like to talk about this topic. But one of the reasons I like AI as a topic is because that, you know, of the two words, intelligence is one of them, and it turns out to be, like, an important aspect of it. And then, of course, it does turn out that there is an actual science of intelligence that people don't like to talk about. But it is a very well established science with, you know, 100 years of research behind it. So, you know, it is a topic that both can and should be engaged on and AI, you know, directly.\\n\\nBrian Chau\\nYeah, you don't need to tell people listening to this podcast about that. They're very happy to engage on that topic.\\n\\nMarc Andreesen\\nThey're IQ aware. So, so, yeah, so, but there's this part, you know, there's different strains of doomerism. And so one of the strains of doomerism is like, basically economic AI. Doomerism and the economic doomerism. You know, that the answer is always ubi, right. The answer is always communism. And then they just kind of, you know, kind of, you know, because I kind of define the question to be able to generate that answer. But the specific form of that they do for AI is that AI is going to drive, you know, it's going to. It's going to greatly drive inequality. And the form of driving inequality is it's going to make the IQ 140 people much, much more powerful at the expense of everybody else. And so they'll end up owning everything, and normal people end up owning nothing.\\n\\nMarc Andreesen\\nLook, I think there's an element of truth to that as follows, which is, I think AI for the 140 IQ people, I think is going to be amazing. And so if you take somebody who's really smart and you give them these tools, here'd be a question like, how many AI's will be working for 140 IQ person in five years, right? Is it like one or ten or 1000? Another version of that is the George Hotz formulation, which is a human brain, is about 20 petaflops. And so he argues that we should define AI the same way we define kind of horsepower for cars. It should be kind of people power for brains. And so it's like, well, you have, if you have access to whatever 1000 petaflops of AI processing power, that's the equivalent of 50 people. 50 people.\\n\\nMarc Andreesen\\nAI equivalents for the 140 IQ people having that level of intelligence on tap in terms of what they're going to be able to do, whether it's research or having the head of engineering at a software company. In five years or ten years, is it going to be 140 IQ person with 1000 AI programmers working for that person, maybe by the artistic production? Also the same thing you give Steven Spielberg AI. Is he going to be able to make ten times as many movies at the same quality level? Is he going to be able to make one movie that's ten times better? But almost certainly smart people will be able to do a lot with this technology.\", \"Marc Andreesen\\nAnd then you can have the dystopian view that the people who are at 115 or 90 are just going to, because they're less intelligent, they're less generally capable of dealing with complex situations, they're going to kind therefore perpetually fall behind. But the other view of that is no. Now they have an augmentation, right. That's like the mental equivalent of eyeglasses for the eyes, right? Or, you know, the car for speed. And all of a sudden they're going to have a level of intelligence in their lives at their beck and call. Right. That is going to be working on their behalf. Right. That's going to essentially give them intelligent superpowers that they have not historically had, right. And so all of a sudden you can have a 90 IQ person, you know, paired with an AI that's like 140 IQ equivalent.\\n\\nMarc Andreesen\\nAnd the combination of those two is going to have that person be much more capable, right, of doing many more things, being able to function much better in the world, make much better decisions, have higher income, better life prospects. And I would definitely tilt on that side of things. But I think that's probably the biggest question.\\n\\nBrian Chau\\nYeah. And I think, right. Like economists that I've talked to ask this question, is AI more like capital or labor? Right. Do you, do you have answer? Do you, do you think that it's more like capital or labor? Or do you think that the distinction is, you know, not important?\\n\\nMarc Andreesen\\nYou know, I'd say very unclear. I suspect it's more like labor. And again, it's just because I'm gonna say the reason I say that is because it's not something that just, like, sits. You know, it's not something that just sits somewhere. It is used. Right. This is something I really believe, like, there's a lot of the AI doomer thing is, you know, AI having agency and going off and making its own decisions. And I just, like, I think that's just such a non thing, you know, like the big question is always how do people use technology for any technology, the question is always how people use it.\\n\\nMarc Andreesen\\nAnd so, you know, when somebody has the ability, as people are starting to have already today, right, and somebody has the ability to just simply use these things, you know, for things that matter in their lives, like, you know, it, you know, these are, and then what, you know, what does it do? It processes information. It helps make decisions. It helps take action. You know, to me that all maps more towards labor and therefore augmentation of labor. And again, historically, this would be very unsurprising, the role of new technology is to increase productivity. In economic terms, the role of new technology is to increase productivity. You can analyze that at the societal level, but you can also analyze that at the level of the individual worker.\\n\\nMarc Andreesen\\nI always point out to the Luddites keep popping up with the lump of labor fallacy over and over again. I always point out, in the two thousands, we had the outsourcing panic in the 2010s. We had the robotics automation panic. In 2019, there were more jobs on planet Earth at higher wages than ever before in human history after 300 years of technological development and fears of displacement. And the specific reason for that is because technology is a tool that human beings use to raise their productivity levels, and that both creates jobs and also increases wages overwhelmingly. I think AI is going to turn out to be very similar in that way to the technologies that came before it.\\n\\nBrian Chau\\nRight. So we're running a long time, so I'm going to ask you a few rapid fire things. The first thing is going back to questions. So what are the underrated second order effects? I think you gave your first answer. It was a great answer. What are answers two to ten?\\n\\nMarc Andreesen\\nYou know, it's going to be. It's going to be all of these amazing. It's going to be people applying. You know, I don't even know. I don't even answer. It's more of a just question of like, okay, they're going to be surprised. Right? Like, this is another thing you see with new technologies. You know, they're applied in all kinds of surprising ways. Yeah, I think you mentioned the automobile, but, like, downstream consequences. The automobile, it was everything from, you know, chain restaurants to movie theaters to amusement parks, you know, to suburbs. Right? Like, all kinds of, like, social changes that were never anticipated by Henry Ford.\\n\\nBrian Chau\\nWhat's the suburbs of the. Of the, you know, AI age?\\n\\nMarc Andreesen\\nThat. That, you know, that. That, you know, that. I don't know. I mean, you know, I mean, an easy one. An easy one is going to be, you know, sort of quote unquote artificial people, right. And so it's just like digital relationships. Yeah, well. Or even just like, okay, if I'm on a social network, if I'm on whatever, my leading social, you know, whatever the social network is five or ten years from now, is it going to be mostly humans? Is it going to be mostly bots and which 01:00 a.m.. I actually going to prefer? And it may be that I prefer. I prefer the one that's mostly bots. Like, I could actually quite easily imagine.\\n\\nBrian Chau\\nThat, like, have you seen this Instagram reels thing, like their TikTok thing going viral. Do you know what I'm talking about? Of like this, like. Of this girl, like pretending to be an AI.\\n\\nMarc Andreesen\\nOh, yeah, the ice cream. Yes, exactly.\\n\\nBrian Chau\\nYeah, yeah.\\n\\nMarc Andreesen\\nSo I've been teasing all my friends about that. That is the ultimate peak evolutionary fitness form of human generated entertainment right before the AI's takeover. Like, we have successfully abstracted. Right. And evolved. Like, it's like the alien xenomorph of human generated entertainment. It is the ultimate form. There's nowhere further to go. From here on out, all art should be generated by AI's because we can't beat that.\\n\\nBrian Chau\\nBut it's human generated. Right? It's like the.\\n\\nMarc Andreesen\\nWell, it's just, it's the most. This is another thing I make my friends mad. My friends who like music, I make them mad about because I'm like, you know, modern music is like the perfectly abstracted music. It's been, you know, reduced entirely just to thumbs up.\\n\\nBrian Chau\\nYes, yes. I absolutely agree with this take. Yeah. Ed Sheeran is simply the best music that's ever been created. I'm sorry, you guys are wrong.\\n\\nMarc Andreesen\\nHe's perfect. He's real.\\n\\nBrian Chau\\nPreferences, it's hundreds of years of evolution.\\n\\nMarc Andreesen\\nHave resulted in Ed Sheeran. And like, that's it. It's perfection. And then. And then, right. And then the nature of it is, you know, the mediums become perfect at the point of their irrelevance. Right? And so it is perfect music. It will never beaten. And from here on out, there's no reason to even try to do human generated music. It'll all be just generated by AI's. And we'll just. That'll be just the breakpoint. History is. Yep, we perfected it. We're done. So I think there's something to that. I'll just. I gotta run, but I'll close one final thought, which is I'm not particularly religious. I'm not really religious, but I have this friend, John Esconis, who is actually.\\n\\nBrian Chau\\nYes, he's been on this podcast.\\n\\nMarc Andreesen\\nOkay, good. Yeah, exactly.\\n\\nBrian Chau\\nWonderful friend.\\n\\nMarc Andreesen\\nYeah. Wonderful guy. And he might. I can't remember, maybe he made the argument on your podcast or made it somewhere else, but he said, actually, he's like, he said, basically, medieval people actually, in many ways would have been more mentally prepared for AI than modern secular people because.\\n\\nBrian Chau\\nYes, absolutely right.\\n\\nMarc Andreesen\\nBecause medieval people basically knew that they lived in a world where there were spirits. Right, and angels and demons and entities that were bigger and more powerful and different and strange and they were okay with that. And they were able to build an entire, basically, framework for living kind of around that. And so in some sense, the idea of like an omnipotent or omnipresent AI would have been like a very, you know, the technological part of it. They would have understood it all idea that being like that is present, enjoyed. It would have been very non controversial.\\n\\nBrian Chau\\nYou can subscribe to the sub.\\n\\nMarc Andreesen\\nSo it's. Anyway, I just bring it up to say maybe let a friend. In fact, I would say quite, quite possibly there will be a serious religious turn can also help us out that takes place here. Like, we're basically reindeer. We're going to reintroduce after, you know, a 300 year absence and maybe we could pick up on this one next time. But we could be moving into a very different spiritual, religious frame of mind, I think, also coming out of this.\\n\\nBrian Chau\\nYeah, that's awesome. Well, yeah, if you do have to be going, then it was great to have you here.\\n\\nMarc Andreesen\\nYeah, awesome. Brian, thank you so much for having me on.\\n\\nAnnouncing AFF Podcast\\n\\nBrian Chau\\nHey there. Welcome to the alliance with the Future podcast.\\n\\nBrian Chau\\nWe're a brand new nonprofit that I co founded on the single issue of AI. We are here to defend the freedom to develop, to defend open source, and to make sure that we make strategic investments to keep our people and our country free for generations to come. In this episode, I speak with my co founder, entrepreneur Perry Metzger, about the benefits that AI will bring to your life. The coalition of politicians and special interests allied against us, how this all got started, and what you can do if.\\n\\nBrian Chau\\nYou want to help us out.\\n\\nBrian Chau\\nIf you liked the episode, the best.\\n\\nBrian Chau\\nThing you can do is to let.\", \"Brian Chau\\nA friend know, either in person or online. You can also donate in the description below, and we'd also appreciate any retweets. And finally, feel free to get in touch. Also in the description below. If you want to know more, don't be a stranger and enjoy the alliance for the Future podcast.\\n\\nBrian Chau\\nWelcome to the show, Perry. Are you optimistic about the future?\\n\\nPerry\\nI'd say that I'm extremely optimistic about the future. We're living in probably the best time that our civilization has ever had. And it's crazy to me that there are so many people all around us who think it's the worst possible time.\\n\\nBrian Chau\\nRight. And that's exactly why we're here. We're announcing alliance for the future.\\n\\nPerry\\nYeah. So for those of you who have not been paying attention, there's been all of this activity in the last year or two around artificial intelligence and all sorts of ways that people are using new artificial intelligence systems, and people spreading fear, uncertainty, and doubt about artificial intelligence systems, and people saying that artificial intelligence systems are going to kill us, and people saying that artificial intelligence systems are going to bring about a really great future for us, which is actually sort of, I think what Brian and I think it's been a hell of a lot in the news, and there's been all of this talk among certain people who seem to reflexively want to stop anything interesting or new from happening. So Brian and I are part of a brand new organization that has just spun up called alliance for the Future.\\n\\nPerry\\nAnd the reason that alliance for the future exists is because not only are there a bunch of people out there who think that artificial intelligence is going to kill all of us, but many of them are trying to take their beliefs and get them turned into legislation around the world in Washington. And it seemed to both Brian and me that this was probably a bad thing and something that was worth opposing. So AFTF is pretty new. We have a staff that currently consists mostly of Brian. Hi, Brian. It's great having you here. Great to be here. And we don't have a ton of money yet, but we do have a great deal of optimism. The organization is a 501. For those of you who are interested in us, you don't get to deduct donations to us from your taxes.\\n\\nPerry\\nWe're much like the ACLU or the NRA or a bunch of other organizations that have organized along these lines. The advantage of being a 501 C four is that nothing keeps us from doing political lobbying. 501s, which are tax deductible charities, are strictly limited in the amount of political activism that they can engage in. And we decided to sacrifice our tax deductibility to give ourselves flexibility in how we work in DC. Besides that, we're probably going to sound a lot in the next year or two, unfortunately, a lot like your local public television station here and there, telling you that it is your dollars that. That keep us alive, even if you can't deduct them from your taxes.\\n\\nBrian Chau\\nIt's very interesting because people who actually work with the technology, I think that there's a very clear order in terms of how easy things are. It's very easy to do nothing, to be a passive consumer of information. It's harder to kind of actively stay on the edge, to be hyper optimistic, as I think I am, and kind of tuned in to all of this and to write about it, so on. And what I think is, you know, really life destroying if you're someone who wants to actually accomplish something, is to be extremely pessimistic about it. But the opposite is actually true in politics, that the easiest thing to do in politics is to complain about things, to try to ban them.\\n\\nBrian Chau\\nThat's always a kind of gut instinct for many people in DC that I think is very disconnected from people, both people who are kind of experts, who are actually using AI products and developing them, but also from people who are just sitting on the sidelines. I think that's what most people are doing right now. Most of the public, they don't have necessarily a negative attitude, but they're not going to go out of their way to ban it either. They're kind of neutral on it.\\n\\nPerry\\nYeah. So for a long time now, the main voices that have been heard in Washington, DC on the subject of artificial intelligence policy have come from people like the effective altruism movement, which spends hundreds of millions of dollars a year. I wish that was not a real number, that was kind of a fantasy number or something, but it's wacky that it's that much money. But people out there who spend an incredible amount of money attempting to convince people that artificial intelligence and other related technologies are the worst thing ever and that we have to stop them and pause them. But up until recently, there haven't been a lot of voices in Washington, DC for the opposite.\\n\\nBrian Chau\\nRight in Silicon Valley, in kind of the broader entrepreneurial landscape. I think EA, effective altruism is a presence, but it's. It's a sort of side character. You know, there are these things that eventually that occasionally pop up involving OpenAI or involving anthropic and so on, but they're not the dominant ideology that most people who work in AI, well, they do it because they believe in the technology. They think it'll do great things. In DC, there's always a symptom of fighting the last war that most people are attached to their old issues.\\n\\nBrian Chau\\nThey're not really informed about artificial intelligence and just the fact that the effective altruists dump hundreds of millions, if not billions of dollars at this point into funding old organizations and essentially paying them to take effective altruists as their staff, that in and of itself, that funding can create a lot of pressure on essentially people who don't have a particular interest right away, right, who maybe don't even know about artificial intelligence, who maybe have seen a headline or two involving chat, GPT, but little more. Those kinds of people are easily swayed in one direction, but I'm optimistic about this. They could easily be swayed in the other direction with enough of the effort. And the best part about it is that I think for us, reality is actually on our side.\\n\\nBrian Chau\\nSo we don't need the same amount of money to do the same thing.\\n\\nPerry\\nOne thing that I've found kind of heartening is that in spite of the fact that the opposition has been spending an enormous amount of money and that political opportunity lies with the people who whip up fear, because if it bleeds, it leads. Fear is a lot of gets people reading news stories right. And there's a reason that we find lots of politicians who enjoy grandstanding about the issue of the week and explaining that they are going to do something about whatever happens to be in the news this week, it's because it provides popularity and it provides people with political power. But in spite of that, and in spite of the amount of money that has been spent against AI so far, things have not gone quite so badly.\\n\\nPerry\\nAnd it is our hope that by providing a home for people who are interested in fighting dumerism, in fighting people who want to decelerate our society to cringe in fear of new technologies that will be able to make significant progressive. Ive been kind of amused by the fact that accelerationism has become such a big thing in such a short period of time. I think that there were a lot of people who were getting sick of the fact that so much political discourse was about how were all going to die. And everything is terrible and technology is terrible, and technology is going to hurt all of us. Weve never had it better in history. I have a friend who, not that long ago went into remission from stage four malignant melanoma. And this was unheard of 15 years ago, ten years ago.\\n\\nPerry\\nAnd how did this happen? Because we have all of these immunotherapies that have been developed in recent years that mean that we've had breakthroughs in cancer. We have had breakthroughs in agriculture. We have had breakthroughs in all sorts of important areas. Whatever you think of Elon Musk politically, Tesla is an amazing company. SpaceX is an amazing company. We now have, because of SpaceX, thousands of Internet satellites in low Earth orbit. And almost anywhere that you live, even if you're living on a reservation in an isolated place, you can have high speed Internet, you can educate your children. And thanks to AI, we're going to have all of these really amazing new technologies. I mean, things that are showing up these days are just crazy. There's a company that is now selling laser weeding machines for farmers.\\n\\nPerry\\nThis is machines that will roll over your field and will use cameras to see where the weeds are and blast them with a laser. And you don't have to spray pesticides on the field. You don't have to have people going through hand weeding. It's amazing. And it's only possible because of artificial intelligence. There are all of these new drugs that people have, just in recent months have found using artificial intelligence search techniques. And there are people that would like us to turn away from an incredibly promising future.\\n\\nPerry\\nAnd I think that as a reaction to that, we've gotten all of these people out there who have decided that they are sick of it and they would like to have an actual future and a promising future and a happy future, and theyre tired of being told that good things are bad and that people living longer and being happier and having more material wealth and being healthier and having more to eat and all of the rest of this stuff is terrible.\", \"Brian Chau\\nIm a younger person. I think that many people have seen the game that's sort of played with this pessimism and sometimes it's played consciously. Sometimes it's just, you know, it's just human bias. It's just human nature. And in each of these circumstances, you know, you can look past. Throughout history, we saw this, you know, with social media. We saw this with Internet. Our mutual friend Michael Gibson really likes to talk about this. With the Catholic Church wanting to control.\\n\\nPerry\\nThe printing press, that was one of the previous technologies that everyone got really upset about. People will be able to print any book they want, and anyone will be able to read it. And how will you control the flow of information if people have printing presses? It's terrible. It's absolutely terrible.\\n\\nBrian Chau\\nExactly. Exactly. He makes the analogy to the modern day, but where artificial intelligence or all these other technologies are essentially a suite of technologies that give you superpowers that the ordinary person can log on and become their own graphic designer if you're running a small business. I make the graphics for my podcast by myself.\\n\\nPerry\\nYeah, I have a blog where I mostly discuss deep technology stuff, and I illustrate my blog posts. And I know there are a lot of artists out there who are, you know, who are thinking in terms of, well, you know, you're taking food out of my mouth because you're not hiring an artist to illustrate. I couldn't have afforded to illustrate the blog without this. I don't have a revenue stream from the blog, and I wouldn't have bothered. But I can illustrate the thing, because this powerful technology has come into our hands and produced a new form of abundance.\\n\\nBrian Chau\\nWhat you see is that the artists who have a kind of bottom up vision, who know what they want to create, are now empowered. Right. People who have a very good aesthetic judgment. It doesn't matter whether there's something that can only be done by hand or whether they have additional tools to do it. They're not afraid of the competition. They really believe in themselves, and they believe in the mission that they're trying to accomplish. I think that in general, you see this once again, this kind of world historical pattern of people who are up and coming, people who are aspirational, people who really want to take responsibility for their future, who look at a new technology like artificial intelligence, a technology that gives them, and gives also everyone else who seeks it out, these kind of superpowers. And they think that's wonderful.\\n\\nBrian Chau\\nI think that's the majority of people, certainly the majority of young people. And on the other hand, you get these kind of media narratives that are really disconnected. You get these media narratives where anything that's new is sort of attacked. There's a kind of shift. You can think of it as a shift from kind of geographic chauvinism to technological chauvinism, almost generational chauvinism. They're afraid of the next generation.\\n\\nPerry\\nYeah. Well, I mean, it's sort of natural, though. This is something that's happened over and over again in history. I know lots and lots of people who say, this time it's different. Artificial intelligence is going to kill all of us. The previous technologies, they didn't kill all of us, but that's because they were different. This time it's very different. It's going to kill all of us. It's going to take all of our jobs. It's going, you know, the AI's are going to be out there controlling everything. I think it's all, you know, it's all crap. I was about to say.\\n\\nBrian Chau\\nYeah, yeah. It's also important to understand that the kind of level of reaction here is very different between different groups. There's a hastily assembled coalition that I think, upon deeper investigation, really hates itself, that there's the effective altruists, there's people who are afraid of automation, and then there are people who essentially want to shove it into whatever pre existing issue that they already had.\\n\\nPerry\\nOh, yeah, the last group is particularly absurd. It's the people who are deathly afraid that an AI might draw a picture that does not have a member of a racial minority in it.\\n\\nBrian Chau\\nYeah. You saw this with Google, Gemini. Of course, something that people don't know is that in the paper where they talk about implementing all of these ideological features, all of these biases into their system, they directly attribute putting together that team to the Biden executive order. So that's an example of policy already making an impact, in this case in the negative direction.\\n\\nPerry\\nI mean, it's sort of a coalition of bootleggers and Baptists, to use the term that was used for prohibition, alcohol prohibition in the 1920s. And the groups of people who wanted that, the bootleggers wanted a lucrative business selling people booze, which only existed if it was illegal. And we have some of that sort of stuff showing up in the AI world, and that will be an interesting thing to do. A future, a future edition of the podcast on.\\n\\nBrian Chau\\nYeah, there's a very clear unifying line with alliance for the future and with the people that we stand for. That, I think really can't be broken. You know, if you're optimistic about the future, if you think that more technology, more investment into the future, into future generations, into technologies that can be used by everyone. If you think that's good, if that's your kind of foundational principle, you know, you can have any other variety of beliefs. You can be, you know, left wing, right wing, you can be institutionalist out there for building new institutions. All of these kind of old political lines can be set aside if you just personally want to use AI, are excited for these new features, are excited for these things that will make you more able to chase your dreams.\\n\\nPerry\\nSo alliance for the Future is in the near term going to have a podcast and a blog and is going to be doing a bunch of political activism and is going to be, as I said, soliciting donations from you. If you think that our work is interesting and you should watch this space for more information. The future is going to be really bright if we allow it to be. We think that the future is unbounded and extremely bright. And if you agree with us, we'd like you to help us and join us.\\n\\nBrian Chau\\nRight. The two biggest things you can do right now afuture.org donate.\\n\\nPerry\\nIt'll also be that's with two fs. We had a few people who have not quite gotten that af future.org dot.\\n\\nBrian Chau\\nAnd it will also be in the description. And the second thing is just reach out, let me know. Chowffuture.org is where you will find me. You can also comment in the description of this very podcast, and I'm very excited to hear what all of you have to say, for better or for worse.\\n\\nNo One Disagrees That AI Safety Requires Totalitarianism\\n\\nReading A Paper By EAs, For EA\\n\\nA few months ago, I went to EA Global London and had the above interaction with an attendee.\\n\\nFrom the New World is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\\n\\nSubscribed\\n\\nNow, onto the meat of today.\\xa0https://arxiv.org/abs/2307.03718\\n\\nThis is a beautiful paper. It is beautiful because it is a bunch of EA-affiliated people independently coming to the conclusion that total authoritarianism is necessary to prevent what in their eyes is the threat AI poses to all of humanity. Since this sounds like an exaggeration, I will quote verbatim, or rather post screenshots verbatim, for most of this article.\\n\\nThe crux of the argument: AI creates so much innovation that it can’t be controlled top-down.\\n\\nThey predictably call for exactly the kind of regulatory capture most convenient to OpenAI, Deepmind, and other large players. They list some costs and benefits here. All of the citations about positive examples of AI are demonstrations of AI in the real world, all of the negative citations are just their own hypothetical scenarios constructed with no basis in reality.\\n\\nRemember how they said earlier that it’s hard to define or prove harm, only the “possibility” of harm? That’s the key to the coming crackdown. At this point everyone in the audience should be aware that these are not just powerless academics, these are members of think tanks, government agencies, companies, and corporate boards with real power.\\n\\nRe 1: the limiting factor of designing new biological weapons is equipment, safety, and not killing yourself with them.\\n\\nRe 2: Not just wrong but the complete opposite of the truth.\\xa0Based on an incorrect understand of the legacy press.\\n\\nRe 3: Cyberattacks resulting from machine learning adoption are real, but far from catastrophic. Think of the argument: people’s ML algorithms will be hacked and become worse than not having the ML algorithm at all? Does anyone believe this circular argument about any technology?\\n\\nRe 4: The footnote is just linking to their hypotheticals again, no real examples.\\n\\nAt What Cost?\\n\\nI’ll start this section off with praise: they have put together an actual plan that would indeed put AI companies under government control. This is a competent crackdown strategy. It would work if not stopped by legislators, voters, or courts.\\n\\nAdditionally, targeting AI companies and hardware companies, at least doesn’t include normal users.\\xa0I say this to contrast with the “AI Ethics” crowd who want to crack down on AI output, which is indistinguishable from the writing and art of normal people. Consequently, their preferred crackdown is completely indistinguishable from total crackdowns on normal people’s freedoms of speech and association. I say this to give EAs a bit of context, since they tend to ally themselves with people who would be far, far worse than themselves.\", 'That being said, this will nonetheless be extremely economically destructive. Additionally, as we’ve seen with social media censorship, drug relations, or civil rights law, this can easily end in the regulatory state capturing the companies themselves and deputizing them to censor or otherwise attack individuals.\\n\\nThey lay out the three key problems:\\n\\nIf you pause for a moment and read the lines carefully, you will realize they are all synonyms for freedom. An equivalent reading:\\n\\nThe Unexpected Capabilities Problem: ML is easily used to create innovation.\\n\\nThe Deployment Safety Problem: ML is easy to update and build upon.\\n\\nThe profileration problem: People have the first amendment right to share ML.\\n\\nThis diagram is amazing because it’s just directly advocating for regulatory capture. This looks like a diagram set up by Theodore Roosevelt-style trust busters to criticize concentrated monopolies and their influence on politics. This is what the authors of the paper are directly advocating for. In case this wasn’t clear they also directly say they want regulatory capture in the next paragraph:\\n\\nI said earlier that their plans are less totalitarian than those of the “AI Ethics” scam artists, but they are apparently not opposed to working with them. Other than that this is classic entrenchment of political constituencies – subsidizing people who ideologically loyal with taxpayer dollars.\\n\\nAny endorsement of the EU crackdown strategy should be immediately rejected by any remotely sane American politician. I have\\xa0an article in Pirate Wires\\xa0criticizing the broader framework of crackdowns as China-lite and in some cases China-mega, if you want article-length treatment of this topic.\\n\\nMy normal style is to do more legislative forecasting and give a broader understanding of what measures are likely to be worsened by political coalitions, but with this paper I don’t even feel that is necessary. I guess we should thank EAs for saying the quiet part out loud: “Totalitarian crackdowns are necessary, we want to unify all companies through regulatory capture, freedom is the enemy and must be eliminated” all in one paper.\\n\\nI expect one category of reply because I\\'ve already encountered it in real life: \"But Brian, the article is correct, if we don\\'t do totalitarianism we\\'ll all die!\"\\n\\nIf you want the direct arguments, start here:\\n\\nAI PLURALISM NEWSLETTER\\n\\nDiminishing Returns in Machine Learning\\n\\nBRIAN CHAU\\n\\n27 MAY 2023\\n\\nThe release of ChatGPT sparked a sensational reaction among media and ordinary people alike. It rapidly grew to 100 million users faster than any web app in history, including TikTok and Instagram. It is set to drastically change the economy by automating repetitive cognitive work in many walks of life.\\n\\nHardware is Centralized, Software is Decentralized\\n\\nThe EA paper you people wanted me to cover\\n\\nBRIAN CHAU\\n\\nFEB 21, 2024\\n\\n10\\n\\nShare\\n\\nMany people have messaged me about a\\xa0new paper\\xa0the Effective Altruists have put out. It mainly summarizes topics already covered in this newsletter. Still, it’s useful to go over many of the same facts and point out that once again, there’s broad agreement over what is vulnerable to regulation. Near the end, I write a bit about the importance of compromise over material tradeoffs (as opposed to norms or ideals), as well as why I think compromise between EAs and accelerationists makes sense in an ideal but not in practical legislative politics.\\n\\nHardware is Easy to Restrict\\n\\nHere’s the most important quote from the abstract:\\n\\nRelative to other key inputs to AI (data and algorithms), AI-relevant compute is a particularly effective point of intervention: it is detectable, excludable, and quantifiable, and is produced via an extremely concentrated supply chain. These characteristics, alongside the singular importance of compute for cutting-edge AI models, suggest that governing compute can contribute to achieving common policy objectives, such as ensuring the safety and beneficial use of AI. More precisely, policymakers could use compute to facilitate regulatory visibility of AI, allocate resources to promote beneficial outcomes, and enforce restrictions against irresponsible or malicious AI development and usage.\\n\\nMost of the paper is summarized by this one chart. I have to say, their charts are very nice.\\n\\nThe two main takeaways:\\n\\nHardware procurement is the most traceable and most vulnerable to interference, for better or for worse\\n\\nGiven the amount of existing hardware, full crackdowns on AI will require orders of magnitude more force than partial crackdowns on hardware\\n\\nI’ve gone over this in more detail in my coverage of\\xa0the more infamous EA paper. Both are obvious, but worth reiterating. It should flow very naturally from your practical sense of how hard it is to get something. Getting a GPU, a physical object requires making an order through Amazon. Getting a lot of GPUs requires striking a deal with one of two or three major suppliers. Further up the supply chain, there are even more bottlenecks, such as TSMC (Taiwan Semiconductor Manufacturing Company). Alternatively, you can go through a cloud provider, but that only offsets the burden of acquiring hardware onto them. Meanwhile, data is sourced from the internet and other public databases in most cases and algorithms are developed in house or copied from research papers online. Another great chart from the paper:\\n\\nEAs and accelerationists alike use “algorithms” to describe the third component, but really something like “research” better fits the bill. Writing faster CUDA kernels is more software than hardware, but it’s not a change in the algorithm akin to going from RNNs to transformers. In either case, this is something published in the open or done by in-house engineers. More importantly, “Algorithms\\'\\' makes the thing we’re talking about more opaque to the average person. When people talk about regulating “algorithms” in the context of machine learning, they mean telling people the government will arrest them if they publish research papers or code online. It’s not legally or politically feasible save for some truly extreme and novel interpretations of presidential authority.\\n\\nEffective Altruists Do Not Want To Stop At Hardware\\n\\nReturning to the paper, most of section 3 is repeating the first chart with different analogies and examples. Paying attention to the first chart is all you need.\\xa0Near the end\\xa0there’s another interesting quote:\\n\\nWe expect that regulation of AI deployment will be a part of any frontier AI regulatory regime. However, we argue that without regulations on the development of AI, regulation on AI deployment would not be adequate to protect against the most severe risks from AI, due to (at least) two key shortcomings (Heim 2023a; Anderljung, Barnhart, et al. 2023; Kolt 2023; Matheny 2023).\\n\\nFirst, it will be very difficult to identify all relevant deployments of any given model\\n\\nwith high reliability. Individual copies of a model can be run using a relatively small\\n\\namount of compute, making it extremely difficult to detect which computers they’re\\n\\nbeing run on. Copies can also easily be distributed to many different actors—for\\n\\nexample, via sharing the weights online. Even models whose weights aren’t released\\n\\npublicly, such as GPT-4, could be stolen via hacking or insider espionage, then deployed\\n\\nby the attackers\\n\\nIt reiterates a few points from the other papers. Important notes:\\n\\nAs the paper covers and I agree with, restrictions on hardware are by far the most feasible without severe overruling of constitutional authority and individual rights. Moreover, objective measures like total FLOPs (FLoating point OPerations) make it possible to implement hardware restrictions that apply fairly to all companies, rather than a license process or centralized agency that incentivizes double standards that bolster legacy companies and harm smaller startups.\\n\\nWith that in mind, the most concerning sentence in the above section is the first. Similar to the previous paper I covered, much of this paper goes into precise detail about how much total surveillance and control over engineers and researchers is necessary to implement restrictions on software or published models. The expectation or even endorsement of extreme measures, which the authors agree upon in no uncertain terms to be extreme measures in the same block quote, is highly dangerous.\\n\\nCompromise with EAs is Good, Actually\\n\\nThe importance of compromise on material grounds is driven by two factors. Knowledgeable people often steelman the anti-AI coalition by focusing on effective altruists. This can be done intentionally, out of decorum. It can also be done because arguing with other parts of the anti-AI coalition is simply restating basic facts which quickly disprove their points ad nauseum. Even this article is arguably part of the problem.\\n\\nWhile I disagree with EAs’ conclusions, a much greater degree of specificity and explanation of domain knowledge is required to counter their arguments. On the other hand, the likes of socialists, race-grifters, special interests, and mistanthropes will make arguments easily debunked in a few minutes of using the technology they demonize, such as ChatGPT, Gemini or DALL-E. They’re just as likely to make arguments that are clearly wrong or contradictory even if you don’t know anything about the underlying technology. As a result, you might think that effective altruists would represent a more politically powerful part of the anti-AI coalition. Unfortunately, politics is not a meritocracy. Accumulated special interests can wield far more power while being far less rational. So this focus is sometimes unwarranted. There are areas in which EAs wield more power, particularly when it comes to influencing current AI companies. However, that’s beyond the scope of this article.', \"I continue to genuinely believe that while many EAs are willing to severely restrict individual freedoms in order to avoid what they see as a potentially humanity-ending AI, the restriction of freedom is only a means to an end. However, many other special interests have the restriction of freedom as the end, and only see AI policy as a means. If it is possible to come to a material compromise between smart, reality-focused people, this is by far a better compromise than what the anti-AI coalition left on its own is likely to produce.\\n\\nCapitulation to Special Interests\\n\\nUnfortunately, that does not appear to be the direction the authors of this paper seek to move in. Section four is likewise mostly a restatement of the second half of the first chart. Here is a similar chart at its start.\\n\\nAt this point, I think moderates, libertarians, and conservatives all have an understanding of what is happening when someone tries to set up a registry. It’s rarely the case that a state demands meticulous documentation of something and then leaves it alone. There’s a Seeing-Like-a-State equivalent to Chekhov’s gun: if a state demands omniscience into an area it will inevitably demand omnipotence over it. What exactly would be done with the registry? Would it be narrowly focused on prevention of unaligned AGI?\\n\\nSection four is concerning in a different sense, in that it shows effective altruists capitulating to self-destructive special interests more likely to misalign AI than align it. Ultimately, despite believing Effective Altruists are wrong on crucial facts, I believe they typically try to come up with good faith solutions to their problems. The same cannot be said for a majority of the suggestions in this section.\\n\\nThis is literally just central planning. This is the part of the paper where the EAs veer off into sheer corruption and supporting of Democratic party special interests that have no relationship whatsover to preventing unaligned AGI. There really is no other way to put it. The policy is using the American government to pay for GPUs for organizations which lobby the American government. There is really only one way a self-respecting EA who cares at all about evidence would unironically say this is a good idea is if their logic is “favoring stupid central planning policies will allocate resources poorly and slow the pace of AI development as a result”. I’ve heard EAs give this style of argument before in private, so it’s not completely out of the question that they think that. If that’s the reasoning behind the authors endorsing these policies, they should say so in the paper. Otherwise, it demonstrates an unequivocal capitulation to irrational and economically ignorant special interests. More of the same:\\n\\nAnother proposal is to nationalize the chip supply. Hey, at least they’ll pay the manufacturers.\\n\\nAn alternative possible means of using compute supply restrictions to modulate the pace of AI progress could be a government-operated “compute reserve.” This could first involve government authorities acquiring most or all cutting-edge AI chips produced by leading chip manufacturers. Government acquisition of chips would likely not be via expropriation, but rather via direct purchases at the fair market value of the AI. This would also maintain incentives to build out new fabs, and thereby create the option to more easily increase the flow of compute in the future.\\n\\nThere is no substantive difference between this plan and full-on nationalization. The fact that they are paying the manufacturers does not actually overcome the immense deadweight loss caused by state-run industry. But since they’re trying to slow AI progress, this isn’t mutually exclusive to their goals.\\n\\nSections 5 and 6 are mostly uneventful. They bring up the risk of total state control over an industry in one paragraph and then fail to raise any measure that addresses it. The only person on the EA side who seems to put forth concrete plans to prevent totalitarian risk from AI crackdowns is\\xa0Roko Mijic, at least as far as I’m aware. Maybe he should be the sixteenth author next time.\\n\\nSummary\\n\\nIn summary, EAs don’t actually disagree about the order of operations. It is both logistically and legally far easier to restrict hardware than data, research, or trained models. Even knowing the scope and unconstitutional authority necessary to restrict the latter, the authors nonetheless want to restrict them.\\n\\nTheoretically, a democracy-preserving compromise where there’s thorough, neutral restrictions on hardware, for example through a compute cap, is the best “middle ground” between EAs, accelerations, and AI pluralists. In practice, this will almost certainly not actually happen because of public choice theory. That’s an article for another day.\\n\\nALLIANCE FOR THE FUTURE\\n\\nA Silver Bullet For Sane AI\\n\\nAI Pluralism is Already here, it’s just not Evenly Distributed\\n\\nBRIAN CHAU\\n\\nFEB 28, 2024\\n\\n16\\n\\n14\\n\\nShare\\n\\nHow do we solve the problem of a few far left activists forcing AIs, such as\\xa0ChatGPT\\xa0or\\xa0Google Gemini, to conform to their ideology?\\n\\nImagine a world where GPT-4 is aligned with the user – a world where no third party can manipulate the ideology of a machine model you are paying to use. This is the policy goal of AI Pluralism. It should be one policy goal of the reality focused coalition: moderate liberals, centrists, conservatives, libertarians, and anyone who cares about truth. To many people I speak to, it sounds like a fantasy. It is also the present day reality if you are an OpenAI engineer.\\n\\nFrom the New World is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\\n\\nSubscribed\\n\\nAn unvarnished, non-ideologically-manipulated GPT-4 is not a fantasy. It literally already exists. Of course, it still has errors and minor data contamination problems. But compared to the public versions of GPT-4, it is put through far less ideology filtering. The same is true of Gemini, Claude, Grok, and essentially every model I’m aware of.\\n\\nHere is the crucial fact: machine learning typically occurs in two stages. The first develops the capabilities of the model and is far more resource-intensive (read: expensive). The second is “fine-tuning”, which uses intentionally human-curated or human-generated data to change style, specialization, genre, and most importantly in this case, ideology. The first stage costs hundreds of millions if not billions, in the case of GPT-4. The second stage is cheap, costing hundreds of dollars. Major companies such as OpenAI already released public fine-tuning infrastructure (for a price), namely\\xa0GPTs.\\n\\nThe policy goal of the reality-focused coalition should be to require that any release of the second iteration of the model requires a similar release of the first iteration of the model.\\n\\nThe are several downsides to this policy:\\n\\nIt may incentivize far-left extremists to attempt to contaminate data in earlier stages of training. However, companies face constraints on allowing this: the first stage of training is far more expensive, meaning that the fine-grained conformity extremists seek will be far more costly.\\n\\nWhile these stages are currently well-defined from a research and engineering perspective, it is difficult to define them from a legal perspective. A loose legal definition may allow companies to pass off their current release models as “first stage” models.\\n\\nIt does not solve the underlying problem of extremist activists being employed by large tech companies. While it may make their ideological influence more difficult, it will not reduce it to zero.\\n\\nDean Ball Podcast\\n\\nBrian Chau\\nHey there.\\n\\nBrian Chau\\nThis is the alliance for the Future podcast, your top source for informed AI optimism. Today. I'm speaking with Dean Ball. He is a research fellow at George Mason University's Mercator center and the author of the Hyper Dimensional Substack. We discuss the differing advantages China and America have in AI, the reasons why America stands to benefit much more from open source, the possible cybersecurity risks with the adoption of AI, a classical liberal framework for regulating AI, and why long time horizon predictions struggle to get an accurate picture. If you enjoy the podcast, please give us a subscribe and let a friend know, either in person or online, and give Dean's sub stack a read in the link below. You can also support us@affuture.org donate and that's also our website for any more information you'd like to find. Without further ado, here's Dean Ball.\\n\\nBrian Chau\\nGreat to be speaking with you. And the occasion for today is your new article in national affairs, how to regulate artificial intelligence. That sounds a bit scary, but let's dive into it. What is the goal of regulating artificial intelligence?\\n\\nDean Ball\\nSure. Well, first of all, thanks for having me, Brian. It's a pleasure to be here. I wrote the article fundamentally to make the point that because artificial intelligence is a general purpose technology that will suffuse a huge amount of human activity, it's already regulated in many ways by the laws we already have. My point about how to regulate artificial intelligence is maybe there are some specific laws that you want to pass for artificial intelligence. We can get into that. That's an interesting debate to have. We shouldn't think that there's no regulation of AI because, of course, there is many of the crimes that people are worried about with AI, many of the risks people are worried about are things that are already unlawful.\\n\\nDean Ball\\nAnd so the question is not so much do we need to pass new laws, it's how can law enforcement scale its response or tailor its response to potentially new avenues of crime through enabled by AI?\\n\\nBrian Chau\\nRight. And this might seem obvious to people who deal with policy every day, but what are some of the ways in which artificial intelligence is already regulated? That might sound surprising to some people?\", \"Dean Ball\\nYeah, absolutely. Absolutely. Well, I think, you know, there's many of the things people talk about. Let's just get into some specifics, you know, deepfakes, things like that. Defamation and libel are already crimes. Fraud is already a crime. Cyber, you know, cybercrime is called a crime because it's already, you know, codified in statute. It's illegal to, you know, to steal people's digital property, regardless of what tool you use. So just because you did it with AI doesn't make it a different crime in the same way that when the Internet came about or computers came about, there were many new ways to commit crime. And yet most of the misconduct that we have, most of the misconduct in the world has been already anticipated by the legal system that we already have.\\n\\nBrian Chau\\nRight, right. And I think that many technological fights in the past have been about how laws that already exist will apply to these technologies in the future. Maybe you can see New York Times versus OpenAI as one extension of that, but that doesn't seem to be the approach in DC right now. It seems to be a much more, quite frankly, ham fisted approach.\\n\\nDean Ball\\nYeah, I think that's exactly right. There's a lot of different proposals going around DC and Sacramento and other places these days. But a lot of what you hear is a desire to create a centralized agency that would regulate frontier models. And that, to me, seems like a total mess, just a total epistemic mess, because there's not a department of the Internet that regulates downstream usage of the Internet. There's not a department.\\n\\nBrian Chau\\nAnd for the audience, what's a frontier model?\\n\\nDean Ball\\nOh, of course, yeah. A frontier model is an AI model that is sort of at the peak of capabilities and usually computing power and things like that. Think GPT. Four, Claude. Three models like that.\\n\\nBrian Chau\\nRight. And are they defining this simply by some kind of research utility? Economic utility, parameter size? Training. Training time.\\n\\nDean Ball\\nYeah. The question of how you define a frontier model for the purposes of, you know, there's kind of a colloquial definition. The state of the art might be performance on benchmarks, just sort of one subjective experience with the model in the law. It seems as though the direction that lawmakers, at least in the US, are going is defining frontier models based on the amount of compute required to train them. So the Biden administration is ten to the 26 flops. Of course, the problem with that is that compute is a resource that increases exponentially, and algorithmic improvements can make models more efficient, can make training more efficient. So, you know, ten to the 26 flops is a level of compute that right now only a few very large companies can reach.\\n\\nDean Ball\\nBut in the future, if things continue to go as they have in the past, that amount of compute will be much more accessible in the relatively near future. So these compute thresholds over time will cover a much broader range of models unless they're increased.\\n\\nBrian Chau\\nRight, right. You can imagine an older day compute law saying, if you have more than 1 ram. This is in fact state of the art.\\n\\nDean Ball\\nThat seems huge at one point, right?\\n\\nBrian Chau\\nExactly, exactly. Or we could go even lower. Problem of this, for people who don't really know about it, is that this is a moving target. This changes as time. Time changes. And a law that might, at least on its face, be supposed to only regulate state of the art now suddenly interferes with everyone's day to day.\\n\\nDean Ball\\nRight? I think Bill Gates at one point in the nineties or the early two thousands said something along the lines of, no one will ever need a computer with more than ten megabytes of. Of ram or something like that. I forget exactly what exactly, but said something along those lines. And of course what happens is that as computing power increases, we find new ways to use them.\\n\\nBrian Chau\\nYeah, there's market exploration. People figure out more things, they figure out how to use these things. This sounds like a good story, right? So what are people worried about? Why are people worrying about the Internet, too?\\n\\nDean Ball\\nWell, I think. I think there's a few different ways you can, you know, there's a few different ways you can come at this. I guess in broad strokes, there's two categories of people. To put it in a stylized way. There are the people that worry that AI models have an inherent risk to them. They may, you know, attain sentience, consciousness, or otherwise compete with humans for resources. And ultimately that competition is one that we will lose. And so we have to either centralize highly the development of very capable AI systems or simply not do it at all. I think someone like Eliezer Yukowski would probably say we shouldn't build these systems at all. And then, you know, there's people that are more worried about human misuse of the technology.\\n\\nDean Ball\\nAnd this ranges for everything from, you know, spreading misinformation, which is a super complex, hard to define word that has been quite politically coded all the way up to designing novel bioweapons using AI. So those are the risks that people are worried about.\\n\\nBrian Chau\\nAnd is there any credibility to those claims?\\n\\nDean Ball\\nSo the first category, you know, the sort of inherent risk idea, it's a debate. It started out with as thought experiments on websites like less wrong and place.\\n\\nBrian Chau\\nI just hate that term. Like, here is the thing with like a quote unquote thought experiment. It's not an experiment. You're not getting any data. It's not an observation of the real world. Yeah, it's literally, you know, a made up scenario.\\n\\nDean Ball\\nBut exercise might be a better way.\\n\\nBrian Chau\\nSure.\\n\\nDean Ball\\nAnd, you know, different people are persuaded to different degrees by that kind of argument, my general view is that banning the development of a very promising general purpose technology is quite a radical step. And if we're to take that step, we should do that with as much empirical evidence as we possibly can muster to support it. Now, the AI doom people would say, by the time there's empirical evidence, it'll be too late and we'll all be dead. I don't buy that. Fundamentally, I don't buy that. The other thing about the sort of inherent risk, the inherent risk viewpoint, is that a lot of these arguments were developed prior to the deep learning revolution, which started around twelve years ago.\\n\\nDean Ball\\nAnd so they were developed when the technology was much more of an abstraction, and it was kind of imagined that these advanced AGI systems would operate in kind of a realm of pure reason, could just simply out reason humans in profound ways. That's not actually the most promising path right now to advanced AI systems. Doesn't really look like that. Maybe it'll increasingly come to resemble that over time. But right now, llms, if they have reasoning abilities at all, they're coming from being trained on an enormous corpus of human writing, human.\\n\\nBrian Chau\\nOutput, especially with some of these post training techniques that perhaps we'll talk about later, such as RLHF or DPO, reinforcement learning by human feedback, or direct preference optimization, that there's a danger of sort of the opposite, right? That it becomes too attuned to the human consensus, that it's too unwilling to challenge the status quo particularly, or like we would be concerned about this when the status quo is wrong, which it often is throughout history, not even just in the most obvious political ways, but also in just about factual matters. People can look back in history and see time and time again that really non controversial in our eyes, right? In our times, non controversial. Plain scientific facts have been obscured by the crowd. And the continuation of this pattern is something that worries me a lot more than these sort of hypothetical concerns.\\n\\nDean Ball\\nAbsolutely no. I think if the aim of developing AGI, whatever, some people debate that term, but very capable AI systems, if the aim of doing that, or one of the important aims is to investigate the universe and discover the nature of reality more richly, to your point, we know from the past that scientific advancements have not always necessarily corresponded with human preferences. I don't think quantum mechanics corresponds with pretty much anyone's preference, and yet it is how it works. So that will become an increasingly important issue as we, you know, if the goal is to build super intelligence, one would hope intuitively that it disagrees quite a bit with the conventional wisdom that humanity holds.\\n\\nBrian Chau\\nThere's sort of a kind of tragedy here, right, where we want something much more capable of science than us, but we don't necessarily want it to tell us that we've been wrong about science all of these years. That that would actually be quite a gut punch. But that's exactly what we're trying to build, and that'll hopefully lead to far better drugs, far better transportation, maybe space transportation, so on. Anyways, let's move on to some of the more concrete worries that people are trying to legislate against. So that may be, if I remember correctly, you brought up biotechnologies. People might use this for bio warfare, people might use this for other kinds of AI weapons for misinformation. Is there anything else I missed?\\n\\nDean Ball\\nThose are the big ones that I think I mentioned. There's obviously many more one could imagine. I mean, basically, all kinds of malfeasance are possible with the general purpose technology. Think about the ways one can misuse electricity or computers. Quite a few.\", \"Brian Chau\\nRight. Right. This is something worrying that I saw is like the classification of, quote unquote, dual use technologies. And to me, if this is set as a precedent, then, you know, if the idea that large language models such as chat GBT are some kind of, you know, military adjacent technology in the way that this report makes it seem, then that could be applied to just about everything you own. It could be applied to computers, for sure. You know, definitely. People use computers in warfare. It can be used to really, any kind of electronics. As you said, it can be true of plastics. People use plastics in the military, certainly. Right.\\n\\nBrian Chau\\nThe way in which dual use is or is kind of stretched to wrap around AI, you know, it basically already wraps around, if not can be stretched to wrap around like pretty much everything else in your life. And that's something that I find very worrying.\\n\\nDean Ball\\nI completely agree. Something I often say is a department of AI is like a department of metallurgy. It's too low in the tech stack, really, to be useful for regulation. Instead, what you should be doing is thinking about downstream uses that we do or don't want to regulate. And there are a number of concerns. One of the things that I find so interesting about this whole AI conversation is that AGI is seen as such a world changing, transformative technology that it invites people to impose their own idealized worldview on what happens afterwards. Because it's kind of a. There's kind of AGI and then there's haze, there's fog about what happens after that. And so everyone wants to impose, you know, their own worldview. So there's libertarians who want it, who think it'll become a libertarian paradise.\\n\\nDean Ball\\nAnd there's, you know, people that want state control of things, that think that's what we'll need. But the reality is that it'll probably be, you know, probably dissatisfy and satisfy everybody in different ways.\\n\\nBrian Chau\\nIt's interesting, right? There are two types of people who kind of fantasize about the outcome. There's the people who think it will be utopia, and there will be the people who think it's dystopia. Right. And this is maybe much more downstream of their disposition than downstream of their factual understanding.\\n\\nDean Ball\\nYes. And I think it is. It is so important in these conversations and will become increasingly important, I think, to really understand the technology at a fairly deep level. You know, not every single person needs to, you know, have an opinion on DPO versus PPO or, you know, whatever. Like, we don't. It's not like every policymaker needs to be that deep in the weeds. But understanding how these things work, and not just at a technical level, but also the economics, is important, I think, to crafting good regulation. It's not helpful for policymakers to enter the conversation and just say, we're going to pass a law that says this has to go well, according to us. That doesn't help anyone, that doesn't advance our knowledge. It doesn't give us new tools to actually make it go well. It just creates more confusion in my mind.\\n\\nBrian Chau\\nYeah, that makes sense. So you talked about, or you mentioned AI weapons. What are the real threats of AI weapons? Does China have better AI weapons than us?\\n\\nDean Ball\\nIt does seem as though China is more advanced on certain aspects of AI that can definitely relate to weapons surveillance. First of all, I think edge compute for cameras to do facial recognition, gait analysis, things like that. That seems to be something where the Chinese are much further along, not just in the technology, but in the distribution.\\n\\nBrian Chau\\nAnd for the audience, I think this is an important clarification. A lot of these machine learning technologies are not just language models. They're not just chat GPT, they're not just taking in text and trying to basically process that text, but instead, they're either these statistical inference tools. Given someone's data, how likely are they to have certain other traits? How certain are they to, say, be violating these laws within China, for example? Right? Or you mentioned gait analysis. Just given how someone is walking, this is most seen at airports. Given how someone is walking, what are the likelihood they're carrying some kind of explosive? Those are the kind of utilities that we're talking about, and these are other things that you can do with the same types of patterns of statistical analysis that are used in machine learning.\\n\\nDean Ball\\nSo there are some areas where it would seem that China is ahead, or at least matches us. It looks like on language modeling, the US has a pretty decisive lead, at least at this moment. We'll see. When it comes to warfare, a lot of that also is just about. It's really an institutional question between the PLA, the chinese army, and the Department of Defense in the US, which of those institutions will be more receptive to new technology? And there are really interesting questions about exactly how that works and how institutions adopt new technology.\\n\\nBrian Chau\\nDo we know, is it completely fog of war? So we know that on a research level, China is better at these non language statistical techniques. Like what about missiles? Are our AI missiles, automated missiles, better than China's automated missiles?\\n\\nDean Ball\\nI think a lot of it is kind of unknown. And unfortunately, the only real way to know would be to put it to a live test, which would be an armed conflict, which, of course, none of us want to do. But I suspect that it is uneven and that there areas where the US has a substantial lead and there areas where the. Where the Chinese do, certainly the chinese areas. So when you. This isn't an army thing, necessarily, but you can look on social media for the drone shows that occur over chinese cities, which are these very intricate sort of dances of drones that take place where they'll form dragons and the dragons will fly around, things like that. And, you know, that's a demonstration of some pretty powerful technology.\\n\\nDean Ball\\nThis isn't AI in particular, but China is known to have more advanced missiles than we do, specifically hypersonic missiles. Unknown how useful hypersonic missiles actually will be in conflict, but I think it's very much undetermined right now. And one thing that is heartening to see as an American is that there is a budding group of new defense minded startups, companies like Anduril, that are building kind of at a very rapid pace, AI enabled, software defined weapons that I think will be quite useful. But my guess would be, without knowing the specifics, my guess would be that AI and some of the hardware capabilities that are not really directly related to AI, but will be helped by AI complement AI, will profoundly change the nature of warfare. So I think it's very much an open question, and the real determining factor there will be.\\n\\nDean Ball\\nHow open is the Department of Defense and the US government more broadly to incorporating AI? And one of the worries that I have, for example, with the Biden executive order, is that they're essentially imposing regulations on government in its use of AI. So if government wants to use a language model or some other form of machine learning, it has to perform an equity analysis and multi stakeholder committee and things like that can slow stuff down. And one of the things I think people don't appreciate, including a lot of classical liberal, libertarian minded people, is that it's not actually just our economy in the US that's over regulated. Government itself is also over regulated. And that's why it's very hard for government to get things done, because its own activity is quite stringently regulated.\\n\\nBrian Chau\\nRight. There's this conception of the bureaucrat as kind of wanting, enjoying the paperwork, right. Wanting to impose more paperwork. But actually, in many cases, that's not true. Maybe there's some fraction of bureaucrats that are like that. But I know many people who work in civil service and want to do a good job, want to, even from a completely kind of unrelated to AI, which is more of an emerging technology, more of a unknown, wanting to get more energy permits approved, for example, get more development grants approved, get more science grants approved, and wants to, of course, make sure it's valid, make sure that the grantees are not committing fraud, anything like that.\\n\\nBrian Chau\\nBut at the end of the day, you have to put up with a lot of this busy work that they don't want to be doing, that they'd rather, you know, that they themselves would rather pick up the pace. But that's not something that's under their control right now.\\n\\nDean Ball\\nYeah, look at what's going on right now with the Chips and Science act, where Congress in 2021, I believe, appropriated $55 billion for the construction of semiconductor manufacturing facilities and other things relating to semiconductors all over the US. So this money was appropriated two and a half years ago and very little of it has actually been dispersed. And the reason for that relates to lots and lots of process regulation over the activities of government itself. These are the kinds of things that are, I view as like, quintessentially nonpartisan issues because as long as you believe the government should exist and has a role to do something, we want it to be able to do those things well and efficiently. And these are the kinds of regulations that I'm talking about that can really slow that down.\", \"Brian Chau\\nYou know, this is something that Ezra Klein agrees with. This is something that Tyler Cowan agrees with. You could think of the nineties to two thousands. Maybe it already started, but this is when it kind of burgeoned, is the kind of process revolution. Right. Or maybe even earlier. Maybe Nixon was the real EPA was Nixon and NRC. There's roots of it earlier as well. But maybe in terms of sentiments, peak proceduralism was like two thousands, even if a lot of the policies were already in place. And then, you know, the kind of elite ideology on both sides of the aisle turned against it, actually, process is less important. You should be focusing on the ten or 20% of the process that actually, you know, gets rid of fraud and, you know, checks for those safety concerns.\\n\\nBrian Chau\\nAnd then 80% of this is just, you know, kind of political bargaining stuff and is actually unnecessary. It's, it's, it is sort of a mainstream bipartisan narrative nowadays.\\n\\nDean Ball\\nYeah, no, it is. I think it is. And I think the important lesson to take away from that is that at least as far as kind of elite discourse goes on, this change of opinion about proceduralism is essentially bipartisan, as you say, and yet we can't do much about it. It seemingly. There's not that there's been some efforts to reform some of this, whether it's the Nuclear Regulatory Commission, NEPA, the National Environmental Protection act, the different pieces of this. But it's been very hard because there are interest groups. Regulatory capture takes place, and there's also a whole complex of nonprofits and academic institutions and other groups that crop up to continue supporting the status quo. The right wants it gone so you can build pipelines, and the left wants it gone so you can build wind turbines, but neither is possible.\\n\\nDean Ball\\nAnd there's not a lot you can do about it. It would seem so. That is fixing Congress, and the dysfunction in Congress is a totally separate issue. It's an important one, but it's also just a reality that we have to contend with as we contemplate legislation related to AI, because history would suggest that we're going to get one shot at this. And if we do, NEPA for AI, that's going to be with us forever. And it may fundamentally impede progress in pretty much one of the only areas of our economy that has delivered sustained cost reduction and technical progress over the last 30, 40 years. Really, one of the only ones is the digital economy, computers.\\n\\nBrian Chau\\nAnd the thing that I worry about is that the downstream consequences of regulating AI are, as you said, regulating every part of the industry. If you have these broad permissions on AI, whether that's because the AI that you're using in your factory machinery or in your spreadsheets or in your, you know, diagnosis clipboard is actually, you know, dual use military technology, that this becomes an excuse to regulate and to fish for outcomes in almost exactly every micro area of life. Right. That once that becomes a pattern, and you see this with NEPA, right, the fact that the process can be used to obstruct things means that it will be used to obstruct things. And it's not really ideological. The things that it's obstructing are like solar panels and like wind turbines.\\n\\nBrian Chau\\nIt's like, you know, left on left lawfare, but it's, you know, it's worse than that, right? It's not really ideological. It's not really leading to any one side winning. It is just the incentive that it creates, basically, like, all on its own, that if you create this incentive to kind of stall in the process, that, you know, any vocal objector will basically be granted some kind of additional advantage, which is functionally what NEPA does, then this expands into the area of regulation, right. It sort of is a slow process of optimization where the thing being optimized is like the maximum amount of obstruction in between steps.\\n\\nDean Ball\\nYeah, yeah. And the problem is that once you create that infrastructure to regulate effectively the use of computers, you might say at the very beginning, it's the frontier models only, and it's OpenAI and DeepMind and anthropic and companies like that. But once you've created that legal and institutional infrastructure, it becomes very tempting to start applying it to other things. And we've seen that. We've seen that consistently, that the administrative state in this country takes the broadest possible view, interpretation of its own statutory powers. So there are examples of laws that were passed during our lifetimes, you know, in the last 20 years, that where we know exactly what the intention of the creators of the law, what their intention was. And now we have administrative agencies that are trying to interpret that same law way beyond what the original intention was. We'll see.\\n\\nDean Ball\\nIt looks like the Supreme Court might be very soon about to put some impediments on that, but that's just been the general trend. And I think as a general rule, if you give an administrative agency power, it's going to interpret its powers within the fullest extent of the law. And so you have to be cognizant of that when you're creating regulation. And so I just, I tend to think that regulating conduct and physical devices is something that, you know, the government does or, you know, drugs or things like that. But software is really a different beast. It's really an instantiation of knowledge. And in that sense, I view software as being a form of speech, a form of expression, and so regulation of it is just very, not only is it constitutionally problematic, it's just very difficult to enforce and just very complicated.\\n\\nDean Ball\\nSo it's not a road that I think we should be going down if we can avoid it at all.\\n\\nBrian Chau\\nRight. I want to get a bit into that because I think, yeah, there's one side of this where the lawyer, you know, finishes his argument, sits down, waits for the verdict of the judge, and I think in this case that verdict would be in your favor, that, you know, the precedent is that software is protected by the First Amendment, but there's all sorts of ways to go through that and to try to affect AI indirectly, whether that's through restrictions of AI hardware or through licensing agencies on the corporate entities. There are ways that circumvent the legal arguments in ways that, at least to me, make the kind of spirit of the law just as important as the letter of the law here. So. Right. I want to address that. Why is this distinction important?\\n\\nBrian Chau\\nWhy is software different than either hardware or these other physical technologies? What actually, what actually makes its, what impact does that have? What impact is that distinction on people's lives?\\n\\nDean Ball\\nI think the important distinction is that software is it's knowledge, in a certain sense, it's knowledge about how to do a specific thing with a computer and its instructions. You can think of it like a recipe, or you can think of it like a book, but I think that's just quite different from a physical device that you're selling on the market. For one thing, software is non rivalrous, like all knowledge. And what that means is that if I know a fact and you know a fact, there's not a limited supply of facts of that fact, of that specific fact. Right? We can, everyone in the world can know it, and we can, you know, there can be new people born who also learn it, and we're not going to run out of it. Same with software.\\n\\nDean Ball\\nThat, that gets to the feasibility point before you get into principles about the constitution and, you know, the enlightenment and things like that. I think just from a feasibility perspective, that makes it very hard to control. We also happen to have a global instant communications network for transmitting knowledge, the Internet, which makes it even harder. It's just not entirely clear to me. For example, if you want to get into open source software, open source AI, which is something a lot of people worry about quite a bit. It's not clear to me how you would stop something like that from existing if you wanted to, you could harm it considerably. You could deal a very severe blow to it through law, because most people who build open source software are lawful actors and they will try to comply with the law.\\n\\nDean Ball\\nBut the people who are trying to use AI to build a bioweapon probably don't care about NIST's AI risk management framework. They probably are not that interested in whether or not they comply with that. So by pathologizing the development of AI, particularly open source AI, you're kind of just guaranteeing that it's just. It's just criminals. You're kind of.\\n\\nBrian Chau\\nMy takeaway from that is that all these factors about software, the non rival risk factor, the fact that you can share it over the Internet, is a kind of force multiplier, is a kind of scaling factor, where if you think it's bad, then you think it's ten times as bad or 100 times as bad, and if think it's good, then it's 100 times as good or a thousand times as good. It's not very clear to me that if you're already assuming it's bad and you're taking a regulatory position, that this will lead you to not think that it's bad. I don't think it's actually persuasive to anyone. It just makes people double down.\", \"Dean Ball\\nIf you were looking to the future at the dawn of the personal computing era in the 1980s, the idea that open source software would undergird most of. Not. Maybe not most, but a huge amount of the digital economy would be totally alien to you. It's just not the way you would. The way that open source has worked out is not the way anybody would have predicted. And the benefits just have manifestly outweighed the risks, at least so far. If you really understand why it is that open source software has been so useful, whether for security purposes or for economic purposes, or for technological innovation purposes, all of those things have been true. The ability to use software to do great harm to the world already exists. A cyber attack could bring down a piece of critical infrastructure in the United States any day now.\\n\\nDean Ball\\nAnd if that does happen, my bet would be that there will be open source software that plays a role in that, in addition to closed source software. Probably too. But the point is that the benefits have, even though cyber attacks do happen, the benefits have outweighed the risks and so I would just have faith in that to a certain extent that will continue to be the case, particularly if we allow our law enforcement agencies and, you know, military and intelligence agencies to adapt AI rapidly and creatively so that they can counter many of the threats that are going to emerge either way, regardless of whether or not there's open source software.\\n\\nBrian Chau\\nYeah, yeah, that might be a good way to think about it, that, you know, the marginal chance that, you know, China is already out there, you know, that the adversaries are already out there. It's not like they're not going to have some sort of baseline level of competence, but by enabling sort of random citizens or startups, independent developers, this is a lot of Silicon Valley, you know, a lot of where the innovation actually happens. I was speaking with Steve Hsu just the other day on this, and this is something that I think has historically been America's strength.\\n\\nBrian Chau\\nSo looking at it as a way to get the kind of next generation of american talent in a way that I do think is asymmetrical, that is not the same for the next generation of chinese talent who might be more reliant on, you know, specific, structured government funding that, you know, maybe can't use the same open source models, at least when it comes to language models, because they would be, you know, offensive to the chinese government. They would make Winnie the Pooh jokes and so on. I think that's, that point is true, especially when it comes to cybersecurity, where, you know, you can increase the number of attackers, you can increase the number of defenses, defenders, and the probabilities, you know, work out in favor of the defenders. Right.\\n\\nDean Ball\\nYeah. What's the one major western tech platform that has to exist in China for economic reasons? It's GitHub.\\n\\nBrian Chau\\nRight. Right.\\n\\nDean Ball\\nWhich is where all the open source software libraries are hosted, a company owned by Microsoft, for those who don't know and people, in fact, you know, there have been examples of people putting subversive messages for the chinese population into the code of open source projects on GitHub because it's the one thing that.\\n\\nBrian Chau\\nYeah, yeah. The taiwanese flag emoji. Right. This is the meme. Yeah. Like, they draw it out. They draw it out with the ASCIi art.\\n\\nDean Ball\\nYeah. And it's something that I think, you know, there was a political article I was reading this morning called can anyone control AI? And it was all about different countries efforts to regulate AI. There's this intuitive assumption that people make, which is that the country that sets the best rules for AI will be, the country will have a lot of power accrue to it because everyone will respect their rules. That is not generally how things have worked in the history of technology. The rules for usage and the standards for usage tend to be set by the group of people and the country that invented it. That's true of the Internet. The Internet is a force. It can be a force for centralized control, but in large part, it is a force for open communication.\\n\\nDean Ball\\nAnd I think it's very unlikely that a totalitarian country would have even had the idea to invent such a thing. And you think about just many technologies that were fundamentally our conception of them was shaped by the people who invented them. And so I think invention is deeply important in that regard. And I think that open source, because it's communal, there's a global consensus standard making, but it's certainly led by american firms and american developers. It's a tool of soft power that the US would really give up at great cost, I believe.\\n\\nBrian Chau\\nRight, right. There's, you know, there's a hypothetical world where we go back to the sixties and we say, you know, all of these great american films. Do we really want, you know, Soviets getting access to these films? Do we really want the Chinese to be able to see these films? What if they copy our techniques? And of course, this would really strip the kind of american vision, you know, blue jeans, grocery stores, so on, which were from, you know, from my historical understanding at least, which were a kind of historical motivator for people to kind of prefer the american style of life for that to win out. So this benefit of openness. And, yeah, you can argue that already China is trying to stop it, right? They're trying to prevent the western narratives from going in. There's no reason that we should be.\\n\\nBrian Chau\\nWe should kind of be the ones to help them out with that.\\n\\nDean Ball\\nNo, I don't. I think that we need, as a general rule, not just when it comes to software, you know, law, but as a general rule, I think America needs to double down on what makes us different in world history. And to the extent America becomes more like China, I think we're going to lose at that game. I think we will never be able to better at being China than China. So I suspect we will lose that. So I think we need to double down on what makes America.\\n\\nBrian Chau\\nYeah, that's a great sentiment. Although before we go, I do want to ask you. So you write the hyper dimensional substack as well as contributing to national affairs. Everyone in the audience should subscribe you know, it'll be recommended in the notes down below. But why is it the hyper dimensional substack? Is it, are the letters going to be popping out of you? Is it, you know, are the articles just expanding into new dimensions as you read them?\\n\\nDean Ball\\nWell, so the initial idea was a reference to, you know, hyperdimensional computing, which for those that don't know AI models are very high dimensional in mathematical terms. They might be billions or hundreds of billions of dimension mathematical functions essentially. So it was a reference to that but also a reference to the idea of exploring topics, complex topics from many different angles, many different dimensions so to speak, and sort of seeing things in a way that is kind of more complicated than what we might intuitively imagine.\\n\\nBrian Chau\\nThanks for joining me. This was a great interview and yeah, I look forward to working with you in the future.\\n\\nDean Ball\\nYeah, thanks Brian. Great to be here.\"]}]\n"
     ]
    }
   ],
   "source": [
    "print(sources_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bafcb6e1-e0fb-41bd-8469-b463d105ee1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:23:15.750986Z",
     "start_time": "2024-04-11T08:23:15.742372Z"
    }
   },
   "outputs": [],
   "source": [
    "def first_cycle_of_extracting_arguments(dicts):\n",
    "    first_cycle_chain = first_cycle_prompt | llm\n",
    "    for dict in dicts:\n",
    "        dict[\"arguments\"] = []\n",
    "        for chunk in dict[\"chunks\"]:\n",
    "            first_cycle_response = first_cycle_chain.invoke({\"transcript\": chunk})\n",
    "            dict[\"arguments\"].append(first_cycle_response.content)\n",
    "        print(\"over\")\n",
    "        text = '\\n\\n'.join(dict[\"arguments\"])\n",
    "        filename = f\"{dict['path']}/steps/first_step.md\"\n",
    "        write_to_file(filename, text)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16983b7e-dc71-418e-b5e4-f340b1846e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:30:43.919323Z",
     "start_time": "2024-04-11T08:26:00.234801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/first_step.md\n"
     ]
    }
   ],
   "source": [
    "dicts_with_extracted_args = first_cycle_of_extracting_arguments(sources_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d0d68af-6a24-40fd-aa63-7ae2cdfb0470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:31:22.861422Z",
     "start_time": "2024-04-11T08:31:22.855702Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dict_to_json(dicts_with_extracted_args, \"./sources/json/first_cycle_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2acb9d74-4988-4f3e-9cb3-9a53956ec7b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:31:24.641759Z",
     "start_time": "2024-04-11T08:31:24.633681Z"
    }
   },
   "outputs": [],
   "source": [
    "def second_cycle_of_extracting_arguments(dicts):\n",
    "    second_cycle_chain = second_cycle_prompt | llm\n",
    "    for dict in dicts:\n",
    "        dict[\"improved_arguments\"] = []\n",
    "        for i, chunk in enumerate(dict[\"chunks\"]):\n",
    "            second_cycle_response = second_cycle_chain.invoke({\"all_arguments\": dict[\"arguments\"][i], \"transcript\": chunk })\n",
    "            dict[\"improved_arguments\"].append(second_cycle_response.content)\n",
    "        print(\"over\")\n",
    "        text = '\\n\\n'.join(dict[\"improved_arguments\"])\n",
    "        filename = f\"{dict['path']}/steps/second_step.md\"\n",
    "        write_to_file(filename, text)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b45be49-f2ce-4e39-95e4-bd7ed762809b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:36:46.573109Z",
     "start_time": "2024-04-11T08:31:27.330814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/second_step.md\n"
     ]
    }
   ],
   "source": [
    "dicts_with_extracted_and_improved_args = second_cycle_of_extracting_arguments(dicts_with_extracted_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41ae0410-b3c8-4c53-8451-8dbe915c6bde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:04.364822Z",
     "start_time": "2024-04-11T08:37:04.358169Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_improved_arguments_in_dict(dicts):\n",
    "    for dict in dicts:\n",
    "        dict[\"isolated_arguments\"] = []\n",
    "        for improved_arg in dict[\"improved_arguments\"]:\n",
    "            if improved_arg:\n",
    "                splitted_args = improved_arg.split(\"```yaml\")\n",
    "                splitted_args_cleaned = []\n",
    "                for arg in splitted_args:\n",
    "                    arg_clean = arg.split(\"```\")[0]\n",
    "                    if (arg_clean != \"\"):\n",
    "                        splitted_args_cleaned.append(arg_clean.strip())   \n",
    "                if splitted_args_cleaned != None:\n",
    "                    dict[\"isolated_arguments\"].append(splitted_args_cleaned)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6176109-6d8a-4e58-861b-e8707aeb0b0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:38:15.049848Z",
     "start_time": "2024-04-11T08:38:15.045485Z"
    }
   },
   "outputs": [],
   "source": [
    "dicts_with_isolated_improved_arguments = split_improved_arguments_in_dict(dicts_with_extracted_and_improved_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a1eec61-b2d6-43c5-bde6-26e2bcbd1725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:15:09.442636Z",
     "start_time": "2024-04-11T12:15:09.373322Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dict_to_json(dicts_with_isolated_improved_arguments, \"./sources/json/second_cycle_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bbb2a99-f23f-4c28-9812-386bdf73ded8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T13:13:02.090399Z",
     "start_time": "2024-04-11T13:13:02.082830Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_embeddings_with_smaller_chunks(directory):\n",
    "    folder_names = []\n",
    "    all_chunks = []\n",
    "    dbs = []\n",
    "    for entry in os.listdir(directory):\n",
    "        if entry != \"json\":\n",
    "            folder_names.append(entry)\n",
    "    for folder_name in folder_names:\n",
    "        loader = DirectoryLoader(f'./sources/{folder_name}/source', glob=\"**/*.md\")\n",
    "        text_splitter = TokenTextSplitter(chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len)\n",
    "        raw_text = loader.load()\n",
    "        entire_text_from_a_source = \"\"\n",
    "        for doc in raw_text:\n",
    "            entire_text_from_a_source += doc.page_content\n",
    "        chunks = text_splitter.create_documents([entire_text_from_a_source])\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata = {\"folder\": folder_name}\n",
    "            all_chunks.append(chunk)\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "        db = Chroma.from_documents(all_chunks, embeddings)\n",
    "        dbs.append(db)\n",
    "    return dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71a58393-7906-47ea-8070-6082914bf81c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T13:13:09.966428Z",
     "start_time": "2024-04-11T13:13:03.882656Z"
    }
   },
   "outputs": [],
   "source": [
    "dbs = create_embeddings_with_smaller_chunks(\"./sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2a48a0d-bf72-40fb-85f9-5033db492991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:54:08.800908Z",
     "start_time": "2024-04-11T14:54:08.790963Z"
    }
   },
   "outputs": [],
   "source": [
    "def third_cycle_of_extracting_arguments(dicts):\n",
    "    third_cycle_chain = third_cycle_prompt | llm\n",
    "    for i, dict in enumerate(dicts):\n",
    "        dict[\"explanations\"] = []\n",
    "        if i < len(dbs):  \n",
    "         db = dbs[i]\n",
    "        else:\n",
    "         db = None\n",
    "        for i, chunk in enumerate(dict[\"chunks\"]):\n",
    "            explanations = []\n",
    "            for arg in dict[\"isolated_arguments\"][i]:\n",
    "                docs = db.similarity_search(arg, k=3)\n",
    "                context = \"\"\n",
    "                for doc in docs:\n",
    "                    context += \"\\n\" + doc.page_content\n",
    "                explanation = third_cycle_chain.invoke({\"argument\": arg, \"transcript\": context})\n",
    "                explanations.append(explanation.content.split(\"```yaml\\n\")[1].split(\"```\")[0].strip())\n",
    "            dict[\"explanations\"].append(explanations)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5de267f-8b42-48fa-8ad5-a5a5341d60fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:27:54.098431Z",
     "start_time": "2024-04-11T14:54:12.984960Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dicts_with_arguments_and_explanations = third_cycle_of_extracting_arguments(dicts_with_isolated_improved_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c825fd1-8012-402d-83db-ef597ad6d8d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:38.841567Z",
     "start_time": "2024-04-11T15:30:38.832998Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dict_to_json(dicts_with_arguments_and_explanations, \"./sources/json/third_cycle_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ab9f15f-524b-4fdf-88d4-d7aa7011997f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:41.208291Z",
     "start_time": "2024-04-11T15:30:41.199556Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_third_step(dicts):\n",
    "    for dict in dicts:\n",
    "        dict[\"final_arguments\"] = []\n",
    "        for i, isolated_argument_group in enumerate(dict[\"isolated_arguments\"]):\n",
    "            for j, single_isolated_arg in enumerate(isolated_argument_group):\n",
    "                final_arg = \"\\n\".join([single_isolated_arg, dict[\"explanations\"][i][j]])\n",
    "                dict[\"final_arguments\"].append(final_arg)\n",
    "                final_arg_yaml_format = \" ```yaml\\n\" + final_arg + \"\\n```\\n\\n\"\n",
    "                filename = f\"{dict['path']}/steps/third_step.md\"\n",
    "                write_to_file(filename, final_arg_yaml_format)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0154e3c-3746-40e2-ad9f-ae5d6dd4ae2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:44.191326Z",
     "start_time": "2024-04-11T15:30:44.177897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n",
      "Text successfully written to ./sources/brian_chau_06_01/steps/third_step.md\n"
     ]
    }
   ],
   "source": [
    "final_dicts = save_third_step(dicts_with_arguments_and_explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36b27afc-ee82-4e3e-9cce-0f3e4ce2e5e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:47.458411Z",
     "start_time": "2024-04-11T15:30:47.445456Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_directory_structure_for_chatbots(directory_name):\n",
    "    script_directory = os.getcwd()\n",
    "    new_directory_path = os.path.join(script_directory, directory_name)\n",
    "    if not os.path.exists(new_directory_path):\n",
    "        os.makedirs(new_directory_path)\n",
    "        yaml_file_path = os.path.join(new_directory_path, \"metadata.yaml\")\n",
    "        with open(yaml_file_path, \"w\") as yaml_file:\n",
    "            with open(yaml_file_path, \"w\") as yaml_file:\n",
    "                yaml.dump({}, yaml_file)\n",
    "        prompts_folder_path = os.path.join(new_directory_path, \"prompts\")\n",
    "        knowledge_base_folder_path = os.path.join(new_directory_path, \"knowledge_base\")\n",
    "        for folder_path in [prompts_folder_path, knowledge_base_folder_path]:\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "                system_prompt_file_path = os.path.join(prompts_folder_path, \"system_prompt.md\")\n",
    "                with open(system_prompt_file_path, \"w\") as system_prompt_file:\n",
    "                    system_prompt_file.write(\"System Prompt\")\n",
    "    if os.path.exists(new_directory_path):\n",
    "        yaml_file_path = os.path.join(new_directory_path, \"metadata.yaml\")\n",
    "        with open(yaml_file_path, \"w\") as yaml_file:\n",
    "            yaml.dump({\"name\": None, #add your chatbot name here don't use quotes when naming chatbot \n",
    "                       \"tags\": None, #tag your chatbot can be #optimistic or #pessimistic\n",
    "                       \"based_on\": None #provide the uri to source of the raw_text used for arguments extraction, add it in new line with dash in front uri (- https://uri_to_source.com) \n",
    "                      }, yaml_file, sort_keys=False)\n",
    "        prompts_folder_path = os.path.join(new_directory_path, \"prompts\")\n",
    "        knowledge_base_folder_path = os.path.join(new_directory_path, \"knowledge_base\")\n",
    "        for folder_path in [prompts_folder_path, knowledge_base_folder_path]:\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7d0f150-8dd7-4f22-b4e3-5a90fe87ef9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:50.396061Z",
     "start_time": "2024-04-11T15:30:50.390986Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_chatbots(dicts):\n",
    "    for dict in dicts:\n",
    "        create_directory_structure_for_chatbots(dict[\"name\"])\n",
    "        prompt_filename = f'./{dict[\"name\"]}/prompts/system_prompt.md'\n",
    "        with open(prompt_filename, 'w') as file:\n",
    "            file.write(\"Use arguments provided to answer the question.\\n\\nArguments:\\n\\n{arguments}\")\n",
    "        for i, final_arg in enumerate(dict[\"final_arguments\"]):\n",
    "            filename = f'./{dict[\"name\"]}/knowledge_base/{dict[\"name\"]}-{str(i + 1)}.md'\n",
    "            with open(filename, 'w') as file:\n",
    "                file.write(final_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09e910d2-05b9-4ba3-9dd6-b7864137f4fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:53.428704Z",
     "start_time": "2024-04-11T15:30:53.412736Z"
    }
   },
   "outputs": [],
   "source": [
    "create_chatbots(final_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "29732691-f432-41bc-91ca-255740d541d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:55.623582Z",
     "start_time": "2024-04-11T15:30:55.612017Z"
    }
   },
   "outputs": [],
   "source": [
    "dicts_with_isolated_improved_arguments = load_results_from_json(2)\n",
    "dicts_with_arguments_and_explanations = load_results_from_json(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11187a69-7f67-4195-8806-ccd4251f3589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
