{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f418b28d-060a-4e1c-b804-d76264ca0e0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:15:31.499674Z",
     "start_time": "2024-04-11T08:15:23.830030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.1.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.8)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (1.4.47)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (0.5.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (0.0.21)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.24 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (0.1.25)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (0.1.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.1.8) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.8) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.8) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.8) (0.8.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.8) (2.3)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.24->langchain==0.1.8) (3.6.2)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.24->langchain==0.1.8) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.8) (4.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain==0.1.8) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain==0.1.8) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain==0.1.8) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.8) (2.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.24->langchain==0.1.8) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.8) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: dropbox 11.36.0 has a non-standard dependency specifier stone>=2.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of dropbox or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai==0.0.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.6)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-openai==0.0.6) (0.1.25)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-openai==0.0.6) (1.23.5)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-openai==0.0.6) (1.12.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-openai==0.0.6) (0.6.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (6.0)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (3.6.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (0.1.5)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.0.6) (2023.3.23)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.0.12)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (1.26.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: dropbox 11.36.0 has a non-standard dependency specifier stone>=2.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of dropbox or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv==1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: dropbox 11.36.0 has a non-standard dependency specifier stone>=2.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of dropbox or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb==0.4.18 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.18)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.10.7)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (0.111.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (0.29.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (4.9.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (3.3.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.15.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (0.42b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.21.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (0.13.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (4.65.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (6.1.1)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.59.3)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (4.0.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (28.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (6.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (4.0.1)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.4.18) (1.23.5)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.18) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.18) (0.0.3)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.18) (0.27.0)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.18) (3.1.2)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.18) (0.0.9)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.18) (5.10.0)\n",
      "Requirement already satisfied: orjson>=3.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.18) (3.10.3)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.18) (2.0.0.post2)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (2022.12.7)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (2.23.4)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (1.5.1)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (3.2.2)\n",
      "Requirement already satisfied: urllib3<2.0,>=1.24.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18) (1.26.15)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (23.5.26)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (23.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (4.23.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.18) (1.2.13)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.18) (6.6.0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.18) (2.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.18) (1.61.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.21.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.18) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.21.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.18) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.42b0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (0.42b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.42b0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (0.42b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.42b0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (0.42b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.42b0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (0.42b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (67.8.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.18) (3.7.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.4.18) (1.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.4.18) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.4.18) (2.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.65.0->chromadb==0.4.18) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer>=0.9.0->chromadb==0.4.18) (8.1.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer>=0.9.0->chromadb==0.4.18) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer>=0.9.0->chromadb==0.4.18) (13.0.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.18) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (0.5.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (1.0.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (0.19.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.18) (12.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb==0.4.18) (2.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.18) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.18) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.18) (4.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb==0.4.18) (3.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb==0.4.18) (1.0.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb==0.4.18) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.18) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb==0.4.18) (2.1.2)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.18) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.18) (2.15.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.18) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.18) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.18) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.18) (0.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: dropbox 11.36.0 has a non-standard dependency specifier stone>=2.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of dropbox or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.1.8\n",
    "!pip install langchain-openai==0.0.6\n",
    "!pip install python-dotenv==1.0.0\n",
    "!pip install chromadb==0.4.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98356761-e795-4338-9aef-8d05b212388f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:15:50.956757Z",
     "start_time": "2024-04-11T08:15:50.952541Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "106d34e5-61c0-4667-81ea-18bd8437d645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:06.618973Z",
     "start_time": "2024-04-11T14:17:06.598094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7dbba34-6940-430b-8057-419f77abf163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:09.015199Z",
     "start_time": "2024-04-11T14:17:08.944252Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-turbo\",\n",
    "                 temperature=0.7,\n",
    "                 model_kwargs={\n",
    "                    \"frequency_penalty\": 0.0,\n",
    "                     \"presence_penalty\": 0.0,\n",
    "                     \"top_p\": 1.0,\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729fece9-860b-4fd9-9e40-10f2962026bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:11.592018Z",
     "start_time": "2024-04-11T14:17:11.577003Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_to_file(filename, text):\n",
    "    try:\n",
    "        directory = os.path.dirname(filename)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        with open(filename, 'a') as file:\n",
    "            file.write(text)\n",
    "        print(\"Text successfully written to\", filename)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd30f70b-2af9-4a99-8018-f7ebbe34108f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:14.291166Z",
     "start_time": "2024-04-11T14:17:14.287058Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_dict_to_json(data, filename):\n",
    "    directory = os.path.dirname(filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78957f9-1683-4c12-ac90-136b14419ee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:16.344374Z",
     "start_time": "2024-04-11T14:17:16.340455Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_results_from_json(int):\n",
    "    if int == 1:\n",
    "        with open(\"./sources/json/first_cycle_results.json\", 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    if int == 2:\n",
    "        with open(\"./sources/json/second_cycle_results.json\", 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    if int == 3:\n",
    "        with open(\"./sources/json/third_cycle_results.json\", 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8360efe1-f307-466d-aa59-312cf30f0cdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:22.226198Z",
     "start_time": "2024-04-11T14:17:22.220334Z"
    }
   },
   "outputs": [],
   "source": [
    "first_cycle_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"\"\"\n",
    "    Read the following transcript and extract all the arguments made about AI safety. Make sure they are self-contained.\n",
    "\n",
    "  You must stick as close as possible to the transcript - use the author's own words and tone of voice.\n",
    "\n",
    "  You must write each argument in a valid YAML format, surrounded with backticks.\n",
    "  You must separate each argument with a new line.\n",
    "\n",
    "\n",
    "  The simplest possible argument must at least contain three claims:\n",
    "\n",
    "  ```yaml\n",
    "  claim: \"Top-level claim\"\n",
    "  premises:\n",
    "    - claim: \"First independent premise supporting the top-level claim\"\n",
    "    - claim: \"Second independent premise supporting the top-level claim\"\n",
    "  ```\n",
    "\n",
    "  And here's an example of a more complex argument, which also includes examples to illustrate lower-level claims:\n",
    "\n",
    "  ```yaml\n",
    "  claim: \"Top-level claim\"\n",
    "  premises:\n",
    "    - claim: \"First independent premise supporting the top-level claim\"\n",
    "    - claim: \"Supporting premise for the top-level claim\"\n",
    "      example: \"Example supporting this premise\"\n",
    "    - claim: \"Another supporting premise for the top-level claim\"\n",
    "    - claim: \"Second independent premise supporting the top-level claim\"\n",
    "      premises:\n",
    "        - claim: \"Supporting premise for the second independent premise\"\n",
    "        - claim: \"Another supporting premise for the second independent premise\"\n",
    "        - claim: \"Independent premise supporting the second independent premise\"\n",
    "          example: \"Example supporting this independent premise\"\n",
    "  ```\n",
    "\n",
    "  Here's how to read this structure:\n",
    "\n",
    "  The top-level claim is the main argument.\n",
    "  Directly nested under the claim are independent premises. These provide justification independently of other premises.\n",
    "  An example can be nested directly under a claim to provide further context or support.\n",
    "  Just like the top-level claim, each premise can itself be supported by further individual premises, or examples, creating a nested structure.\n",
    "\n",
    "  # Here is the transcript:\n",
    "\n",
    "  {transcript}\n",
    "  \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ae46fd-4e4d-4780-9b11-f839d2a02e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:23.702811Z",
     "start_time": "2024-04-11T14:17:23.696224Z"
    }
   },
   "outputs": [],
   "source": [
    "second_cycle_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"\"\"\n",
    "  Based on the following transcript, make the arguments clear and distinct. \n",
    "  You may need to merge similar arguments to create a better, more logical argument.\n",
    "\n",
    "  Create the best, strongest possible version of the arguments, here's what to do:\n",
    "\n",
    "  - Make sure the arguments is self-contained\n",
    "  - Make the arguments understandable on their own, out-of-context\n",
    "  - Remember, arguments are not a description or an explanation\n",
    "  - Premise must always give a reason to believe the claim above\n",
    "  - Avoid using pronouns in premises\n",
    "  - A claim can have a maximum of two child claims (premises), rewrite if needed\n",
    "\n",
    "  # Argument format \n",
    "\n",
    "  You must write each argument in valid YAML format, surrounded with backticks.\n",
    "  Separate each argument with new line.\n",
    "  You must stick as closely as possible to the transcript. \n",
    "  Above all, you must express the argument in the words of the author, stick as close as possible to the tone of voice and phrases used in the transcript.\n",
    "\n",
    "\n",
    "\n",
    "  Here are some examples:\n",
    "\n",
    "  A simple argument might look like this: \n",
    "\n",
    "  ```yaml\n",
    "  claim: \"Top-level claim\"\n",
    "  premises:\n",
    "    - claim: \"First independent premise supporting the top-level claim\"\n",
    "    - claim: \"Second independent premise supporting the top-level claim\"\n",
    "  ```\n",
    "\n",
    "  And here's an example of a more complex argument, which also includes examples to illustrate lower-level claims:\n",
    "\n",
    "  ```yaml\n",
    "  claim: \"Top-level claim\"\n",
    "  premises:\n",
    "    - claim: \"First independent premise supporting the top-level claim\"\n",
    "    - claim: \"Supporting premise for the top-level claim\"\n",
    "      example: \"Example supporting this premise\"\n",
    "    - claim: \"Another supporting premise for the top-level claim\"\n",
    "    - claim: \"Second independent premise supporting the top-level claim\"\n",
    "      premises:\n",
    "        - claim: \"Supporting premise for the second independent premise\"\n",
    "        - claim: \"Another supporting premise for the second independent premise\"\n",
    "        - claim: \"Independent premise supporting the second independent premise\"\n",
    "          example: \"Example supporting this independent premise\"\n",
    "  ```\n",
    "\n",
    "  Here's how to read this structure:\n",
    "\n",
    "  The top-level claim is the main argument.\n",
    "  Directly nested under the claim are independent premises. These provide justification independently of other premises.\n",
    "  An example can be nested directly under a claim to provide further context or support.\n",
    "  Just like the top-level claim, each premise can itself be supported by further individual premises, or examples, creating a nested structure.\n",
    "\n",
    "\n",
    "  # Arguments to improve:\n",
    "\n",
    "  {all_arguments}\n",
    "\n",
    "  # Transcript\n",
    "\n",
    "  {transcript}\n",
    "  \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa75b11-6b3b-493c-8c32-01ef58a562cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:17:26.711954Z",
     "start_time": "2024-04-11T14:17:26.706788Z"
    }
   },
   "outputs": [],
   "source": [
    "third_cycle_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world expert at creating accessible, persuasive explanations.\"),\n",
    "    (\"user\", \"\"\"\n",
    "  Based on your own knowledge and the transcript, create a structured explanation for the following argument. Use the context from the transcript for the explanation.\n",
    "\n",
    "  # Argument to use for the explanation\n",
    "\n",
    "  {argument}\n",
    "\n",
    "\n",
    "  The structured explanation must be directly based on the argument. You can also use the provided transcript for context.\n",
    "\n",
    "  You must follow this YAML format:\n",
    "\n",
    "  ```yaml\n",
    "  counteragument_to: (what would be the argument, to which this argument is a counterargument? use your own knowledge. use bullet points)\n",
    "\n",
    "  strongest_objection: (what is the strongest, good-faith, honest objection that a thoughful person might have? use bullet points)\n",
    "  consequences_if_true: (if true, what would be the consequences? write in causal language,  use bullet points, max 3)\n",
    "\n",
    "  link_to_ai_safety: (how is this linked to AI safety? 1 sentence.)\n",
    "\n",
    "  simple_explanation: (explain this clearly to a college student in max. 4 sentences, speak persuasively as the author of this argument. don't use bullet points)\n",
    "\n",
    "  examples: (max 3 examples, use bullet points)\n",
    "\n",
    "  ```\n",
    "\n",
    "  # Here is the transcript:\n",
    "\n",
    "  {transcript}\n",
    "  \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "274af6ed-0d16-4e5b-ae57-868d7418efa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:22:12.279536Z",
     "start_time": "2024-04-11T08:22:12.272852Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_text_from_sources_and_make_chunks(directory):\n",
    "    folder_names = []\n",
    "    raw_texts = []\n",
    "    for entry in os.listdir(directory):\n",
    "        folder_names.append(entry)\n",
    "        print(folder_names)\n",
    "    for folder_name in folder_names:\n",
    "        loader = DirectoryLoader(f'./sources/{folder_name}/source', glob=\"**/*.md\")\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=10000,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len)\n",
    "        raw_text = loader.load()\n",
    "        entire_text_from_a_source = \"\"\n",
    "        for doc in raw_text:\n",
    "            entire_text_from_a_source += doc.page_content\n",
    "        chunks = text_splitter.create_documents([entire_text_from_a_source])\n",
    "        formatted_chunks = []\n",
    "        for chunk in chunks:\n",
    "            formatted_chunks.append(chunk.page_content)\n",
    "        temp = {\n",
    "            \"name\" : folder_name,\n",
    "            \"path\": f'./sources/{folder_name}',\n",
    "            \"chunks\": formatted_chunks,\n",
    "        }\n",
    "        raw_texts.append(temp)\n",
    "    return raw_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cffef20-d5b3-474c-995c-636f00c05cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:22:27.776139Z",
     "start_time": "2024-04-11T08:22:14.374332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plamer_luckey']\n"
     ]
    }
   ],
   "source": [
    "sources_dicts = load_text_from_sources_and_make_chunks(\"./sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d3166a2-6f4d-4b3d-8d48-79097c7afaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'plamer_luckey', 'path': './sources/plamer_luckey', 'chunks': ['Luckey is more concerned about the combination of human malevolence and AI capabilities than about AGI going rogue. He believes that people with ill intent, even if they only have access to moderately sophisticated AI, could cause immense harm. The amplification of human malice through AI tools is a more pressing concern to him than a superintelligent AI system pursuing destructive goals of its own accord.\\n\\nExpanding on the first point, Luckey is more worried about the misuse of narrow AI by malicious actors than the potential risks posed by advanced AI in responsible hands. He suggests that the destructive potential of AI doesn\\'t necessarily correlate with its level of sophistication - even relatively simple AI systems could be extremely dangerous if deployed by people with harmful intentions.\\n\\nLuckey sees the main danger of AI as the ways in which it could be used to further human-driven agendas of oppression, violence, and destruction. He provides examples such as religious extremists using AI to target and attack people of other faiths, or belligerent nations using AI to gain an upper hand in conflicts with their adversaries. The concern is that AI will become a powerful tool for humans to inflict harm on one another on a massive scale.\\n\\nWhen discussing \"AI in the hands of bad people,\" Luckey is referring not to AI acting autonomously, but to the various ways in which humans could harness AI to develop and deploy destructive technologies. One particularly concerning scenario he mentions is the potential for AI to be used in the creation of targeted biological weapons. As AI advances lower the technical barriers to engineering viruses and other bio-threats, even small states or groups could potentially attempt to develop weapons that target specific genetic profiles. The combination of increasingly accessible AI and biotech could be a major destabilizing force.\\n\\nIn light of these more immediate risks stemming from the malicious use of AI by humans, Luckey is comparatively less concerned about the \"sci-fi\" scenario of an advanced AI system killing off humanity entirely of its own volition. He believes there are scarier and more plausible threats that could emerge from the nexus of narrow AI and human malevolence, even without positing a highly advanced, self-directed artificial general intelligence.\\n\\nLuckey emphasizes that AI doesn\\'t need to achieve human-like self-awareness or superintelligence to pose an existential threat if deployed as tools of destruction by malicious actors. In fact, he suggests that a superintelligent AI might be better at avoiding destructive outcomes than a more limited AI system in the wrong hands. The key risk is not the sophistication of the AI itself, but the intentions of the humans wielding it. Even narrow AI could become a catastrophic weapon if used by those seeking to cause harm.\\n\\nhttps://app.fireflies.ai/view/Palmer-Luckey-On-The-Future-of-War-Israel-TikTok-Ban-U-S-Manufacturing-mp4::mf30Nfn653QHsXHn\\n\\nSure, let\\'s dive into each of Palmer Luckey\\'s arguments about AI risk in more detail:\\n\\nAI-enabled bioweapons and biodefense:\\n   Luckey acknowledges that AI could potentially be used to create highly dangerous, targeted bioweapons. However, he argues that AI is likely to provide even stronger advantages in biodefense. He envisions a future where AI could be used to develop advanced defensive measures, such as nanobots that circulate in the bloodstream and eliminate any pathogens they encounter. These AI-powered defenses could potentially adapt to and neutralize a wide range of bioweapons, providing a robust defense against engineered biological threats. Luckey suggests that the defensive advantages of AI in this domain may outweigh the risks of AI-enabled bioweapons.\\n\\nIrrational actors and targeted bioweapons:\\n   Luckey raises the concern that large language models and AI could be used by irrational actors, such as those willing to be martyrs, to create bioweapons designed to target people with specific genetic profiles. Even if these actors have a flawed understanding of the technology or the pathogen mutates, their belief that they possess such a targeted weapon could alter their decision-making around the use of biological weapons. This could lead to a scenario where an irrational actor deploys a bioweapon believing it will only affect a specific population, when in reality, it could have far more widespread and devastating consequences.\\n\\nAI-powered pathogen defense systems:\\n   Building on the idea of AI-enabled biodefense, Luckey envisions a future where multiple AI systems are constantly monitoring and updating their defenses to detect and stop any potential pathogen. He suggests that having competing AI systems working on this problem could create a layered, robust defense against biological threats. For example, an individual might have several different AI-powered \"nanobots\" in their body, each designed to identify and neutralize pathogens using different methods. These AI defenses would continually adapt and improve, making it increasingly difficult for any aggressor to successfully deploy a biological weapon. Luckey argues that this type of AI-driven defensive capability could significantly reduce the risk posed by biological threats, whether natural or engineered.\\n\\nIn summary, while Luckey acknowledges the potential risks of AI being used to create more sophisticated and targeted bioweapons, he seems to place greater emphasis on the potential defensive advantages of AI in combating these threats. He argues that the development of AI-powered defensive systems, such as advanced pathogen detection and neutralization, could provide a strong counterbalance to the risks posed by AI-enabled bioweapons.\\n\\nhttps://app.fireflies.ai/view/Palmer-Luckey-Wants-to-Be-Silicon-Valley-s-War-King-The-Circuit-mp4::MVve62Pb1v1AEbyW\\n\\nSure, let\\'s dive deeper into each of Palmer Luckey\\'s arguments about AI risk:\\n\\nPotential for misuse:\\n   Luckey acknowledges that AI, like any powerful technology such as weapons or aircraft, could be misused. However, he believes that rather than avoiding the technology altogether, we should place our trust in the democratic system and policymaking to control their use appropriately. This implies that the risks can be managed through proper governance and oversight, rather than by shunning the technology.\\n\\nHuman accountability:\\n   Luckey strongly emphasizes that humans must remain accountable for the deployment and use of AI-powered weapon systems. He argues that the presence of an algorithm does not absolve humans of responsibility. The person in charge of deploying the system must have a deep understanding of its limitations and be held accountable for any consequences. This principle is crucial to ensuring that AI is used responsibly and ethically in warfare.\\n\\nInevitability of AI-related deaths:\\n   Luckey believes that if AI becomes a core part of warfare, it is inevitable that some people will be killed by AI who should not have been. He sees this as a certainty, given the imperfect nature of any technology. However, he argues that keeping humans accountable for these mistakes is essential, as it will drive the development of better solutions and help minimize unintended deaths and civilian casualties over time.\\n\\nAI as a potentially better alternative:\\n   While Luckey acknowledges that the use of AI in warfare is not ideal, he suggests that it may still be preferable to existing technologies in terms of preventing unintended deaths. This implies that AI, when developed and deployed responsibly, could potentially be more precise and discriminating than current weapons systems, thereby reducing collateral damage.\\n\\nAI as a tool for clarity:\\n   Luckey challenges the notion that AI could deepen the \"fog of war,\" a term referring to the uncertainty and confusion in battlefield situations. Instead, he believes that AI will provide more information to all parties involved, including adversaries. He suggests that even dictators would have access to better information, which could help them make more informed decisions and possibly prevent conflicts. In this sense, he sees AI as a tool that could enhance clarity and reduce misunderstandings that lead to war.\\n\\nBy elaborating on each point, we can see that Luckey\\'s perspective on AI risk is nuanced. While he recognizes the potential dangers, he believes they can be mitigated through responsible development, human accountability, and proper governance. He sees AI as an inevitable part of the future of warfare and argues that the focus should be on harnessing its potential benefits while minimizing its risks.\\n\\nhttps://app.fireflies.ai/view/This-Autonomous-Reusable-Fighter-Jet-Can-Blow-Up-Drones-2024-05-24-15-07-26-mp4::ubc6FXEE0NWtoayn\\n\\nSure, let me provide more detailed elaborations on each of Palmer Luckey\\'s points regarding AI risk:\\n\\nThe U.S. government has been deploying autonomous weapons systems for decades that can identify and engage targets without human input for each individual engagement. Luckey cites examples like radar-seeking missiles that can independently acquire and track targets, as well as defensive systems like SeaRAM and Phalanx CIWS on naval ships that automatically detect, track and destroy incoming threats like missiles, artillery, etc. His argument is that AI-enabled autonomous engagement is not a new concept, as the military has been using such systems for a long time already.', 'Luckey states that established U.S. military doctrine has strong guidelines that ensure meaningful human control and accountability over autonomous weapons, even if humans are not manually controlling each engagement decision. The key point is that a human still makes the decision to arm and deploy the autonomous system itself, understanding its capabilities and rules of engagement. So while the system may automatically engage targets meeting certain criteria during its operation, the human operator retains oversight and responsibility.\\n\\nHe directly pushes back against the idea that AI should never be able to independently decide to engage and \"pull the trigger\" on a target. Luckey argues that allowing AI targeting can actually be more ethical and discriminate than legacy \"dumb\" munitions like landmines that cannot distinguish between civilian and military targets. His view is that as long as a human authorizes the deployment with an understanding of the autonomous system\\'s target identification parameters, it is better to have AI determination of individual aimpoints rather than indiscriminate area effects.\\n\\nLuckey strongly criticizes many recent voices raising concerns about AI weapons as being \"performative\" - doing it for show rather than from true expertise. He claims these people have little background actually studying the ethical and policy questions around autonomous weapons that military organizations have grappled with for decades. In his view, they are vastly behind the work that has already been done in this domain.\\n\\nHe portrays the United States military as the clear global leader when it comes to responsibly developing and applying AI for autonomous weapons systems, backed by long-standing policies, ethical frameworks and technical expertise built up over many years. Luckey sees the U.S. as being far ahead of recent entrants to the discussion around AI safety for military applications.\\n\\nThe overarching thrust of Luckey\\'s arguments is that AI-enabled autonomous engagement is an evolution of prior military technologies, not a revolutionary new concept. He believes there are robust processes in place to ensure meaningful human control. And he positions the U.S. as not only being aware of AI risks for decades, but as the vanguard in responsibly managing those risks through proper doctrine and ethical guidelines, contrary to what he sees as uninformed criticisms.']}]\n"
     ]
    }
   ],
   "source": [
    "print(sources_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bafcb6e1-e0fb-41bd-8469-b463d105ee1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:23:15.750986Z",
     "start_time": "2024-04-11T08:23:15.742372Z"
    }
   },
   "outputs": [],
   "source": [
    "def first_cycle_of_extracting_arguments(dicts):\n",
    "    first_cycle_chain = first_cycle_prompt | llm\n",
    "    for dict in dicts:\n",
    "        dict[\"arguments\"] = []\n",
    "        for chunk in dict[\"chunks\"]:\n",
    "            first_cycle_response = first_cycle_chain.invoke({\"transcript\": chunk})\n",
    "            dict[\"arguments\"].append(first_cycle_response.content)\n",
    "        print(\"over\")\n",
    "        text = '\\n\\n'.join(dict[\"arguments\"])\n",
    "        filename = f\"{dict['path']}/steps/first_step.md\"\n",
    "        write_to_file(filename, text)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16983b7e-dc71-418e-b5e4-f340b1846e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:30:43.919323Z",
     "start_time": "2024-04-11T08:26:00.234801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n",
      "Text successfully written to ./sources/plamer_luckey/steps/first_step.md\n"
     ]
    }
   ],
   "source": [
    "dicts_with_extracted_args = first_cycle_of_extracting_arguments(sources_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d0d68af-6a24-40fd-aa63-7ae2cdfb0470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:31:22.861422Z",
     "start_time": "2024-04-11T08:31:22.855702Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dict_to_json(dicts_with_extracted_args, \"./sources/json/first_cycle_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2acb9d74-4988-4f3e-9cb3-9a53956ec7b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:31:24.641759Z",
     "start_time": "2024-04-11T08:31:24.633681Z"
    }
   },
   "outputs": [],
   "source": [
    "def second_cycle_of_extracting_arguments(dicts):\n",
    "    second_cycle_chain = second_cycle_prompt | llm\n",
    "    for dict in dicts:\n",
    "        dict[\"improved_arguments\"] = []\n",
    "        for i, chunk in enumerate(dict[\"chunks\"]):\n",
    "            second_cycle_response = second_cycle_chain.invoke({\"all_arguments\": dict[\"arguments\"][i], \"transcript\": chunk })\n",
    "            dict[\"improved_arguments\"].append(second_cycle_response.content)\n",
    "        print(\"over\")\n",
    "        text = '\\n\\n'.join(dict[\"improved_arguments\"])\n",
    "        filename = f\"{dict['path']}/steps/second_step.md\"\n",
    "        write_to_file(filename, text)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b45be49-f2ce-4e39-95e4-bd7ed762809b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:36:46.573109Z",
     "start_time": "2024-04-11T08:31:27.330814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n",
      "Text successfully written to ./sources/plamer_luckey/steps/second_step.md\n"
     ]
    }
   ],
   "source": [
    "dicts_with_extracted_and_improved_args = second_cycle_of_extracting_arguments(dicts_with_extracted_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41ae0410-b3c8-4c53-8451-8dbe915c6bde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:04.364822Z",
     "start_time": "2024-04-11T08:37:04.358169Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_improved_arguments_in_dict(dicts):\n",
    "    for dict in dicts:\n",
    "        dict[\"isolated_arguments\"] = []\n",
    "        for improved_arg in dict[\"improved_arguments\"]:\n",
    "            if improved_arg:\n",
    "                splitted_args = improved_arg.split(\"```yaml\")\n",
    "                splitted_args_cleaned = []\n",
    "                for arg in splitted_args:\n",
    "                    arg_clean = arg.split(\"```\")[0]\n",
    "                    if (arg_clean != \"\"):\n",
    "                        splitted_args_cleaned.append(arg_clean.strip())   \n",
    "                if splitted_args_cleaned != None:\n",
    "                    dict[\"isolated_arguments\"].append(splitted_args_cleaned)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6176109-6d8a-4e58-861b-e8707aeb0b0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:38:15.049848Z",
     "start_time": "2024-04-11T08:38:15.045485Z"
    }
   },
   "outputs": [],
   "source": [
    "dicts_with_isolated_improved_arguments = split_improved_arguments_in_dict(dicts_with_extracted_and_improved_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a1eec61-b2d6-43c5-bde6-26e2bcbd1725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:15:09.442636Z",
     "start_time": "2024-04-11T12:15:09.373322Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dict_to_json(dicts_with_isolated_improved_arguments, \"./sources/json/second_cycle_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bbb2a99-f23f-4c28-9812-386bdf73ded8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T13:13:02.090399Z",
     "start_time": "2024-04-11T13:13:02.082830Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_embeddings_with_smaller_chunks(directory):\n",
    "    folder_names = []\n",
    "    all_chunks = []\n",
    "    dbs = []\n",
    "    for entry in os.listdir(directory):\n",
    "        if entry != \"json\":\n",
    "            folder_names.append(entry)\n",
    "    for folder_name in folder_names:\n",
    "        loader = DirectoryLoader(f'./sources/{folder_name}/source', glob=\"**/*.md\")\n",
    "        text_splitter = TokenTextSplitter(chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len)\n",
    "        raw_text = loader.load()\n",
    "        entire_text_from_a_source = \"\"\n",
    "        for doc in raw_text:\n",
    "            entire_text_from_a_source += doc.page_content\n",
    "        chunks = text_splitter.create_documents([entire_text_from_a_source])\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata = {\"folder\": folder_name}\n",
    "            all_chunks.append(chunk)\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "        db = Chroma.from_documents(all_chunks, embeddings)\n",
    "        dbs.append(db)\n",
    "    return dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71a58393-7906-47ea-8070-6082914bf81c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T13:13:09.966428Z",
     "start_time": "2024-04-11T13:13:03.882656Z"
    }
   },
   "outputs": [],
   "source": [
    "dbs = create_embeddings_with_smaller_chunks(\"./sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2a48a0d-bf72-40fb-85f9-5033db492991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T14:54:08.800908Z",
     "start_time": "2024-04-11T14:54:08.790963Z"
    }
   },
   "outputs": [],
   "source": [
    "def third_cycle_of_extracting_arguments(dicts):\n",
    "    third_cycle_chain = third_cycle_prompt | llm\n",
    "    for i, dict in enumerate(dicts):\n",
    "        dict[\"explanations\"] = []\n",
    "        if i < len(dbs):  \n",
    "         db = dbs[i]\n",
    "        else:\n",
    "         db = None\n",
    "        for i, chunk in enumerate(dict[\"chunks\"]):\n",
    "            explanations = []\n",
    "            for arg in dict[\"isolated_arguments\"][i]:\n",
    "                docs = db.similarity_search(arg, k=3)\n",
    "                context = \"\"\n",
    "                for doc in docs:\n",
    "                    context += \"\\n\" + doc.page_content\n",
    "                explanation = third_cycle_chain.invoke({\"argument\": arg, \"transcript\": context})\n",
    "                explanations.append(explanation.content.split(\"```yaml\\n\")[1].split(\"```\")[0].strip())\n",
    "            dict[\"explanations\"].append(explanations)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5de267f-8b42-48fa-8ad5-a5a5341d60fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:27:54.098431Z",
     "start_time": "2024-04-11T14:54:12.984960Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dicts_with_arguments_and_explanations = third_cycle_of_extracting_arguments(dicts_with_isolated_improved_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c825fd1-8012-402d-83db-ef597ad6d8d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:38.841567Z",
     "start_time": "2024-04-11T15:30:38.832998Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dict_to_json(dicts_with_arguments_and_explanations, \"./sources/json/third_cycle_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ab9f15f-524b-4fdf-88d4-d7aa7011997f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:41.208291Z",
     "start_time": "2024-04-11T15:30:41.199556Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_third_step(dicts):\n",
    "    for dict in dicts:\n",
    "        dict[\"final_arguments\"] = []\n",
    "        for i, isolated_argument_group in enumerate(dict[\"isolated_arguments\"]):\n",
    "            for j, single_isolated_arg in enumerate(isolated_argument_group):\n",
    "                final_arg = \"\\n\".join([single_isolated_arg, dict[\"explanations\"][i][j]])\n",
    "                dict[\"final_arguments\"].append(final_arg)\n",
    "                final_arg_yaml_format = \" ```yaml\\n\" + final_arg + \"\\n```\\n\\n\"\n",
    "                filename = f\"{dict['path']}/steps/third_step.md\"\n",
    "                write_to_file(filename, final_arg_yaml_format)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0154e3c-3746-40e2-ad9f-ae5d6dd4ae2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:44.191326Z",
     "start_time": "2024-04-11T15:30:44.177897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n",
      "Text successfully written to ./sources/plamer_luckey/steps/third_step.md\n"
     ]
    }
   ],
   "source": [
    "final_dicts = save_third_step(dicts_with_arguments_and_explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36b27afc-ee82-4e3e-9cce-0f3e4ce2e5e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:47.458411Z",
     "start_time": "2024-04-11T15:30:47.445456Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_directory_structure_for_chatbots(directory_name):\n",
    "    script_directory = os.getcwd()\n",
    "    new_directory_path = os.path.join(script_directory, directory_name)\n",
    "    if not os.path.exists(new_directory_path):\n",
    "        os.makedirs(new_directory_path)\n",
    "        yaml_file_path = os.path.join(new_directory_path, \"metadata.yaml\")\n",
    "        with open(yaml_file_path, \"w\") as yaml_file:\n",
    "            with open(yaml_file_path, \"w\") as yaml_file:\n",
    "                yaml.dump({}, yaml_file)\n",
    "        prompts_folder_path = os.path.join(new_directory_path, \"prompts\")\n",
    "        knowledge_base_folder_path = os.path.join(new_directory_path, \"knowledge_base\")\n",
    "        for folder_path in [prompts_folder_path, knowledge_base_folder_path]:\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "                system_prompt_file_path = os.path.join(prompts_folder_path, \"system_prompt.md\")\n",
    "                with open(system_prompt_file_path, \"w\") as system_prompt_file:\n",
    "                    system_prompt_file.write(\"System Prompt\")\n",
    "    if os.path.exists(new_directory_path):\n",
    "        yaml_file_path = os.path.join(new_directory_path, \"metadata.yaml\")\n",
    "        with open(yaml_file_path, \"w\") as yaml_file:\n",
    "            yaml.dump({\"name\": None, #add your chatbot name here don't use quotes when naming chatbot \n",
    "                       \"tags\": None, #tag your chatbot can be #optimistic or #pessimistic\n",
    "                       \"based_on\": None #provide the uri to source of the raw_text used for arguments extraction, add it in new line with dash in front uri (- https://uri_to_source.com) \n",
    "                      }, yaml_file, sort_keys=False)\n",
    "        prompts_folder_path = os.path.join(new_directory_path, \"prompts\")\n",
    "        knowledge_base_folder_path = os.path.join(new_directory_path, \"knowledge_base\")\n",
    "        for folder_path in [prompts_folder_path, knowledge_base_folder_path]:\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7d0f150-8dd7-4f22-b4e3-5a90fe87ef9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:50.396061Z",
     "start_time": "2024-04-11T15:30:50.390986Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_chatbots(dicts):\n",
    "    for dict in dicts:\n",
    "        create_directory_structure_for_chatbots(dict[\"name\"])\n",
    "        prompt_filename = f'./{dict[\"name\"]}/prompts/system_prompt.md'\n",
    "        with open(prompt_filename, 'w') as file:\n",
    "            file.write(\"Use arguments provided to answer the question.\\n\\nArguments:\\n\\n{arguments}\")\n",
    "        for i, final_arg in enumerate(dict[\"final_arguments\"]):\n",
    "            filename = f'./{dict[\"name\"]}/knowledge_base/{dict[\"name\"]}-{str(i + 1)}.md'\n",
    "            with open(filename, 'w') as file:\n",
    "                file.write(final_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09e910d2-05b9-4ba3-9dd6-b7864137f4fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:53.428704Z",
     "start_time": "2024-04-11T15:30:53.412736Z"
    }
   },
   "outputs": [],
   "source": [
    "create_chatbots(final_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "29732691-f432-41bc-91ca-255740d541d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:30:55.623582Z",
     "start_time": "2024-04-11T15:30:55.612017Z"
    }
   },
   "outputs": [],
   "source": [
    "dicts_with_isolated_improved_arguments = load_results_from_json(2)\n",
    "dicts_with_arguments_and_explanations = load_results_from_json(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11187a69-7f67-4195-8806-ccd4251f3589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
