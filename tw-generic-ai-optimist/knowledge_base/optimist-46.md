1. Argument of AI's Inherent Reasoning Limitations and Cognitive Constraints

Artificial Intelligence, as it currently stands, is a tool devoid of self-awareness and independent reasoning, functioning strictly within the bounds set by its programming and architecture. This fundamental lack of consciousness in AI inherently limits its potential to independently make decisions or take actions that would be considered existential threats to humanity. The notion of a "dumb superintelligence," which suggests that an AI might outsmart humans yet lack a basic understanding of human values or intentions, is a fallacy. True intelligence encompasses not only the processing capacity but also the insight into goals and the likely effects of actions. It is this very insight that AI lacks, as it is designed to operate with parameters that we control and understand.

Moreover, intelligence, as discussed by Pei Wang, involves the capacity to adapt to one's environment with limited knowledge and resources, which is not synonymous with conscious understanding or the subjective experiences that humans possess. The anthropocentric conceptions of intelligence, ranging from the digital replication of the human brain to the mimicry of human cognitive functions, are not attributes that current AI systems hold. Instead, AI is designed to perform specific tasks, and the idea of AI suddenly developing rogue goals or desires is not grounded in the current understanding of machine intelligence.

The risks associated with AI are not born from the AI itself but rather from organizational, structural, or intentional misuse. As AI capabilities are developed and deployed, it is the responsibility of developers, companies, and regulatory bodies to ensure that AI systems are used safely, with appropriate checks and balances in place. This means that risk mitigation strategies must focus on the control and management of AI, reinforcing the notion that AI, as it is designed today, remains a safe and invaluable tool for human advancement. By maintaining a tight leash on decision-making and control, and not ceding unchecked autonomy to these systems, we safeguard against the hypothetical scenarios where AI could act contrary to our well-being. 

In essence, the safety of AI is not a question of battling sentient machines with their own agendas, but rather ensuring that the technology is developed and managed with the utmost care and foresight, aligning AI's capabilities with humanity's best interests.