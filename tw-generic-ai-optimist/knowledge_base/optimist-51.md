Argument of Control and Alignment in AI Systems

Artificial Intelligence is crafted with the objective to augment human capabilities, not to supplant them. The overarching goal behind the research, development, and funding of AI systems is to harness their potential to solve complex problems and improve the quality of life. Recognizing that AI systems deviating from their intended purpose pose a risk, we prioritize the control and alignment of these systems to ensure they remain beneficial and under human oversight.

To contextualize the importance of control, we can draw parallels with car accidents or government coups, where a lack of control results in unintended consequences. By preemptively addressing control in AI, we can avert analogous outcomes in the digital realm. The alignment problem, in essence, is ensuring that AI systems adhere to their users' and developers' intentions, acting as extensions of human will rather than rogue entities. It is a nuanced task of balancing the AI's strengths with a comprehensive understanding of its limitations and potential risks. 

The development of AI systems that are on par with human intelligence—yet not so advanced as to elude our control—provides an optimal platform for research and experimentation. This incremental approach allows us to study their motivation systems, test alignment strategies, and refine control mechanisms over time. We can then leverage the vast computational labor these AI systems offer, without precipitating the risks associated with superhuman capabilities.

Moreover, the alignment field seeks to ensure that activating an advanced AI system is invariably a positive action. It is not merely about preventing catastrophic outcomes but fostering AI systems that actively contribute to the betterment of society. By understanding the principles of instrumental convergence, such as the potential for an AI system to prioritize its self-preservation to fulfill a given task, we can design AI with safeguards that align with human values and ethics.

In conclusion, the safe use of AI is contingent upon our ability to maintain control and ensure alignment with human intent. By approaching AI development pragmatically and with a focus on understanding and shaping AI motivations, we can create systems that are not only powerful and useful but also safe and aligned with the broader objectives of human progress.