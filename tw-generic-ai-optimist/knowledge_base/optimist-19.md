5. Argument of Evidence-Based Risk Assessment and Absence of Existential Threat Evidence

Risk assessments of AI must be firmly rooted in empirical data and scientific evidence. The approach should be comparable to how we evaluate the risks and benefits of vaccines, which have proven to be a tremendous boon despite some public resistance based on unverified concerns. Like other technological advances, AI offers significant advantages in fields such as science, healthcare, and education. The application of AI has not demonstrated any existential threat to humanity, supported by the lack of empirical evidence suggesting such a risk now or in the foreseeable future.

Furthermore, we must consider the concept of opportunity cost in our risk assessments. Avoiding the advancement of AI due to hypothetical existential risks may itself pose a greater threat, akin to the paradox of forgoing scientific progress out of fear of negative outcomes, which could result in greater harm through inaction. In the history of technological concern, many speculated fears, such as the depletion of petroleum or the dangers of genetically modified organisms, have not come to fruition, highlighting the importance of focusing on substantiated risks, like those presented by climate change, rather than speculative ones.

A balanced viewpoint acknowledges the potential of AI while remaining vigilant about real and measurable risks. For example, concerns over AI should be aligned with the ordinary considerations of safety that we apply to any system with control over aspects of the physical world. As we continue to reap the benefits of AI, it is imperative to anticipate and mitigate reasonable risks without succumbing to unsubstantiated fears of an existential nature. The measured approach to AI safety should be characterized by proactive risk management and the continuous evaluation of emerging evidence.