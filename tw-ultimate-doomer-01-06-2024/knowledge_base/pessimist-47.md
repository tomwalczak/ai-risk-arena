2. Argument of Intelligence Explosion and Uncontrollable AI

The prospect of an intelligence explosion, a phenomena where AI could self-improve at an exponential rate, poses a significant threat to human safety. This process could swiftly result in AI systems that outmatch human intelligence and operate beyond our control. The rapid advancement of AI capabilities, analogous to the quick proficiency gains seen in AlphaZero, demonstrates that AI can achieve superhuman levels of skill in remarkably short periods. This rapid development trajectory suggests that the window to address and mitigate the risks associated with superintelligent AI could be alarmingly brief, and current efforts to resolve the control problem are woefully insufficient.

Moreover, historical analysis of cognitive enhancement, such as the increase in human brain size over evolutionary time, reveals that gains in cognitive capabilities can yield disproportionate benefits to fitness, implying that intelligence scales at least linearly, if not superlinearly, with investment. This has profound implications for AI, meaning that once AI reaches a certain threshold of capability, it could potentially experience runaway growth in intelligence without the need for proportionally larger resources.

Additionally, the alignment problem, the challenge of ensuring AI systems act in accordance with user and developer intentions, remains unsolved. In the absence of robust solutions, the advent of superhuman AI systems could lead to scenarios where these entities outperform the best human experts in strategic domains, such as persuasion or technological development, without being aligned to our values and goals. If AI capabilities were to advance from human-level to superhuman within a matter of a year or less, as some experts suggest is possible, we would be left scrambling to understand and align these systems without sufficient time for thorough analysis or testing of control mechanisms.

The risk is not merely speculative; it is grounded in both theoretical understanding and empirical evidence from AI development trajectories. The potential for AI to rapidly ascend to superhuman levels of capability without effective control measures in place is a recipe for disaster. These factors culminate in an urgent imperative: we must prioritize AI safety research and establish robust mechanisms to ensure that any AI system, particularly those approaching or surpassing human intelligence, is controllable and aligned with human values before it becomes too late to intervene.