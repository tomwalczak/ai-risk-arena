The Peril of Overdependence on Sub-Human-Level AI Systems

The reliance on artificial intelligence that has yet to achieve human-level intelligence is fraught with risks that warrant serious concern. The current landscape of AI development is such that we lack a comprehensive understanding of the alignment problem—the challenge of ensuring AI systems conform to the intents of their users and developers. This gap in our knowledge poses a significant danger as we inch closer to creating AI with capabilities that may soon exceed human expertise in strategic, persuasive, or technological domains.

As AI systems begin to automate tasks historically done by human researchers, we risk accelerating development at a pace that outstrips our ability to understand and govern these systems. The hypothetical scenario in which AI could automate 80% of research tasks by 2030 illustrates a trajectory where the rate of AI advancement could lead to superhuman AI within mere years. If the quality of AI algorithms and hardware were to improve at three times the current rate, we could face a future where AI not only matches but significantly surpasses human capabilities in numerous areas, and this transition could occur in a disturbingly brief time span.

The dangers of AI are not confined to distant superhuman prospects; they are also present in the near term with systems that are less than human-level. The deployment of autonomous vehicles, for instance, provides a tangible example of the perils of overdependence on AI. Despite their advanced capabilities, these vehicles have been involved in accidents when faced with complex driving scenarios—a testament to the fact that current AI systems can fail in high-stakes situations.

The projected speed of AI development does not afford us the luxury of gradual adaptation or the establishment of effective governance and regulation, which traditionally take considerable time to implement. Without these safeguards, we are essentially 'flying by the seat of our pants,' relying on hastily conceived solutions to control AI systems whose motivations and potential for harm we do not fully grasp.

This reckless pace of advancement, coupled with the lack of time to develop and test strategies for alignment and control, leads us to a precipice where we may inadvertently relinquish control to AI systems. The vision of a future where AI makes critical decisions across society, embedded in government, law enforcement, and influential corporations, is not a distant dystopia but a plausible outcome of our current trajectory. Such a future, where thousands of decisions are made without adequate human oversight, is indeed a cause for alarm.

Given these concerns, it is evident that the integration of AI into roles traditionally held by humans requires caution, robust research into safety and alignment, and a measured approach to development. The assumption that AI systems are ready to safely take on complex tasks is not only premature but also potentially catastrophic. It is imperative that we prioritize the understanding and mitigation of risks associated with AI to prevent the overreliance that could lead to dangerous outcomes.