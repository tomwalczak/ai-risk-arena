claim: "There's no actual predictive model for the probability that AI will kill us all."
premises:
  - claim: "The 95% chance number is completely made up."
  - claim: "Predictions about AI dangers are based on personal feelings rather than empirical data."
counterargument_to:
  - "AI poses a significant existential risk to humanity, with a high probability of causing human extinction."
  - "Experts in the field agree on a high probability that AI will lead to humanity's demise."

strongest_objjection:
  - "Many discussions and predictions around AI risks involve experts in the field, and dismissing their concerns as merely personal feelings overlooks the depth of their understanding and the complexity of AI development."

consequences_if_true:
  - "If there's no accurate predictive model, then resources dedicated to mitigating AI existential risks may be misallocated."
  - "Public and policy discussions about AI safety might be led astray by unfounded claims, undermining effective strategies for managing AI development."
  - "The lack of empirical data to support predictions could lead to either complacency or unnecessary panic regarding AI advancements."

link_to_ai_safety: This argument challenges the foundation of AI safety discussions by questioning the validity of existential risk predictions.

simple_explanation: The claim that AI will definitely lead to humanity's extinction is not supported by any concrete predictive model; it's largely based on personal judgments rather than hard evidence. The widely varying estimates, such as a 20% or 95% chance of AI causing human extinction, indicate more about individual perspectives than about any objective reality. Relying on such subjective assessments can mislead both public opinion and policy decisions regarding AI, especially when actual surveys show a more nuanced view of AI's potential risks and benefits.

examples:
  - "The Ipsos Global Views on AI 2023 report showing varying attitudes towards AI based on age and nationality, rather than a consensus on existential risk."
  - "Historical instances where expert consensus was wrong, highlighting the need for empirical evidence in making predictive claims."
  - "The debate within the AI alignment community, with vastly different estimates of existential risk, demonstrates the subjective nature of these predictions."