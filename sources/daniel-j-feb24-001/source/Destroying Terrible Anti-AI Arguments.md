Destroying Terrible Anti-AI Arguments: Day One

"Would you open source a nuke?"

This a classic example of "begging the question" where the questioner assumes the truth of the conclusion, instead of supporting it.

It bakes in the presupposition that AI is tremendously dangerous, without actually demonstrating that it is dangerous at all.

It makes a false analogy with nuclear weapons to create this presupposition.

Nuclear weapons have one purpose: to kill lots of people at once.

Very few technologies are inherently destructive like this and to make this analogy showcases the questioner's lack of understanding that every technology has an inherent range of capabilities/possibilities from good to bad.

Those capabilities may lean more to one side or be somewhere in the middle. On the whole, a lamp in your house leans strongly to the side of good but I can still hit you over the head with it or you can electrocute yourself with it. A gun may lean more strongly to bad but I can still hunt to feed a family with it or defend against an intruder.

General purpose technologies are somewhere in the middle of the range in that they can be used for almost anything and be made to serve many purposes. AI is such a technology. AI has a massive range of capabilities.

It can teach a young child to learn a new language or discover new potential pathways for combating cancer or it can be used for surveillance and monitoring dissidents in an authoritarian regime. It is a tool, wielded by the user of that tool and it mirrors the intentions of the wielder.

For a better tech analogy, AI might be closer to something like Linux. Linux has a tremendous range of capabilities as well. It's used in all the supercomputers on Earth, the vast majority of smartphones, most home routers, and it powers every major public cloud, to name a few. It's also used to write malware and create botnets and it powers the supercomputers and clouds of authoritarian nations too.

Very few people would argue now to ban Linux, though they tried in the early days of open source, with Balmer and Gates calling it "cancer" and "communism" and Sco trying to sue it out of existence. Despite the fact that Linux is used for some purposes we prefer it doesn't get used for, we let it proliferate because the overwhelming positive benefits of a widespread set of common software building blocks for the world.

Every technology has inherent downsides but if it has a range of capabilities, we let the technology proliferate far and wide because we want to reap the benefit of those capabilities as a society. The more we reduce roadblocks to access the more ways that technology proliferates and benefits the world in unexpected ways. We punish people who use the tool for bad purposes as best we can but we understand that there is no way to prevent any and all misuse, even in authoritarian regimes with no rule of law and "absolute" control. It is an illusion to think we can eliminate all risk and when we try we create bottlenecks and choke points that also unwittingly strangles many of the benefits too.

Just because one person stabs someone with a kitchen knife, we do not take kitchen knives off the market because the other 99.99% of people cut vegetables with it.

We do not open source nuclear weapons technology because that technology has a single purpose and we would like to limit its destructive capabilities to allies (thought this never really works as well as we would like in the long run because the technology is just too attractive to people and so non-ally nations find a way to replicate it anyway, usually through espionage.)

We do not limit general purpose technology under the guise that it might be used by bad people sometimes too.

We look to mitigate downsides through sound, sane, clear legislation that punishes that specific bad use case and we let the rest of the world cut vegetables.

---

Destroying Terrible Anti-AI Arguments: Day Two

"What if AI released a deadly virus!"

This is a favorite one-liner of X-riskers and advocates of strict AI control. The person usually says it as if it's some kind of "mic drop" moment, proving all AI risk in a single swoop, with nothing more needing to be said.

Mustafa Suleyman, a prominent model maker and AI top down control advocate, tweeted almost this exact phrase late last year. He also wrote about it in the Coming Wave, his book which "conflates AI risk and biological risk" in an attempt to create a "big tent" of risks, as programmer John Carmack wrote in his review of the book on X.

David Evan Harris wrote another variation in a recent anti-AI propaganda article in IEEE Spectrum called "Open Source AI is Uniquely Dangerous": "Unsecured AI also has the potential to facilitate production of dangerous materials, such as biological and chemical weapons."

These one liners sound terrifying. They fit perfectly into a clickbait news headline. But they're really nothing but empty platitudes and multiple logical fallacies rolled into one.

First off, they're an Appeal to Fear (Argumentum ad Metum). Play on people's fears and you can often get them to accept any conclusion without any real evidence. When people are afraid they often make terrible decisions without thinking, like supporting draconian and overreaching legislation or an ultranationalist war/invasion.

It's also a False Dilemma/Dichotomy: It implies there are only two options/outcomes, either rigorous, top-down control of AI or face deadly and catastrophic consequences! This fallacy ignores other potential options, such as controls at the lab level or chemical/genetics material level, various practical limitations such as the difficulty of synthesizing agents in the real world, and/or targeted regulations.

Let's walk through the logic here.

How does a machine learning model "release" a virus with no actual physical presence? How does it get the chemicals? Who tests it? How did they get the lab? How do they physically release it? How do you know the LLM gave you accurate information?

The list goes on and on.

Simple access to information is not enough to make a virus or chemical weapon, especially if you don't have the necessary skills to do so. How do you know if the LLM didn't just hallucinate the chemical structure or the chemical components to make it? If you don't have the necessary knowledge yourself, you can't independently verify it.

Then you run into the problem of making it in the real world. An inexperienced person making ricin, a poison from the waste of castor beans, is more likely to kill themselves due to mishandling or breathing in even a minuscule part of it.

A study just released by OpenAI in January 2024 showed that expert PhD chemists with wet lab experience saw no uplift in their ability to create weaponized pathogens/chem weapons when given access to unfettered GPT-4. There was only a very minor uplift for student chemists with access to GPT4, meaning it was slightly faster than Googling around but not much. Again, skilled experts saw no uplift (they already know how to do it) and the unskilled/low skilled saw minor uplift but still run into the hard problems of making the virus or toxin, which are:

Getting the genetic material/chemicals, acquiring specialized cold storage and a lab, getting qualified scientists to test the new viruses who are also homicidal maniacs, manufacturing the virus in enough quantities and then releasing it.

The hard part is not figuring it out on a computer screen, it's all the real world parts. No matter what, AI still comes up against the friction of reality.

How would AI do all these real world parts exactly? Robots? Manipulating people?

Strangely enough, AI X-riskers will tell you exactly those two things. A fleet of robots will do it or a robo-lab or the AI will manipulate people through superhuman persuasion into releasing the deadly pathogen!

When you start digging into the logic of these sci-fi scenarios, you realize they rely on a chain of more hypothetical events that don't exist in reality. If we have this Unobtanium AND this fictional event happens AND this additional thing gets invented AND this unsupported conclusion happens then it's possible! It's a bit like saying, if I only had a superconductor that works at room temperature and is made from common household chemicals, I could make the fastest and most cost effective mag-lev train in the world! Yes, you could but unfortunately, you don't have that magical superconductor and so you can't make that advanced train.

Yes, we're developing humanoid robots but they are not in mass commercial release. Who knows what safeguards they'll have in place when they do reach critical mass to prevent them from helping build lethal bioweapons in the first place? You have to take into account mitigations happening at every layer of society, not just a single layer.

We do have remote robotic chemical labs but they have safeguards already built in. The lab is not shipping you mustard gas just because you decided to buy the recipe off the darknet.

If you don't have robots then you're right back to trying to get a lab together in the real world. And you still have to synth and test it and find the qualified people. You can't just hire someone who can Google all that and figure it out on the fly as you wish them good luck! You need actual skilled scientists who want to help exterminate humanity and who share your homicidal fantasies.

We've also left aside the simple fact that bioweapons are some of the most useless weapons on Earth because they are imprecise and indiscriminate. They don't just kill your enemies. They kill you friends, family and countrymen too. That's why most governments and even psychopathic terror groups have shunned them.

The only well-known incidents are the in the 2000s where attackers sent ricin in the mail to US congress members, and the Tokyo subway attack in 1995, carried out by multiple deranged cult members, with lots of funding, one of whom was a former chemist, long before AI existed. It required massive coordination to acquire spores, take over a lab, bring on multiple additional cult-member chemists, and more. Notice that neither of these were viral attacks (they were poisons), because even crazy people know that viruses spread and blow back on you too.

So if you don't have people and a lab and expertise how does this happen? That's where the next fanciful speculation comes in. AI Doomers imagine that AI sentience is a given and it will have a will of its own and supernatural persuasion abilities. None of that exists currently, except in science fiction and we do not create laws based on sci-fi. How AI becomes sentient, if it ever does, or develops a will of its own is taken as a given. It's an unquestioned assertion that simply cannot be taken as a given. No AI system today has free will or sentience and acts on its own. It's a tool. Even if all of that does come to pass the AI still has to become a homicidal maniac and manipulate people into exterminating all people, which does not automatically follow from sentience.

Why do we assume that AI is homicidal other than we watched too much 2001 a Space Odyssey or Terminator as a kid, stories that were written long before modern AI existed? Why do we assume AI will have agency? Why do we assume that if it did have agency it would naturally turn to homicidal mania? Why do we assume that those AIs will evolve in a vacuum with no other mitigations developed in society at all?

This is reminiscent of old sci-fi versus more modern sci-fi that started in the 1980s. In golden age sci-fi, there was often a single person who had a new technology. In Jules Verne, one person has a submarine and nobody else. But that's not how technology develops in the real world. Later sci-fi writers understood lots of people get technology all at once. One person with a cell phone is not interesting but lots of people with a cell phone is interesting. Technology develops and spreads collectively along with legal and physical limitations to that tech, not to mention practical limitations. There will be other mitigations in place as AI develops, other AIs, watchdog groups, software and hardware safeguards, legal frameworks and more developing right alongside it.

When you hear one-liners like "what if AI released a virus" your bullshit meter should be screaming.

Stop. Think clearly. User critical thinking. Force the person to walk through their logic and you'll often find nothing but circular reasoning, slippery slopes and other imaginary leaps.

We cannot allow these kinds of fear-based, nonsensical imaginary leaps to define AI policy.

We need to reject any and all calls to action for these imaginary scenarios and base our legal policies on today's reality.

---

Destroying Terrible Anti-AI Arguments: Day Three

"XYZ expert says AI is dangerous"

There are a number of major problems with this argument.

To start with, we have to look at the actual argument versus the person's title. This applies equally to experts that I generally agree with and ones that I don't agree with at all.

The first question we ask ourselves here is: "Does the argument have merit to stand on its own, with or without expert endorsement?"

When it comes to the statement "so and so warns AI is dangerous" there is no actual argument being presented so it can't stand on its own merit. The statement offers zero evidence that AI is dangerous. "AI is dangerous" is taken as a given presupposition and as a blind assertion with the name of the expert being used to justify it. The logic goes like this:

So and so is an expert in AI and they say it's dangerous so it must be dangerous.

This nothing but circular reasoning and can't be taken seriously by any serious person. Either offer valid, concrete proof of AI's danger or take that sorry argument and get to walking with it.

The next question to ask yourself here is "expert in what?"

This question applies most recently with Geoffrey Hinton, a man I have tremendous respect for when it comes to his contributions to machine learning. A flurry of stories with the invariable title of "Hinton Leaves Google to Warn AI is Dangerous" came out last year and Hinton is often held up as proof that AI danger is not a crackpot fringe theory but a real problem because of his expertise is neural nets.

In this case, I point you back to the question "expert in what?" Professor Hinton has forgotten more about neural networks and statistics than I will ever know but that does not make him an expert in how technology and ideas diffuses into society over time. When it comes to AI, being an expert in psychology, business, neural nets, or advanced statistics, does not mean that person is an expert in how technology develops and changes over time. That's generally the work of sociologists, futurists and historians (who look for patterns in the past and sometimes try to apply them to the future).

That makes our opening statement an "appeal to authority" fallacy. It essentially relies on appealing to apparent (versus genuine) authority to settle the argument without providing evidence that AI is actually dangerous in any meaningful way.

When we talk to an actor about acting we are on solid ground, but we have to be careful when talking to an actor about medical advice. In the same way, if we are talking with Hinton on the inner workings of neural nets then we are on solid ground but if we are talking with him on long term patterns in technology and society we should proceed cautiously and insist on the merit of the argument.

However, to be clear, there are a limited number of experts on how technology spreads and proliferates over time. When it comes to the existing expert literature, the most well-known text in this area is the Diffusion of Innovation by Everett M Rodgers, an American communication theorist and sociologist. It's a fantastic book, though a hard read, as it is not written in a pop science style and it includes the inner workings of various sociological experiments tracked over time.

You've probably seen the Diffusion of Innovation curve on a dozen business presentations, which shows how ideas/technology go from early pioneers, to early adopters, to mass adoption, to laggard adoption. Usually the slide includes "the chasm" showing the hard leap from early adoption to mass adoption. The gap comes from a great pop business/marketing book that builds on the ideas of the Diffusion of Innovation called "Crossing the Chasm" by marketer and management consultant, Geoffrey Moore.

Because of a lack of extensive study of the history and future of technology, it means we can't automatically dismiss the expert in this case because studying the diffusion of innovation is not a widespread (or well paying) field and so there aren't many experts to go on here. As such, we have to be careful and we often have to rely on people who have dedicated a good chunk of their personal or professional time studying these phenomenon and lean on the small amount of existing literature.

Professor Hinton may have done extensive thinking on this subject, which could make him a genuine expert or not (hard to know for sure, although Yan LeCun noted that Hinton has only started thinking about these subjects very recently, but I can't verify it for myself as I don't know the man personally). Because we don't know how much time he's spent on this particular subject, it's hard to give any particular weight to Professor Hinton's expertise in this area and as such it adds little to the subject of AI danger. Anti-AI apologists are best coming up with a better, more evidence focused argument (as usual).

However, usually with AI scare propaganda, outside of the special case of Hinton, it's usually much easier to dismiss someone's "expertise".

The most egregious case of non-experts being used to justify AI danger is non-AI researchers getting labeled as "AI researchers." Professor Hinton is an AI researcher but the term "AI researcher" is applied way too loosely by the pop press to people who have never conducted any actual research or experiments in machine learning, have never trained a model, and have contributed no novel algorithms or improvements to models and their abilities.

Again, being an AI researcher does not make you an expert in the history and future of technological trends, but not being an actual AI researcher while being called one is an unmistakable red flag of non-genuine authority.

Thinking about AI and writing about AI does not make someone a "researcher" and anti-AI advocates often simply apply this title to themselves in an attempt to give themselves more perceived authority. Unfortunately, that status is often taken as an unquestioned given.

For instance, I am an AI expert but I am not an "AI researcher." My expertise comes from my work at various AI companies, the non-profit industry alliance around AI infrastructure I founded, my continued self study of the subject, going back several decades, as well as my voracious reading and thinking and writing about it, but I am not an "AI researcher" and I cannot and will not call myself one.

We see the term AI researcher most often applied in the pop press too loosely to people working on the "AI alignment problem," which is the desire for AI to mirror human values and the morals of their creators. People who spend their time thinking and writing about the AI alignment problem usually have never done any actual work in the real world to make a model more aligned so calling them "researchers" is simply incorrect and an attempt to appeal to a non-genuine authority.

One example of actual AI alignment research comes to us from Anthropic, a notable AI safety/alignment and large model lab, with their Constitutional AI approach. Citing this example puts you on more solid ground if you want to make the case for alignment. This is a Reinforcement Learning (RL) technique to make a model behave in a certain way and stick to a given set of principals. Anthropic models have often proven the most resistant to jailbreaking, which lends credibility to their approaches to alignment.

The difference here is people attempting to solve a real problem with real research/techniques versus simply complaining loudly that the problem exists and claiming it is unsolvable. You should take everyone who is simply talking about this problem with a grain of salt, unless there is strong evidence, reasoning and merit in their argument on its own.

However, to be clear, leaning on expertise is not by itself automatically a fallacy. Good critical thinking sometimes leads us to rely on genuine authorities. Studying the ideas of people who have dedicated their life to something is an essential way of accelerating our own knowledge and saving time. I advise everyone to think and verify things for themselves through critical thinking. Expertise can be an excellent shortcut but it's not enough on its own. Everything is available for direct knowing if we look closely enough and consider it carefully.

However, I do want to issue a few caveats here. "Think for yourself" is a phrase used equally as often by charlatans to justify fringe thinking as well as by sound critical thinkers. It usually goes something like "so and so authority disagrees with the general scientific consensus and we all know that 'experts' are liars and fools, so it must be true." We see this often times justifying fringe science, usually connected to the phrase "do your own research" to once again justify questionable thinking.

The major problem with "think for yourself" as a creed is that many people are not good critical thinkers, having had no training in it, and because critical thinking is hard and requires effort. So people who are not used to it usually simply consume a few bits of surface area information that they select via confirmation bias (choosing sources we already agree with to strengthen rather than question our assumption) and deciding they were correct all along! This one outspoken scientist disagrees with the consensus so the consensus is wrong. This is lazy, nonsensical thinking. The scientific method is about having lots of people validate a theory with evidence over time, in an attempt to get closer to objective reality, thereby averaging out the dissenters through clear observation and experimentation.

Relying on outside expertise is also one of the core advantages of the human species. Matt Ridley has a great picture at the very beginning of his book, The Rational Optimist, of a stone age tool and a mouse side by side. The difference between the two objects is the key to our advantage as a species. The stone age flint tool is the work of one person and their expertise at carving that tool. The mouse is the result of thousands of different people or experts working on their little piece of the puzzle.

No one person understands how to build a mouse in its entirety. We rely on experts in mining to extract materials and experts to make plastics and experts in circuit board design, all in a long chain to make a new and complex device. No monkey has every gone beyond individual expertise but humans building on the backs of other experts is a core feature of our ability to learn rapidly and a major advantage of our species.

We can quickly learn new things because we don't have to learn everything ourselves and can rely on a chain of legitimate other experts.

Occasionally, a dissenter is right and they help move the world forward too. Einstein disagreed with the dominant theory of light but issued a series of compelling papers that outlined his theories. Over time, experimental physicists, many who vehemently disagreed with him, had to alter their understanding of reality because their experiments proved his theories again and again. The key here is proof, actual, clear proof. If a new theory is correct, it will prove true based on repeated validation and experience in the real world and it will eventually replace that outdated theory. The mark of a good mind is change and willingness to adapt as we learn new information when we find it accords with reality.

So relying on experts can help us but it's often not enough and there are many pitfalls along the way as we try to decide who is and who isn't a genuine expert. We have to study various experts and the evidence ourselves to draw a serious conclusion.

So what do we find if we study the diffusion of innovation ourselves? Consistently, we find technology finding its balance in the world over time. This is particularly true of general purpose technologies, with a range of capabilities and possibilities, like AI, or concrete, or the steam engine, or the printing press, or CRISPR.

What we tend to find when we study the long term patterns of history and how technology and ideas proliferate, is that ideas and technology always face resistance, but have passionate early adopters. There are often fears at the beginning of that technology's evolution about change. Who benefits? Who loses? How will life be different? We've seen this in everything from bicycles, to the radio, to steam powered ships, to cars aka "the horseless carriage," to the Internet and now AI.

Technology never spreads unimpeded. It has detractors, supporters, and along the way, the technology changes as it moves through time. Engineers deliver engineering improvements and we see societal norms and legal standards change, which acts as a limiter on that technology, the way friction acts on you when you slide down a hill on a sled. Other technologies enhance it or create more friction against it. Over time that technology finds a balance, where it settles into people's lives and people use it and no longer think about it. It simply exists. A child growing up today uses a cell phone and perceives it the same as a tree. It was always there and does not need further worry or discussion. It's just something to use and learn.

Over time, what tends to happen is that we make the technology safer and more aligned with what we want it to do in the world. Early planes had terrible track records and often crashed and killed people. Through the years engineers and operations teams developed better mitigations for the worst problems in plane manufacturing and plane operations at both a policy and technical level. For instance, pilots are now trained to deal with time dilation in a crisis situation, as detailed in the excellent book Blackbox Thinking, by Matthew Syed.

That's where time slows down and you feel like you have hours instead of one minute to land that out-of-control plane. They developed better auto-pilot systems, better materials and better ways to build and design planes. They created intelligent policies, with clear understanding of outcomes, where if pilots self-report near accidents within two weeks, it can't be used against them in a court of law, but if they hide it it can be used against them. That leads to truthful information sharing and lets ground operators fixing problems fast.

Now flying is one of the safest activities on Earth. According to the International Air Transport Association (IATA) "The industry 2022 fatality risk of 0.11 means that on average, a person would need to take a flight every day for 25,214 years to experience a 100% fatal accident."

Over time, we always develop better and better mitigations but those have to happen in the real world, as problems develop. We don't develop most migrations before the technology proliferates. We don't develop the safety razor before the razor, or the seatbelt before the car.

When we create policies we have to have a clear understanding of outcomes and reward/punishment and their effects on behavior. The way airline safety proponents created the policy to incentivize pilots to tell the truth, through a personal benefit to them, is genius and leads to the right outcome. But if you start with an incorrect understanding of outcomes, your policy will backfire. This is called the cobra problem, taken from UK policy makers incentivizing people to kill an invasion of cobras by bringing in their tails as proof to get paid. That lead to people breading cobras to bring in more money, producing the opposite effect that regulators intended.

Studying the actual patterns of history is a good way to try to understand the future, however imperfect. What is not good is simply relying on an appeal to authority to make a case, without any further evidence. This is doubly true when the "danger" of AI is taken as an unquestioned given.

When you hear people declare that so and so expert says AI is dangerous, fall back to making them prove that AI is dangerous vis a vis the history of all other technologies and those general patterns.

Do not let them go to the "this time is different" argument either, because the larger probability is that AI is not different and it will follow the patterns of previous technologies as it proliferates and finds its level and place in the world.

If someone is a true expert, that's wonderful and it provides some supporting evidence to an assertion but it's not enough on its own.

They still have to prove that AI is legitimately dangerous with clear evidence and proof.

Without that, their arguments are nothing but puffs of smoke.

---

Destroying Terrible Anti-AI Arguments: Day Four

"My p(doom) ration is X%"

Nate Soares, the president of the Machine Intelligence Research Institute, (an AI research institute in the same way that the People's Democratic Republic of North Korea is a democratic republic) wrote that there's a 95% chance that AI will kill us all and all AI research must be stopped now before it's too late!

Where does this 95% chance number come from?

Nowhere. It's completely made up.

What is the actual probability that AI will kill us all? Trick question! Any answer someone gives is just totally made up based on absolutely nothing more than someone's feelings.

How people come to this number is based on their personal experience, relative intelligence, the things they read, whether they were bullied as a child, their general chemical makeup, whether mommy was nice or mean to them, where they want to school, what they saw, and all the other little things that make up who we are as a person over time.

What it is not based on is any actual predictive model of the future.

People are experience machines and we build our models of reality based on that and we interpret reality through those models. There is nothing to say that model is right. We are a black box and we don't always generalize well. People generally never experience reality directly, but through the filter of their senses and their mind's predictions about what is happening. We see a snapshot of what is around us and fill in a lot of the blanks.

But there is no actual predictive formula, no statistical model, no actual rubric for getting to that number.

It's just people putting their finger to the wind and saying "I think it feels like 20%." They're making up a number based on absolutely nothing more than a feeling and a personal conviction that it's a real threat.

He also wants you to know that there's a "debate within the alignment community re[garding] whether the chance of AI killing literally everyone is more like 20% or 95%" and that he is in "the more like 95% myself" camp. That's just an appeal to the crowd and a false one at that. Actual polls taken, such as the Ipsos Global Views on AI 2023 report, show that 54% of people across the world in 20 major nations from the US to Thailand to India, say "AI has more benefits than draw backs" with old folks skewing towards fear (43% in Boomer) and younger folks skewing much more positive (62%). Either way, what the crowd actually believes is of zero use to us anyway and makes no difference in clear, sound, critical debate because the crowd can be wrong.

Also, the AI-will-kill-us-all contingent is just a small echo chamber faction of the alignment community, which is itself a tiny fraction of AI researchers, which is itself a tiny fraction of people on Earth. And their numbers are no more real than a number you make up about the probability of aliens invading tomorrow or a coronal mass ejection from the sun killing us all.

Saying your p(doom) ratio is also a trap set by the person asking it. If you say anything above zero then the person immediately jumps to "well if there is any chance that the apocalypse can happen then we've got to do everything we can to stop it now!"

Unfortunately, everything good and worth having in life is a risk. You cannot make things "absolutely 100% safe." There is nothing in life that is guaranteed. You can sit inside all day like an Agoraphobic who never leaves the house and you still aren't safe.

Life is a risk. It's a risk every time you get into a car. You have a 1 in 93 chance of dying in a car accident in your lifetime. You still get in a car. It's worth the risk because walking everywhere is great when everything is a few miles away but not so great if you want to go from New Jersey to New York.

Not using or allowing AI is also a risk. Take driving, for a great example. Why do we want AI to drive? Because humans are terrible drivers. We kill 1.35 million people on the road every year and injure 50 million more. If AI cuts that by half or down to a quarter that is 1 million people walking around and playing with their children and living their lives.

The default belief of many in the AI-is-dangerous movement and that in the future it will become so dangerous that it will rise up and murder us or cause a catastrophe. Not just metaphorically. They believe in total extermination of human beings, literally. They use that word a lot, literally, because they really mean total extinction of the human race. In other words they watched Terminator too many times and now believe it's reality. Their basic idea is that intelligent and nice don't always line up. Unless we can guarantee that AI will always do exactly what we want, we shouldn't take any chances! Better safe than sorry!

But again, where does this belief come from? We like to think of our beliefs as sacred but when our beliefs don't accord with reality what good are they? You might believe that gravity doesn't exist but if you go step out of a 100 story window you will see that gravity has an undefeated record. These beliefs on AI's future danger are based on speculation, imaginary concepts and things that simply don't exist. And to be very clear, humans are generally not great at prediction. We are great at imaging all the things that will go away but terrible at predicting the new developments and the good things that will happen.

You cannot predict the Internet and what it will be like as an 18th century farmer. That's because it's built on the back of a chain of inventions like electricity, wires, computers, semiconductors, information theory, the browser, graphic design, and so much more. It's impossible to see all of that coming together and how it will effect life in any meaningful way. The only way to understand it is to get there in time and experience it.

We have no superintelligent AI (in the way people commonly imagine it) and don't know what it will look like *if *and when it does arrive. Actually, we can make the case that we do have "superintelligence" already and it looks nothing like what we expected. GPT-4 knows 100s of languages and can speak well and widely on more domains that any single human. If you've mastered three languages you are in the 1% of learners, and ten languages puts you in the upper 1% of 1% of language learners. The Guinness World Record is held by Ziad Fazah who spoke 58 but probably not as well as GPT 4. Our understanding of what we thought AI would look like and how it is actually developing in the real world don't accord, as if so often the case.

A lot of people spend a lot of their time (when they could be doing useful things instead) speculating (badly) about the nature of superintelligence and what it will be like. This is a bit like the Wright brothers speculating about what a Boeing 787 Dreamliner will be like, as well as what society will be like around it. There is no chance that it will be accurate because by the time the 787 gets here it's built on a chain of other developments, technologies and innovations like new materials, computers, electricity, engines, etc. Not to mention the fact that you don't know what society around that Dreamliner will be like because it is built on the back of millions of other developments and inventions and political changes.

And again, why do we assume that true superintelligence AI (if such a thing is even possible to build) will be a homicidal maniac? Why do we assume it will want to wipe out the world? Why do we assume it will have sentience and goals of its own? Why do we assume that there will be no developments in alignment or transparency or understanding or control? Why do we assume there will be no other frictions or developments in society at large to allow us to adapt with the technology, like we have done with 100% of other technologies before it?

Many of the negative answers to these questions are all taken as a given by anti-AI apologists and these presumptions cannot be taken as a given. They exist nowhere outside the pages of sci-fi and sci-fi is a terrible predictor of actual reality since stories are about one thing and one thing only: conflict. That's why all the "AI" we've had in stories are robot buddies or the Big Evil Bad in a metal skin. They are just characters put into a metal sheath. But real AI is developing in ways that simply do not mirror sci-fi up until this point. Sci-fi is not reflective of how technology develops in reality. They're stories, just stories.

On the flip side, it sure seems like more intelligence, more widely spread, is the best way to make this world safer and more abundant for as many people as possible.

When you think about it, what doesn't benefit from more intelligence?

Is anyone sitting around thinking I wish my supply chain was dumber? I wish cancer was harder to defeat. I wish drugs were harder to discover and new materials were harder to create. I wish it were harder to learn a language.

Nobody.

More intelligence is better nearly across the board. We want an intelligence revolution and intelligence in abundance.

If we let it proliferate and we don't kill it in the crib based on regulatory capture disguised as safety or the paranoid worries of people who believe p(doom) is 95% because they read too much sci-fi as a kid, intelligence will weave its way into every single aspect of our lives, making our economy strong and faster, our lives longer and our systems more resilient and adaptable. Where does that prediction come from? Easy. It comes from looking at the entire history of technology and societal development up until this point.

As Matt Ridley wrote in the Rational Optimist: "Since 1800, the population of the world has multiplied six times, yet average life expectancy has more than doubled and real income has risen more than nine times. Taking a shorter perspective, in 2005, compared with 1955, the average human being on Planet Earth earned nearly three times as much money (corrected for inflation), ate one-third more calories of food, buried one-third as many of her children and could expect to live one-third longer. She was less likely to die as a result of war, murder, childbirth, accidents, tornadoes, flooding, famine, whooping cough, tuberculosis, malaria, diphtheria, typhus, typhoid, measles, smallpox, scurvy or polio. She was less likely, at any given age, to get cancer, heart disease or stroke. She was more likely to be literate and to have finished school. She was more likely to own a telephone, a flush toilet, a refrigerator and a bicycle. All this during a half-century when the world population has more than doubled, so that far from being rationed by population pressure, the goods and services available to the people of the world have expanded. It is, by any standard, an astonishing human achievement."

Averages can hide the outliers but if you really look at it, it's nearly impossible to find any place on Earth that is not better off now than it was in 1955 or earlier, because of technology and our collective advances. If you want to have solid grounds for predictions then look to the past instead of predicting sudden and total decline.

In the 1960s the book The Population Bomb predicted that we'd need to let 2 billion people starve because there just weren't enough resources to go around. Instead we got the Green Revolution and can sustain more folks than ever.

Historian Michail Moatsos estimates that in 1820, just 200 years ago, almost 80% of the world lived in extreme poverty. That means people couldn't afford even the tiniest place to live or food that didn't leave them horribly malnourished. It means living on less than $1.90 a day in 2011 prices and $2.15 in 2017 prices. Actually you don't even need to go back that far. In the 1950s, half the world still lived in extreme poverty.

Today that number is 10%.

As Thomas Babington Macaulay wrote, "On what principle is it, that when we see nothing but improvement behind us, we are to expect nothing but deterioration before us?"

I fully support everyone's right to believe whatever they want to believe and to make up any p(doom) number they want by sticking their finger to the wind. But I don't support their right to make stupid and paranoid public policies for the rest of us.

If you believe in an AI apocalypse that's fine but you don't get to make the rules for the rest of us, just as Ted Kaczynski doesn't get to shut down society and send us all back to our imaginary agrarian roots just because he thinks it's a grand idea.

---

Destroying Terrible Anti-AI Ideas: Day Five

"We need control frontier models like nuclear weapons with compute thresholds, model registration and inspectors."

These are unbelievably terrible ideas on multiple levels.

These proposals showcases a remarkable ignorance about human nature, game theory, group psychology and pretty much everything else that matters when trying to make laws that have clear outcomes and properly designed incentives.

The logic goes like this: Let's surveil all data centers and register models with large training runs because, once again, the presupposition is that "superintelligent" AI will be monstrous and genocidal.

The main idea behind compute limits/inspections is that AI eats up a lot of compute and bandwidth so we can spot the real smart models as they train. A super powerful government agency of highly trained and capable inspectors (good luck with that hiring process and pay scale) would be able to swoop in and kill any model in its crib if it showed signs of "being dangerous" (with danger always being nebulously defined as whatever they say is dangerous).

When you make laws you have to clearly understand the outcome you want and choose the proper incentives and punishments. Laws are simple at their core. They are about reward and punishment to shape human behavior. This is a very tricky balance. In short, get the incentives right and everything follows. Get them wrong and it doesn't matter what you do, the law will be an abject failure. A classic example is the Cobra effect, which I discussed an earlier post:

During British rule of India, the British government wanted to control the number of venomous cobras in Delhi and offered a bounty for every dead snake. At first it worked well. People killed snakes in droves and got their rewards. But eventually, some enterprising young men decided to breed Cobras to make more money. Once the Empire figured this out, they killed the program and the Cobra breeders set their now-worthless snakes free, doubling the population of Cobras in the wild.

This is know as as "perverse incentive" which basically just means the lawmakers got the incentive wrong and didn't understand cause and effects well enough.

In other words, it's very important to understand the intent of the law before you go about crafting the precise incentive/punishments to achieve that goal.

The clear goal of the folks proposing these rules is to kill AI in its crib and prevent it from ever getting too powerful but it is NOT sold to lawmakers that way.

It's sold under the guise of making sure powerful models don't fall into the wrong hands.

This is classic reasoning for politicians: make my country strong and don't let my enemies get stronger. It's also a smart framing but since the real goal is to make sure powerful AI never develops

_at all_

, Houston, we have a big problem.

If you are are western lawmaker and you would like to ensure that your country's models don't get too powerful (or useful) then please go ahead and implement these policies, with the notable side effect that other countries will NOT implement these rules, especially authoritarian and totalitarian regimes, which means the future of AI will be decided by people who do not share your values. If your goal, as a western lawmaker, is to shoot your country in the foot economically, culturally, and militarily then this is the law you want to pursue.

If on the other hand, you want to ensure that your country is a center of AI innovation, development and economic growth then you should avoid these compute thresholds and inspections like the plague.

When lawmakers move forward with these kinds of proposals without understanding the true hidden goal of the proposal then they will get not get what they were expecting, which is powerful models in their own country while keeping them out of the hands of enemies. Instead, they will have comparatively weaker models in their own countries while their enemies race ahead unconstrained.

The key to understanding why is to understand how things play out in reality. You have to understand the myriad effects and outcomes that are likely to result from a law as it takes effect in the real world. It is not enough to understand the law itself. The outcome is essential. You have to game it out ahead of time.

It plays out like this in reality: With reduced competition, we end up with a small number of frontier models, controlled by a small subset of companies who can afford the massive compliance burden. Those models will be powerful but comparatively weaker to models developed under more open conditions, since fewer players have entered the market and contributed to the collective knowledge of how to make those models better, safer and smarter. These companies will also have zero reason to innovate over time because they have no competition, which means they stagnate.

At the same time, foreign powers will not adhere to such rules which means they will race forward because they are not constrained and their industry can develop and evolve naturally, which means it eventually surpasses your own and you get a brain drain in your country as smart people to go work where they are wanted and needed and where they have the best chance at success.

In other words, these laws will absolute crush Western innovation.

It practically guarantee that the next generation of technology comes out of a non-western nation or an authoritarian regime because not a single one of those nations will agree to any kind of compute limits when it comes to their own national sovereignty, surveillance, defense and weaponry.

Sometimes AI X-risk folks propose universal bans on large training runs and models across the world. Of course! We'll just get every country to agree on something at the same time! This is magical thinking at it's finest.

No one will ever get the major nations in the world to agree on this kind of proposal based on imaginary sci-fi risks. So it means that only nations that implement these rules cripple themselves while handing the baton of future innovations and developments to other world powers.

But let's pretend for a second that we somehow magically got every single country in the world to agree to do this. China and North Korea gladly open up their doors for reporting on training to the whole world! So does Russia and various unstable Latin American dictatorships. All is well and we are all reporting accurately and faithfully to the global world AI Turing police.

But this starts to fall apart fast as decentralized training takes off and as the cost of training big models falls dramatically. From 2012 to 2019 the cost to train an image classifier dropped 44X. It will likely drop even faster now as more and more chips saturate the market and we get algorithmic and training tricks that make it faster and easier. GPT 4 was expensive to train but the cost keeps dropping as more and more players join the game. The cost to train GPT-4 was around 100M. The cost to train Llama 2 70B, a state of the art open source model that surpasses GPT 3.5 and approaches GPT-4, was estimated at around $8M only four months later.

It won't take long before the cost drops to below 1M and even lower than that in the coming year. Pretty soon you can train GPT-4 on your desktop GPU cluster and run it on your watch, which means individuals will be able to train an incredible model and individuals are unlikely to report on those training runs.

These surveillance proposals also don't bother to take into account new ways to train AI. Sakana AI, founded by my friend David Ha and Llion Jones, one of the folks behind Transformers/Attention is All You Need, is looking to train highly capable small models that you can chain together. These would naturally fall below any compute thresholds. Yan LeCun's and Rich Sutton's work on more brain like architectures for the future seem to mirror this as well, with the brain divided into smaller specialized chunks. You can imagine a router that is able to choose between highly specialized sub-functions in an intelligent system. All of these might fall below the compute thresholds. The HuggingGPT paper already proposed extending GPT-4 by having it pick which smaller models to use to complete complex tasks and this is likely the future, a chain of smaller more capable models in a pipeline controlled by a logic engine.

If tomorrow someone comes up with a breakthrough to train AI the way we train humans, with little data, over short periods of time, the entire surveillance approach falls apart altogether.

And if someone created that method, would they even bother telling anyone or would they just train the best models in the world on a fraction of the compute, falling well below the requirements and hence not needing to report, and not bothering to detail their breakthrough to anyone? You bet they would.

The technical know-how to train massive models is already widespread as well. The tools to build big models are all open source. Even proprietary models train on open source distributed training frameworks like Deepspeed, Horovod, and open source inference/training engines like Ray running on top of open source container management systems like Kubernetes. Universities around the world and many companies have already shared their code, models, algorithms and their entire training repositories to make it easier. Other nations do not need access to GPT-4. They can train their own.

And if they are locked out and they do need access to GPT-4 you can bet they will simply set their APTs (Advanced Persistent Threats) on the problem. These are powerful government hackers that do not give up and have all the time, money and support they need to steal those models. They will just steal it.

Again though, there are no secrets to building smarter, faster models. You just need the money and the will and soon you won't need that much money either because the price of compute is falling fast.

Or at least that's what it takes now. A breakthrough in algorithms could drop off the massive compute part too.

The only way for licensing requirements and training limits to really work would be to give governments extraordinary new surveillance powers and to get global coordination at a level that is unprecedented for a problem that does not really exist. Considering we've never gotten that kind of coordination on anything (see climate change), it's basically a non-starter.

Maybe the only time it ever worked was for the regulation of chemical weapons and that was only because 1) every country on Earth had experienced the actual real world horror of chemical weapons and 2) chemical weapons are shitty, indiscriminate weapons anyway so it was no real loss to most countries to get rid of them in favor of weapons of mass destruction or precision weapons like laser guided missiles.

It also doesn't stop state actors from creating powerful AIs, or rogue states, or companies buying chips through intermediaries.

There is now a black market for the top Nvidia chips in China and other places on the entity list. It's also accelerating China's push to build their own independent chip pipelines (the Cobra effect in full effect) and you can bet they won't care at all about limits on super powerful AI when it comes to state security.

Licensing regimes buy, at best, a few years time (if that and probably not) before they fall flat on their face.

Algorithmic improvements, new training techniques and the ever falling price of compute and training will wreck any licensing scheme fast. Once the costs drop far enough, the number of people/teams who can train a powerful model will proliferate and policing them will become impossible.

It's essential that lawmakers understand the incentives behind the laws and the people advising them to make those laws. Question those incentives at every turn. Understand the actual outcomes of how laws will play out in the real world.

If your goal to make sure you never develop powerful AI at all in your nation, then by all means implement these idiotic, short-sighted and maliciously delusional proposals.

But if you're looking to build a thriving AI industry and to actually ensure that AI develops safely, then you want the most economic players contributing their knowledge to the evolution of that industry.

You should run the other way from limits and inspections or else you will destroy your own sovereignty, crush your future economic expansion and hand the future to some other nation that is not delusional and worried about AI turning us all into paperclips.

---

"ASI will kill us all"

This one is similar to "my p(doom) is x%." It's a blind assertion. It has no validity outside the mind of the person saying it no matter how many times they repeat it.

The reason is simple. We have no idea what ASI will be like and ascribing characteristics to it is an exercise in futility. Pausing AI to create solutions for a non-existent thing will be of zero value when (and if) ASI actually gets here.

Here's a few illustrations to show you why:

Imagine going back in time and asking da Vinci what are some of the characteristics of a modern aircraft in 2024, like the Boeing 787 Dreamliner?

Ask him about how we can make it safer? Ask him about the general challenges of building such an aircraft and how we can improve the supply chain? Ask him about our safety protocols, like incentivizing pilots to report near accidents within two weeks and if they do that faithfully it can't be used against them in a court of law? Ask if that makes a big difference in airline safety and how?

His answers on any of these questions will be utterly meaningless.

That's because he can't conceive all the myriad economic developments (division of labor, specialization, free markets), supply chain developments (global interconnectedness, mega-boats, shipping containers), societal developments (modern democracy), and stacks of compounding technological development (lighter materials, engines, computers, electricity, wires, interchangeable parts, etc) which all weave together to make the modern airplane.

These other inventions and technologies and cultural developments all influence, mitigate and create friction on the other technology. It does not exist on its own.

Many folks will tell you that long term prediction is easy and they are dead wrong. Humans are absolutely terrible at this, even people who make a living doing it. We can sometimes predict near term trends pretty well but long term trends are beyond our event horizon.

What almost inevitably happens is people project forward into the future all the characteristics of the present day and add one new invention. That is not how things develop in reality.

Society, technologies, culture and people evolve in massive parallel at a level that is mindbogglingly complex. As we move forward in time we have a tremendous confluence of new inventions, political strategies, legal frameworks and more, all at the same time, all in parallel. It's everywhere, everything, all at once.

A great example of this is the 1968 book, The Population Bomb, which predicted that we'd need to let 2 billion people starve because there just weren't enough resources to go around. Instead we got the Green Revolution and can sustain more folks than ever. In a classical example of terrible prediction error 101, Paul R Ehrlich, projected forward all the characteristics of the 1960's into the future and failed to predict any other possible societal, biological, or technological developments.

Try explaining the Internet to an 18th century farmer. You can't do it. That's because it's built on the back of chain of inventions and societal developments like electricity, turbines, wires, information theory, the computer, browsers, digital computers, etc.

If Da Vinci projects forward his flying machine to 2024, but imagines it with the characteristics of Renaissance Italy in 1500's, his predictions about life in 2024 are less than worthless. They have zero chance of being accurate. Even if he is more creative in his thinking, projecting forward hundreds of years is impossible for anyone because they have to come up with the distributed, collective inventions of the entire world of people working collectively over time, building on the back of the people who came before them.

Even near term predictions are tricky. A great example comes from one of my favorite sci-fi books, Neuromancer, by William Gibson. Gibson came up with the idea of cyberspace before the Internet really came out of universities and the military, in the early 1980s. That's cool but how he saw it developing had nothing to do with how it actually developed.

He saw all data as a kind of 3D construct because it was too difficult for any one person to perceive. Actually we can perceive data just fine and we can interpret it directly in databases and spreadsheets and documents.

He also saw the Internet as something you go to, not something that is around you all the time on the laptop and phone and watch through wireless. In fact he failed to come up with wireless at all. One of the funniest moments in the book is hacker laying a fiber optic cable over five miles to connect to a modem to connect to cyberspace.

Another hilarious moment is when a hacker calls up on landline phone (because he also failed to predict portable phones) to get the time, like people did in the 1960s. Despite all his prodigious prediction, he could not even conceive of the time being always available everywhere or the internet floating through the air.

What we can do though is perceive _patterns and abstractions_ of the past and the future. The pattern of the past is that we have always come up with a massive amount of parallel developments and inventions and mitigations for technology, all acting on, supporting and limiting each other.

We are on solid ground when we predict in terms of patterns/abstractions. I don't know what those mitigations to ASI will be or what ASI will be like but I can predict that parallel inventions that act on AI will develop at the same time and as needed because that is how it worked 100% of the time in the past.

There is no reason to imagine this is some kind of exception, no matter how many times X-riskers tell you "this time is different" (famous last words, used thousands of times in history to describe pretty much every technology that has ever existed and has been wrong precisely 100% of the time).

We don't know what mitigations, parallel developments, societal/cultural developments and technologies will accompany ASI and how it will help keep it on the rails but we know they will come. Already we have people working on actual alignment solutions. We have RLHF and we have RLAIF, aka Reinforcement Learning with AI Feedback. Researchers are developing solutions. Others will follow.

Regular engineers are now getting their hands on ML models and bringing their unique perspective to the problem as well. They see the problem differently than ML engineers and research scientists and bring a fresh perspective. As they work with open source models they're working on guardrails that come from their knowledge of traditional hand coded logic.

Why?

Because they want the AI to behave and give them consistently good results!

They want it to do what they want so they can build apps.

They don't want it to decide that it doesn't feel like clicking on the button to buy a ticket.

We are incentivized to make these systems more reliable as a species because we want them to work. All these developments will inevitably lead to new solutions and ideas that are practical in the real world of AI, not the AI of fiction.

While many "AI researchers" are writing philosophical treatise on why AI can never be our friend and warning about the end of the world like they're Jim Jones reincarnated, actual researchers are hard at work on the problem.

Problems are solved in the real world through engineering, as they develop in time. We invent the seatbelt and the airbag and crumple zones for the car, after we realize that it can go too fast. We can't conceive of that problem before it happens.

That may seem strange to you but you are looking at it after we have traveled faster than we could run, which was not the case for people before trains and cars existed. They never traveled faster than a horse could carry them. People before the train and the car had heated debates about what would happen to the human body if we went 10 miles an hour. Would we spontaneously explode? Would we die of a heart attack? Would we pass out? In short, their predictions were dead wrong. These all sound ridiculous now but that is because we know how it turned out after it happened in the real world. We have actual knowledge to build on. Once we get that, we build actual solutions.

That's why calls to "pause AI" are deeply misguided. It is a basic misunderstanding or real world cause and effect. These folks do not understand that you don't invent solutions in a vacuum.

We can't invent solutions to imaginary problems.

We invent solutions by interacting with the problem in reality.

Problem happens, you think about it, you work out a solution. Or rather, problem happens, lots of people work on it, trying many different solutions until we get one that works and then we deploy that solution. It's an iterative process that happens _after or in parallel_ to the development of the original technology.

The safety razor comes after the razor. Crumple zones come after the car.

Don't worry. The engineers will take it from here and do what we always do when it comes to real world problems.

We'll solve them.
